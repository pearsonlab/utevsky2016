{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}(e^\\lambda)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B \\cdot X)_{us} + (C \\cdot Z)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-7, 1^2) \\\\\n",
    "Z_{ks} &\\sim \\mathrm{Bernoulli}(\\pi_k) \\\\\n",
    "\\pi_k &\\equiv \\prod_{i=1}^k \\delta_k \\\\\n",
    "\\delta_j &\\sim \\mathrm{Beta}(3, 1)\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we assume that $A$, $B$, and $C$ are all hierarchically distributed:\n",
    "\n",
    "$$\\begin{align}\n",
    "A_u &\\sim \\mathcal{N}(\\mu_A, \\sigma^2_A) \\\\\n",
    "B_{up} &\\sim \\mathcal{N}(\\mu_{Bp}, \\sigma^2_{Bp}) \\\\\n",
    "C_{uk} &\\sim \\mathcal{N}(\\mu_{Ck}, \\sigma^2_{Ck}) \\\\\n",
    "\\mu_A &\\sim \\mathcal{N}(\\log 25, 0.5^2) \\\\\n",
    "\\sigma_A &\\sim \\mathrm{Ga}(2, 4) \\\\\n",
    "\\mu_{Bp} &\\sim \\mathcal{N}(0, 0.5^2) \\\\\n",
    "\\sigma_{Bp} &\\sim \\mathrm{Ga}(2, 4) \\\\\n",
    "\\mu_{Ck} &\\sim \\mathcal{N}(0, 0.5^2) \\\\\n",
    "\\sigma_{Ck} &\\sim \\mathrm{Ga}(2, 4)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "Nrep = 40  # number of observations per unit per stim\n",
    "NB = 1000  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = np.log(softplus(25 + 5 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.1 * np.random.randn(NU, P))  # regressor effects\n",
    "dC = np.log(np.array([0.25, 0.55, 1.4, 2.2])[np.newaxis, :] + 0.1 * np.random.randn(NU, K))  # latent effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70137886  0.57732232  0.24895886  0.49916835] [ 0.70137886  0.40492167  0.10080884  0.05032058]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACmlJREFUeJzt3V+IbedZB+Dfe3poqBSKWtpIjjGIaDEU2puI5MKxUnJU\naLwSg4h6rSSgSLU3HS8E7yTQS9MSArXVXJgUBFuJQ6liE2xCQ5O0BSH9g+d4UyklINW+XsxuGY+T\n2TvfrDWz1j7PAwf2XmfNt7/1rXetPb/51t6rujsAAAAw4spldwAAAID1EioBAAAYJlQCAAAwTKgE\nAABgmFAJAADAMKESAACAYRcSKqvqelW9UlVfqaoPXsRrwhSq6rGqullVXzyx7Ier6tNV9eWq+vuq\nettl9hG2qaprVfVMVX2pql6sqoc3y9Uyq1JVd1TV56vq+U0tf3izXC2zOlV1paq+UFVPb56rY1Zr\n9lBZVVeSfCTJA0nuTfJQVb1r7teFiXwsx7V70h8n+Yfu/pkkzyT5kwvvFbwx/53kD7r73iQ/n+T3\nNudhtcyqdPd/JfnF7n5vkvck+eWqui9qmXV6JMlLJ56rY1brImYq70vy1e5+tbu/m+QTSR68gNeF\nc+vuzyX51i2LH0zy+Obx40l+7UI7BW9Qd9/o7hc2j7+T5OUk16KWWaHufm3z8I4kV5N01DIrU1XX\nkvxKkr88sVgds1oXESrvSvL1E8+/sVkGa/WO7r6ZHP+ynuQdl9wf2FlV3ZPjGZ5/SfJOtczabC4Z\nfD7JjSSf6e7nopZZn79I8kc5/qPI96ljVssX9cD59fZV4PJV1VuTPJnkkc2M5a21q5ZZvO7+3uby\n12tJ7quqe6OWWZGq+tUkNzdXkNQZq6pjVuMiQuU3k9x94vm1zTJYq5tV9c4kqao7k/zHJfcHtqqq\nqzkOlE9091ObxWqZ1erubyc5SnI9apl1uT/JB6rq35L8VZL3VdUTSW6oY9bqIkLlc0l+qqp+oqre\nnOQ3kjx9Aa8LU6n8378kPp3kdzaPfzvJU7f+ACzQR5O81N2PnlimllmVqnr7978Rs6rekuT9Of6M\nsFpmNbr7Q919d3f/ZI5/L36mu38ryaeijlmp6p5/Zr2qrid5NMch9rHu/vPZXxQmUFUfT3KQ5EeT\n3Ezy4SR/m+Rvkvx4kleT/Hp3/+dl9RG2qar7k3w2yYs5vpyqk3woybNJ/jpqmZWoqnfn+AtMrmz+\nfbK7/6yqfiRqmRWqql9I8ofd/QF1zJpdSKgEAABgP/miHgAAAIYJlQAAAAwTKgEAABgmVAIAADBM\nqAQAAGDY1akaqipfIwsAALCnurtOWz5ZqNy8yJn/f3h4mMPDwylfcnZVp47bpZjq9i9TbdPtejua\nXep4SXWTqB1Ot8ZzMrcf5535GeP57esY7+t27aO5fzd1+SsAAADDhEoAAACGXWioPDg4uMiXg1mo\nY/aFWgYAplATftaq9/F66CV9Ns7n4tZjSXWTqB1gvZx35meM57evY7yv27WPJtxXpzbk8lcAAACG\nCZUAAAAMEyoBAAAYtlOorKrrVfVKVX2lqj44d6cAAABYh62hsqquJPlIkgeS3Jvkoap619wdAwAA\nYPl2mam8L8lXu/vV7v5ukk8keXDebgEAALAGu4TKu5J8/cTzb2yWAQAAcJvzRT0AAAAMu7rDOt9M\ncveJ59c2y/6fw8PDHzw+ODjIwcHBOboGAADA0lV3n71C1ZuSfDnJLyX59yTPJnmou1++Zb3e1tYa\nVdVld+EHphrfqbZpH/f3VJZUN4naAdbLeWd+xnh++zrG+7pd+2jCfXVqQ1tnKrv7f6rq95N8OseX\nyz52a6AEAADg9rR1pnLnhsxUzs5s03osqW4StQOsl/PO/Izx/PZ1jPd1u/bR3DOVvqgHAACAYUIl\nAAAAw4RKAAAAhgmVAAAADBMqAQAAGCZUAgAAMEyoBAAAYJhQCQAAwDChEgAAgGFXp2ysqqZsjpl0\n92V3YdGmqOOpxniqY2pfj82ptmuK/bW0MV5aDS7tvLOk/WVsLsa+bteSGOPbj/PX69vH9+Gz+mKm\nEgAAgGFCJQAAAMOESgAAAIYJlQAAAAwTKgEAABgmVAIAADBMqAQAAGCYUAkAAMAwoRIAAIBhW0Nl\nVT1WVTer6osX0SEAAADWY5eZyo8leWDujgAAALA+W0Nld38uybcuoC8AAACsjM9UAgAAMEyoBAAA\nYNjVy+4AAAAAy3J0dJSjo6Od1q3u3r5S1T1JPtXd7z5jne0NcS677CvOr6rO3cZU+2qKvizR0sZn\niv4sbV/t4xhPaUn7y9gAu1ra+WJplnT+2sf34apKd5/aoV1uKfLxJP+c5Ker6mtV9bvn7hEAAAB7\nYaeZyp0aMlM5O3+duhhmKue3tPExU/n6ljTGU1rS/jI2wK6Wdr5YmiWdv/bxffhcM5UAAADweoRK\nAAAAhgmVAAAADBMqAQAAGCZUAgAAMEyoBAAAYJhQCQAAwDChEgAAgGFCJQAAAMOESgAAAIZVd0/T\nUNU0DQGTm/A4n6SdqezrdjG/KWpH3ZzN8Tk/Y7we9tXZ9nF89nGbkqS7T+2QmUoAAACGCZUAAAAM\nEyoBAAAYJlQCAAAwTKgEAABgmFAJAADAMKESAACAYUIlAAAAw7aGyqq6VlXPVNWXqurFqnr4IjoG\nAADA8lV3n71C1Z1J7uzuF6rqrUn+NcmD3f3KLeud3RBwabYd57uqqknamcq+bhfzm6J21M3ZHJ/z\nM8brYV+dbR/HZx+3KUm6+9QObZ2p7O4b3f3C5vF3kryc5K5puwcAAMAavaHPVFbVPUnek+Tzc3QG\nAACAddk5VG4ufX0yySObGUsAAABuczuFyqq6muNA+UR3PzVvlwAAAFiLXWcqP5rkpe5+dM7OAAAA\nsC673FLk/iS/meR9VfV8VX2hqq7P3zUAAACWbustRXZuyC1FYLH2+GutJ2lnadvF/NxSZH6Oz/kZ\n4/Wwr862j+Ozj9uUnOOWIgAAAPB6hEoAAACGCZUAAAAMEyoBAAAYJlQCAAAwTKgEAABgmFAJAADA\nMKESAACAYUIlAAAAw4RKAAAAhlV3T9NQ1SQNTdifSdoBgDXw/gnL5fhkX3T3qUVophIAAIBhQiUA\nAADDhEoAAACGCZUAAAAMEyoBAAAYJlQCAAAwTKgEAABgmFAJAADAsKvbVqiqO5J8NsmbN+s/2d1/\nOnfHAAAAWL7q7u0rVf1Qd79WVW9K8k9JHu7uZ29ZZ3tDO9ilP7uoqknaAYA18P4Jy+X4ZF9096lF\nuNPlr9392ubhHTmerZzmyAAAAGDVdgqVVXWlqp5PciPJZ7r7uXm7BQAAwBrsOlP5ve5+b5JrSX6u\nqn523m4BAACwBm/o21+7+9tJ/jHJ9Xm6AwAAwJpsDZVV9faqetvm8VuSvD/JK3N3DAAAgOXbekuR\nJD+W5PGqupLjEPrJ7v67ebsFAADAGux0S5GdGnJLEQC4NN4/Ybkcn+yLc91SBAAAAE4jVAIAADBM\nqAQAAGCYUAkAAMAwoRIAAIBhQiUAAADDhEoAALhkR0dHl90FGCZUAgDAJRMqWTOhEgAAgGFCJQAA\nAMOqu6dpqGqahgAAAFic7q7Tlk8WKgEAALj9uPwVAACAYUIlAAAAw4RKAAAAhgmVAAAADBMqAQAA\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224fe208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dX = 0.1 * np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "ddelta = stats.beta.rvs(1.2, 1, size=(K,))\n",
    "dpi = np.cumprod(ddelta)\n",
    "print(ddelta, dpi)\n",
    "dZ = stats.bernoulli.rvs(dpi[:, np.newaxis], size=(K, NS))\n",
    "\n",
    "# plot states\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = np.tile(dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1) + np.sum(dC[dU] * dZ[:, dS].T, axis=1), Nrep)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcount = stats.poisson.rvs(np.exp(dlam))\n",
    "dcount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG7BJREFUeJzt3X+sXOV95/H3Jya+UEIo7S73bu1gQ6mJySYNbOqkzWqZ\nJIvBqRao1FJns0sCiJWAbVC7imKnf/j+tcXdrULalVlVcVxYhVouVYrpEhssmEoogL0LrAnXgStV\nvtje3Ml2g1hFKJaNv/vHnLHPHc+9d36cM2fOnM9Lsjj3mXPmPMO9851nvuc530cRgZmZVcP7iu6A\nmZkNj4O+mVmFOOibmVWIg76ZWYU46JuZVYiDvplZhSwb9CXtlNSQdDjV9quSXpD0iqSDkj6Remyr\npFlJRyRtTLVfL+mwpDclPZT9SzEzs+V0M9LfBdzU1vbHwLaIuA7YBvwnAEnXArcD64FNwA5JSo55\nGLg7ItYB6yS1P6eZmeVs2aAfEc8Db7c1nwEuTbZ/HjiRbN8C7I6I0xFxFJgFNkiaAi6JiEPJfo8C\ntw3YdzMz69EFfR73+8B+SX8CCPiNpH0V8EJqvxNJ22ngeKr9eNJuZmZD1O+F3HuBByLiCpofAN/O\nrktmZpaXfkf6X4qIBwAi4nFJ30raTwAfSu23OmlbrL0jSS4IZGbWh4jQUo93O9JX8q/lhKQbACR9\njmbuHmAvsFnSSklXAlcDByNiHnhH0obkwu4dwBPLdHxs/23btq3wPvi1+fX59Y3fv24sO9KX9BhQ\nA35R0ls0Z+vcA/yppBXAz4B/lwTqGUl7gBngFHBfnOvJ/cBfABcCT0XEvq56aGZmmVk26EfEv17k\noU90aoyIPwL+qEP7/wQ+2lPvzMwsU74jtwC1Wq3oLuRmnF8b+PWV3bi/vm6o2zzQMEmKUeyXmdko\nk0RkdCHXzMzGgIO+mVmFOOibmVWIg76ZWYU46JuZVYiDvplZhTjom5lViIO+mVmFOOibmVWIg76Z\nWYU46JuZVYiDvplZhTjom5lViIO+WQ6mptYyNbW26G6YnWfZoC9pp6SGpMNt7b8n6Yik1yQ9mGrf\nKmk2eWxjqv16SYclvSnpoWxfhtloaTTmaDTmiu6G2Xm6GenvAm5KN0iqAf8K+GhEfBT4z0n7euB2\nYD2wCdiRrIkL8DBwd0SsA9ZJWvCcZkWbmlqLJI/QbawtG/Qj4nng7bbme4EHI+J0ss8/JO23Arsj\n4nREHKW5YPoGSVPAJRFxKNnvUeC2DPpvlpnmyDw8Qrex1m9Ofx3wLyS9KOk5Sf8saV8FHEvtdyJp\nWwUcT7UfT9rMzGyIll0YfYnjLouIT0n6NeCvgKuy6xZMT0+f3a7Val7b0oZqamotjcYck5NrmJ8/\nWnR3zDqq1+vU6/WejulqjVxJa4AnI+Jjyc9PAdsj4u+Sn2eBTwH3AETEg0n7PmAbMAc8FxHrk/bN\nwA0Rce8i5/MauTZ0zctPAbQuQzW3+/lbbF3K8t+xDVOWa+SKc+8EgL8BPpucZB2wMiL+L7AX+F1J\nKyVdCVwNHIyIeeAdSRuSC7t3AE/09nLMzGxQy6Z3JD0G1IBflPQWzZH7t4Fdkl4DTtIM4kTEjKQ9\nwAxwCrgvNWS/H/gL4ELgqYjYl+1LMTOz5XSV3hk2p3esCNmndyaAk74uYEPTTXrHQd8skUdOf5Dn\nMOtVljl9MzMbAw76ZmYV4qBvZlYhDvpmZhXioG9mViEO+mYZalXqNBtVnrJplshiymaW0z7NeuUp\nm2Z9mSi6A2a5cdA3O8/JojtglhsHfTOzCnHQNzOrEAd9M7MKcdA3M6sQB30zswpx0Dczq5Blg76k\nnZIakg53eOw/SDoj6RdSbVslzUo6Imljqv16SYclvSnpoexegpmZdaubkf4u4Kb2RkmrgRtpLnre\nalsP3A6sBzYBO3TunvSHgbsjYh2wTtJ5z2lmZvlaNuhHxPPA2x0e+gbw1ba2W4HdEXE6Io4Cs8AG\nSVPAJRFxKNnvUeC2vnttZmZ96SunL+kW4FhEvNb20CrgWOrnE0nbKuB4qv140mZmZkN0Qa8HSLoI\n+DrN1E5upqenz27XajVqtVqepzMzK516vU69Xu/pmK6qbEpaAzwZER+T9E+BA8C7NEsJrqY5ot8A\n3AUQEQ8mx+0DttHM+z8XEeuT9s3ADRFx7yLnc5VN69rU1FoajTkmJ9cwP3+07+dxlU0ruyyrbCr5\nR0T8ICKmIuKqiLiSZqrmuoj4MbAX+F1JKyVdCVwNHIyIeeAdSRuSC7t3AE/0+brMFmg05oBI/mtm\nS+lmyuZjwPdpzrh5S9KdbbucHdZExAywB5gBngLuSw3Z7wd2Am8CsxGxL5uXYGZm3fIiKlZ66ZTK\nIH83Tu9Y2XkRFRtbrWUJp6bWFt0Vs1LxSN9KqfOI+kImJ6f6vpjrkb6VXTcjfQd9K6XFgivQd4B1\n0Leyc3rHzMwWcNA3M6sQB32zIfNFaCuSc/pWSmXO6Wc1xdSsnXP6Zma2gIO+jZkJp07MltBzlU2z\n0TIBnEz9fJJmHZ4lv+GaVZZH+lZyJxd9ZFgXTH1h1srEF3KtlJa6kNvvxdhe7/JtlXRO79v6puEL\nuVYE35FrY2t4QX/xGUEL9+/+/A76lhfP3jEzswUc9M3MKsRB3wx8EdYqo5uVs3ZKakg6nGr7Y0lH\nJL0q6a8lfTD12FZJs8njG1Pt10s6LOlNSQ9l/1LM+uelFq0quhnp7wJuamt7GvhIRHwcmAW2Aki6\nFrgdWA9sAnYka+ICPAzcHRHraC692P6cZl3xqNysf8sG/Yh4Hni7re1ARJxJfnwRWJ1s3wLsjojT\nEXGU5gfCBklTwCURcSjZ71Hgtgz6bxXkUblZ/7LI6d9FcxF0gFXAsdRjJ5K2VcDxVPvxpM3MzIZo\noDIMkv4QOBURf5lRf86anp4+u12r1ajValmfwkqkldLpdylEs3FUr9ep1+s9HdPVzVmS1gBPRsTH\nUm1fBu4BPhsRJ5O2LUBExPbk533ANmAOeC4i1iftm4EbIuLeRc7nm7NsgdaloYjg3GWi7G7OWu45\nfXOWlUGWN2eJc3/FSLoZ+CpwSyvgJ/YCmyWtlHQlcDVwMCLmgXckbUgu7N4BPNHDazEzswx0M2Xz\nMeD7NGfcvCXpTuDPgA8Az0h6WdIOgIiYAfYAMzTz/Pelhuz3AzuBN4HZiNiX+asxy0V25Zo988iK\n5to7VgqDpXcWL5rW/vz9pmm6Pa79PP47tyy54JqNjUFz+q1jl3t+B30rMxdcMzOzBRz0zcwqxEHf\nxtRE0R0oJa8CNv68Rq6NqcWXUbTOzq0E5jWGx5lH+mYGuKZRVTjom5lViIO+WdcmnOu20vM8fRsp\nixVWG5V5+unnGbd5+qPcN+uO5+lb6TQac7nnlj1DxarMQd8q59wMlXkHfqscB32rsJNDm7HS+nZh\nVjQHfau4iaEE49a3C7OiOehbieQRoE/iYGxV4jtyrURaAbqfwN/8wJicXJNxn8zKpZtFVHZKakg6\nnGq7TNLTkt6QtF/SpanHtkqalXRE0sZU+/WSDkt6U9JD2b8Us6U0PzBG665Tz/u34esmvbMLuKmt\nbQtwICKuAZ4FtgJIuha4HVgPbAJ26Nz38YeBuyNiHc1VuNqf06xihnch2axl2aAfEc8Db7c13wo8\nkmw/AtyWbN8C7I6I0xFxFJgFNkiaAi6JiEPJfo+mjjFrk93yhGa2UL85/csjogEQEfOSLk/aVwEv\npPY7kbSdBo6n2o8n7WYdtFIxnuJolrWsZu94+oOZWQn0O9JvSJqMiEaSuvlx0n4C+FBqv9VJ22Lt\ni5qenj67XavVqNVqfXbVLG2CMtXaX6wWkRlAvV6nXq/3dExXBdckrQWejIiPJj9vB34SEdslfQ24\nLCK2JBdyvwN8kmb65hngVyIiJL0IfAU4BPx34E8jYt8i53PBtYparOhXv0XW8tjuveDahZw/3bS3\nQnDDeD+44Fr5ZVJwTdJjwPdpzrh5S9KdwIPAjZLeAD6X/ExEzAB7gBngKeC+VPS+H9gJvAnMLhbw\nzcbPUt8sfNHahsullW2kjOdIv7vtTn/zHulbL1xa2czMFnDQNzOrEAd9M8vU1NRaX6MYYS64ZlZx\nU1NrMy0H4dISo80jfbORUFzxNdf6rxYHfRtpo7fiVF5TLF18zYbDQd9G2uiNQoso0ey5/JYd5/TN\nRp4L0Fl2PNI3M6sQB32zCnPKqHoc9M0qzBePq8dB38ysQhz0zcwqxEHfrKRa9zA4L2+9cGllK1x6\ndahRKqHc23a2ZZtblip3nC7t3O/7Zan/34M+p9/Dw+fSylYKjcbcIhcUJ4beF2spriyE5WugoC/p\n9yX9QNJhSd+RtFLSZZKelvSGpP2SLk3tv1XSrKQjkjYO3n0rs+VLLJRnLdvx47IQ46rvoC/pl4Df\nA66PiI/RvLv3C8AW4EBEXAM8C2xN9r8WuB1YD2wCdmi0iqrYkI1eiQVbyOUfxtGg6Z0VwMWSLgAu\nAk4AtwKPJI8/AtyWbN8C7I6I0xFxFJgFNgx4fjPLTRF1hixvfQf9iPjfwJ8Ab9EM9u9ExAFgMiIa\nyT7zwOXJIauAY6mnOJG0mRngkbUNwyDpnZ+nOapfA/wSzRH/Fzn/+7q/v5t1ZfCRtVetsuUMUmXz\nXwJ/HxE/AZD0XeA3gIakyYhoSJoCfpzsfwL4UOr41UlbR9PT02e3a7UatVptgK4aLJwaaePJqZhq\nqdfr1Ov1no7pe56+pA3ATuDXaA5RdgGHgCuAn0TEdklfAy6LiC3JhdzvAJ+kmdZ5BviVThPyPU8/\nH6M2fzo9zxya/ar6PP32OfK9ztPv9Xfc7f/vXv5mRu3vrEq6maff90g/Ig5Kehx4BTiV/PfPgUuA\nPZLuAuZoztghImYk7QFmkv3vc2Q36yzrdWvNWnxHboWM2ghs4Uh/goXz8oseufe6neXzXUgrv9++\nz+TkGhqNubP/9Ujf0roZ6TvoV8iovRnb0zvl2m4PzMX2y0HfwGUYzHLku4WtnBz0zcwqxEHfzKxC\nHPTNzCrEQd+s9FwG2brnoG9Wei6DbN1z0DczqxAH/cpxJUezKhuk4JqVUquSo9evMasij/TNSqOo\nNYN9oXicOOiblUb6LuBhfgD4QvE4cdA3K6Xiy0B4wZZyctCvLH9lt8E0GnMLvgFMTa1NFXCzUeUq\nmxXSqaJikf+fy11lM99FVPrZTi9Ck3WVzU7P2X6uTovi2HC5yqaNnNZo0N8yzIoxUNCXdKmkv5J0\nRNLrkj4p6TJJT0t6Q9J+SZem9t8qaTbZf+Pg3bci9ZPTbS380WjM59InM1vaoCP9bwJPRcR64FeB\nHwJbgAMRcQ3wLLAVIFkj93ZgPbAJ2CEnAEdCvxfk2nO6vSn+QqT1wjf1jYtBFkb/IPBKRPxyW/sP\ngRsioiFpCqhHxIclbQEiIrYn+30PmI6Ilzo8t3P6OVgsp9/vSkfp41rBYH7+aBfHFJ//dk6/qZ+F\n6Bdbpcs5/eLlndO/EvgHSbskvSzpzyX9HDAZEQ2AiJgHLk/2XwUcSx1/Immzwkyk3vSDjeQGG/Wb\n2bAMUobhAuB64P6I+B+SvkEztdP+8d7Xx/309PTZ7VqtRq1W66+XtoT0Gq8uz2BWNvV6nXq93tMx\ng6R3JoEXIuKq5Od/TjPo/zJQS6V3nouI9R3SO/uAbU7vDE/Wi2Cnv953m1ZwesfpHctPrumdJIVz\nTNK6pOlzwOvAXuDLSduXgCeS7b3AZkkrJV0JXA0c7Pf8ZrY03zFrnQxaZfMrwHckvR/4e+BOYAWw\nR9JdwBzNGTtExIykPcAMcAq4z8N5s6ykr880+RqLdeI7cisk//TOBHCSyck1i87icXqnuFTP1NRa\nGo25Bb+fwdM7537nrXswnN4pTjfpHQf9ChlGTn+553DQLy7op//ft+flB8npL9UHGy6XYTCzjlwc\nrboc9MecL+ZZJ+dSMVY1DvpjbpCbplwczWz8eI1cW9S54mhOA4yX5sVXqyaP9Cvh/Ol85Xhuy8cw\nAr4LtI0qB/1KaJVbKOa5nSaqolZZD98rMGoc9K0rgwTuc2kiBwCzojnoW1fSgdvT/czKy0Hfeubp\nfmbl5dk7ZmOteUF1cnJN0R2xEeGgbzbWhrVOgqeBloXTOzZEE0V3wHLjgF8WDvqWq4WzfRwYzIrm\noG+58jTNIuR7M96KFRd79laJOehbLjyts0j53ox35sy7OT6/5W3goC/pfZJelrQ3+fkySU9LekPS\nfkmXpvbdKmlW0hFJGwc9t40uT+s0G01ZjPQfoLkEYssW4EBEXAM8C2wFkHQtzaUT1wObgB3yULAk\nfAHWbFwMFPQlrQY+D3wr1Xwr8Eiy/QhwW7J9C7A7Ik5HxFFgFtgwyPltWHwB1vo14ZpLI2bQkf43\ngK+y8Hv8ZEQ0ACJiHrg8aV8FHEvtdyJpM7OxddIX80dM3zdnSfpNoBERr0qqLbFrX4nd6enps9u1\nWo1abalTmNnoOndXcGtBdstGvV6nXq/3dEzfC6NL+o/AvwFOAxcBlwDfBT4B1CKiIWkKeC4i1kva\nAkREbE+O3wdsi4iXOjy3F0bPyOALXxe/yHc5tos+fzm2/b7OV64Lo0fE1yPiioi4CtgMPBsR/xZ4\nEvhystuXgCeS7b3AZkkrJV0JXA0c7Pf8ZmbWuzxq7zwI7JF0FzBHc8YOETEjaQ/NmT6ngPs8nDcz\nG66+0zt5cnonO07vOL0zStt+X+cr1/SOjTdPsxs3vtfCmhz0rSNPsxs3vtfCmhz0rU2exbrMrGgO\n+tYmz2JdZlY0B/0x5SqXZtaJg/6YcpVLM+vEQd/MrEIc9M3MKsRB38ysQhz0LQe+EchsVDnoW4+6\nCei+EchsVDnol8DU1NoRKovggG798ipao8AF10qgNd8+Is6+aZZbjGKwYmmjU6CrPNtFn788235v\n56ebgmt5lFa2HLkmjpkNwumd0nBNHDMbnIN+aZxfE2e0cv1mVgZ9B31JqyU9K+l1Sa9J+krSfpmk\npyW9IWm/pEtTx2yVNCvpiKSNWbyAKms05pzuMbOeDDLSPw38QUR8BPh14H5JHwa2AAci4hrgWWAr\ngKRraS6duB7YBOyQ8xWZmZpay4oVFyPJo38zW9QgC6PPR8SryfZPgSPAauBW4JFkt0eA25LtW4Dd\nEXE6Io4Cs8CGfs8/jgZJ1zQac5w58y4QHv2b2aIymb0jaS3wceBFYDIiGtD8YJB0ebLbKuCF1GEn\nkjZLdB+sfVHXzPozcNCX9AHgceCBiPippPZJuH1Nyp2enj67XavVqNVq/XZxDLUu6gp/AJhVV71e\np16v93TMQDdnSboA+FvgexHxzaTtCFCLiIakKeC5iFgvaQsQEbE92W8fsC0iXurwvKW9Oav95qmp\nqbU0GnNMTq7p8oaq829eORfUfaPR6G4Xff7ybJf1vV0G3dycNeiUzW8DM62An9gLfDnZ/hLwRKp9\ns6SVkq4ErgYODnj+kdM+o6a1mEmvqZtWft+jeDPLUt/pHUmfBr4IvCbpFZof5V8HtgN7JN0FzNGc\nsUNEzEjaA8wAp4D7Sjucz1UzddNodBotdWsC18ixsum2xIgNxrV3MtJK47S0+p+ugbPcaxp+Gmex\n7aLPX8btos9fnu3F3geLpTete8NI71jCa9KajZdxvffFQT9Hg/2heCESq4ZRvXY1rve+OOjnaLA/\nFOfkrRr8LXm4HPTNzCrEQd/MrEIc9HPhu2TNyqrXawyt/ctysddBPxfn1773+qBmkL75cFT1eo2h\n9xswi+Wg30E+i5OcLM0fhVl+WjcfNt8Loxz8F+pt0DbKCxx5jdwOHJzN8la2FOjJ8z6ozr9zeIIV\nKy5OpnmOLgd9MytAulJsuSw+KDzJmTMw6q/L6Z1EUV/HRvlroNk4G+wC7Ohfm1hM5Uf67TVzzmn+\nUrspiTwIp5LMhu/c+z5d3LAXrWsTF2bcs/xVPuifu1Lf/otvr3a5uME/7cuW3zQrt+wGW+W7c97p\nnQz0Uyt/oU5TPM0sC9nNo+9/cDZKc/krX1o5XfoYmmVdF5Y4vpDJyaklUzz9lEReWHq5t2Pz3y76\n/GXcLvr8ZdzurfRyvzqVN29/36XP2R4TBi0jnU4lpc+Vx/oBI1laWdLNkn4o6U1JXxv2+V955RVu\nuul3uOeeB3jvvfe6OMLz682GpzmaXrHi4rNljVesuDjnEXJeN042X8ti8aO5yt780L8BDDXoS3of\n8F+Am4CPAF+Q9OFh9mH//v0cOHCKXbv+Kz/72c+6PKq8V+qHr150B6zUmqnOM2fePVvW+MyZdzMb\neHVeRDyvgV3nO/NbH2bpfdrPn04HZZ0aGvZIfwMwGxFzEXEK2A3cOuQ+IH2Y5pru3er8i+nfirZf\n/DipF90Bq7j0NOiFgXKCz3zmxiXfd/kP7E6e/TBbaOG3jXRph6zLPAx79s4q4Fjq5+M0PwhGxLBm\n0byX+sWPY+A3K04zOHZ6L7dm2iz+visulXvybKpncnJNqj379a4rN3vn/e9/Pxdc8DgRp7jqqmvb\nHl1uFo2LppkVo9cUa68z4kZh2nQrozDf1patoc7ekfQpYDoibk5+3gJERGxv22/0phSZmZXAcrN3\nhh30VwBvAJ8DfgQcBL4QEUeG1gkzswobak4/It6T9O+Bp2mmlnY64JuZDc9I3pxlZmb5GMkLuZJ+\nW9IPJL0n6fqi+5OVom9My5OknZIakg4X3Zc8SFot6VlJr0t6TdJXiu5TViRNSHpJ0ivJa9tWdJ/y\nIOl9kl6WtLfovmRN0lFJ/yv5HR5cat+RDPrAa8BvAX9XdEeyMgo3puVsF83XNq5OA38QER8Bfh24\nf1x+fxFxEvhMRFwHfBzYJGmEplJn5gFgpuhO5OQMUIuI6yJiyd/dSAb9iHgjImYZr0nsI3FjWl4i\n4nng7aL7kZeImI+IV5PtnwJHaN53MhYiorXc0wTNa31jlfeVtBr4PPCtovuSE9FlPB/JoD+mOt2Y\nNjZBo0okraU5In6p2J5kJ0l9vALMA89ExKGi+5SxbwBfZcw+zFICeEbSIUn3LLVjYfX0JT0DTKab\naHb8DyPiyWJ6ZbY0SR8AHgceSEb8YyEizgDXSfog8DeSro2IsUiFSPpNoBERr0qqMV4ZhJZPR8SP\nJP1jmsH/SPLt+zyFBf2IuLGocxfkBHBF6ufVSZuVhJoFmx4H/ltEPFF0f/IQEf9P0nPAzYxP/vvT\nwC2SPg9cBFwi6dGIuKPgfmUmIn6U/Pf/SPouzXRyx6BfhvTOuHwqHwKulrRG0kpgMzBuswjE+Py+\nOvk2MBMR3yy6I1mS9I8kXZpsXwTcCPyw2F5lJyK+HhFXRMRVNN93z45TwJf0c8k3UCRdDGwEfrDY\n/iMZ9CXdJukY8CngbyV9r+g+DSoi3gNaN6a9DuwepxvTJD0GfB9YJ+ktSXcW3acsSfo08EXgs8m0\nuJcl3Vx0vzLyT4DnJL1K8zrF/oh4quA+WfcmgeeTazIvAk9GxNOL7eybs8zMKmQkR/pmZpYPB30z\nswpx0DczqxAHfTOzCnHQNzOrEAd9M7MKcdA3M6sQB30zswr5/yZJ0TfFBCphAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122598390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dlam, bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128820be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.exp(dlam), bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAfJ6lNIuWa/7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1287d4c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount.copy()\n",
    "Xdat = np.tile(dX[:, dS], (1, Nrep)).T\n",
    "unit = np.tile(dU, Nrep)\n",
    "stim = np.tile(dS, Nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we'll need to know the number of observations of each (stim, unit) pair. This is constant for this toy example, but need not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({'stim': stim, 'unit': unit, 'count': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim</th>\n",
       "      <th>unit</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim  unit  count\n",
       "0     0     0     40\n",
       "1     0     1     40\n",
       "2     0     2     40\n",
       "3     0     3     40\n",
       "4     0     4     40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of occurrences of each pair\n",
    "pair_counts_df_long = pairs.groupby(['stim', 'unit']).sum().reset_index()\n",
    "pair_counts_df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair_counts = pair_counts_df_long.pivot(values='count', index='stim', columns='unit').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to know a unique code for each *possible* (stim, unit) pair. (Some might be missing, but in this case, we still want to code that, since we'll be using these codes to sum over all observations, and the resulting matrix may have some 0 entries, but it must be dense.\n",
    "\n",
    "We will assume that units and stims are consecutively numbered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "su_codes = NU * stim + unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GLM to get inits\n",
    "\n",
    "Here, we run a GLM on the observed data with $A$ and $B$ included (but not $C \\cdot Z$) to get a rough starting point. This takes time, but is a net win in terms of saving SGD iterations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "Xdf = pd.DataFrame(Xdat)\n",
    "Xdf.columns = ['X' + str(c) for c in Xdf.columns]\n",
    "dta = pd.concat([pd.DataFrame({'count': count, 'unit': unit, 'stim': stim}), Xdf], axis=1)\n",
    "formula = 'count ~ -1 + C(unit) + C(unit) * (' + '+'.join(Xdf.columns) + ')'\n",
    "mod = smf.glm(formula=formula, data=dta, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the model's fitted parameters into inits for $A$ and $B$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A_init = mod.params.values[:NU].astype('float32')\n",
    "B_init = mod.params.values[NU:].reshape(P, NU).T.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))\n",
    "NSU = tf.constant(pair_counts.astype('float32'))\n",
    "codes_SU = tf.constant(su_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100000), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.train.range_input_producer(N).dequeue_many(NB, name='batch_inds')\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB, name='batch_counts')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = N\n",
    "batch_inds = np.arange(N)\n",
    "batch_counts = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    muA = ed.models.Normal(mu=np.log(25) * tf.ones(1), sigma=0.5 * tf.ones(1), name='mu_A')\n",
    "    muB = ed.models.Normal(mu=tf.zeros(P), sigma=0.5 * tf.ones(P), name='mu_B')\n",
    "    muC = ed.models.Normal(mu=tf.zeros(K), sigma=0.5 * tf.ones(K), name='mu_C')\n",
    "    sigA = ed.models.Gamma(alpha=2. * tf.ones(1), beta=4. * tf.ones(1), name='sig_A')\n",
    "    sigB = ed.models.Gamma(alpha=2. * tf.ones(P), beta=4. * tf.ones(P), name='sig_B')\n",
    "    sigC = ed.models.Gamma(alpha=2. * tf.ones(K), beta=4. * tf.ones(K), name='sig_C') \n",
    "\n",
    "    A = ed.models.Normal(mu=tf.tile(muA, (NU,)), \n",
    "                         sigma=tf.tile(sigA, (NU,)), \n",
    "                         name='A')\n",
    "    B = ed.models.Normal(mu=tf.tile(tf.expand_dims(muB, 0), (NU, 1)), \n",
    "                         sigma=tf.tile(tf.expand_dims(sigB, 0), (NU, 1)), \n",
    "                         name='B')\n",
    "    C = ed.models.Normal(mu=tf.tile(tf.expand_dims(muC, 0), (NU, 1)), \n",
    "                         sigma=tf.tile(tf.expand_dims(sigC, 0), (NU, 1)), \n",
    "                         name='C')  \n",
    "\n",
    "    a_prior = tf.ones(K)\n",
    "    b_prior = tf.ones(K)\n",
    "    pi = ed.models.Beta(a=a_prior, b=b_prior, name='pi')\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-7.0], sigma=[1.], name='sig')\n",
    "\n",
    "    lam_vars = (tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, U) * tf.gather(tf.to_float(Z), S), 1))\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(NB), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    # population means\n",
    "    q_muA = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(np.log(25) * tf.ones(1)), \n",
    "                                              sigma=tf.Variable(0.5 * tf.ones(1)),\n",
    "                                              name='mu_A')\n",
    "    tf.scalar_summary('q_muA', tf.reduce_mean(q_muA.mean()))\n",
    "\n",
    "    q_muB = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.zeros(P)), \n",
    "                                              sigma=tf.Variable(0.5 * tf.ones(P)),\n",
    "                                              name='mu_B')\n",
    "    tf.scalar_summary('q_muB', tf.reduce_mean(q_muB.mean()))\n",
    "\n",
    "    q_muC = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.zeros(K)), \n",
    "                                              sigma=tf.Variable(0.5 * tf.ones(K)),\n",
    "                                              name='mu_C')\n",
    "    tf.scalar_summary('q_muC', tf.reduce_mean(q_muC.mean()))\n",
    "\n",
    "\n",
    "    # population standard deviations\n",
    "    q_sigA = ed.models.GammaWithSoftplusAlphaBeta(alpha=tf.Variable(2. * tf.ones(1)), \n",
    "                                                  beta=tf.Variable(4. * tf.ones(1)),\n",
    "                                                  name='sig_A')\n",
    "    tf.scalar_summary('q_sigA', tf.reduce_mean(q_sigA.mean()))\n",
    "\n",
    "    q_sigB = ed.models.GammaWithSoftplusAlphaBeta(alpha=tf.Variable(2. * tf.ones(P)), \n",
    "                                                  beta=tf.Variable(4. * tf.ones(P)),\n",
    "                                                  name='sig_B')\n",
    "    tf.scalar_summary('q_sigB', tf.reduce_mean(q_sigB.mean()))\n",
    "\n",
    "    q_sigC = ed.models.GammaWithSoftplusAlphaBeta(alpha=tf.Variable(2. * tf.ones(K)), \n",
    "                                                  beta=tf.Variable(4. * tf.ones(K)),\n",
    "                                                  name='sig_C')\n",
    "    tf.scalar_summary('q_sigC', tf.reduce_mean(q_sigC.mean()))\n",
    "\n",
    "    \n",
    "    # individual unit coefficients\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    tf.scalar_summary('q_A', tf.reduce_mean(q_A.mean()))\n",
    "\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    tf.scalar_summary('q_B', tf.reduce_mean(q_B.mean()))\n",
    "\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    tf.scalar_summary('q_C', tf.reduce_mean(q_C.mean()))\n",
    "    \n",
    "    # latent variables for each stimulus\n",
    "    Zlogit = tf.Variable(-0.0 + tf.zeros((NS, K)), name='Z_logit_clip')\n",
    "    tf.scalar_summary('max_Z_logit', tf.reduce_max(Zlogit))\n",
    "    tf.scalar_summary('min_Z_logit', tf.reduce_min(Zlogit))\n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=Zlogit, name='Z')\n",
    "    tf.scalar_summary('q_Z', tf.reduce_mean(q_Z.mean()))\n",
    "    tf.scalar_summary('max_q_Z', tf.reduce_max(q_Z.mean()))\n",
    "    tf.scalar_summary('min_q_Z', tf.reduce_min(q_Z.mean()))\n",
    "\n",
    "    # latent variables for stimulus category\n",
    "    a_var = tf.Variable(1. + tf.random_uniform((K,)))\n",
    "    b_var = tf.Variable(1. + tf.random_uniform((K,)))\n",
    "    tf.scalar_summary('max_a', tf.reduce_max(a_var))\n",
    "    tf.scalar_summary('max_b', tf.reduce_max(b_var))\n",
    "    tf.scalar_summary('min_a', tf.reduce_min(a_var))\n",
    "    tf.scalar_summary('min_b', tf.reduce_min(b_var))\n",
    "    q_pi = ed.models.Beta(a=a_var, b=b_var, name='pi')\n",
    "    tf.scalar_summary('min_q_pi', tf.reduce_min(q_pi.value()))\n",
    "    tf.scalar_summary('max_q_pi', tf.reduce_max(q_pi.value()))\n",
    "\n",
    "    # log firing rates\n",
    "    lam_mu = tf.Variable(2 + tf.random_normal((N,)))\n",
    "    tf.scalar_summary('lam_mu_mean', tf.reduce_mean(tf.gather(lam_mu, batch_inds)))\n",
    "    lam_sig = tf.Variable(3 * tf.random_uniform((N,)) + 2)\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_ELBO(latent_vars, data, scale):\n",
    "    from edward.util import copy\n",
    "    p_log_prob = 0.0\n",
    "    q_log_prob = 0.0\n",
    "    z_sample = {}\n",
    "    scope = \"ELBO\"\n",
    "\n",
    "    for z, qz in latent_vars.items():\n",
    "        # Copy q(z) to obtain new set of posterior samples.\n",
    "        qz_copy = copy(qz, scope=scope)\n",
    "        z_sample[z] = qz_copy.value()\n",
    "        z_log_prob = tf.reduce_sum(qz.log_prob(tf.stop_gradient(z_sample[z])))\n",
    "        if z in scale:\n",
    "            z_log_prob *= scale[z]\n",
    "\n",
    "        q_log_prob += z_log_prob\n",
    "\n",
    "    dict_swap = z_sample\n",
    "    for x, qx in data.items():\n",
    "        if isinstance(x, ed.RandomVariable):\n",
    "            if isinstance(qx, ed.RandomVariable):\n",
    "                qx_copy = copy(qx, scope=scope)\n",
    "                dict_swap[x] = qx_copy.value()\n",
    "            else:\n",
    "                dict_swap[x] = qx\n",
    "\n",
    "            for z in latent_vars.keys():\n",
    "                z_copy = copy(z, dict_swap, scope=scope)\n",
    "                z_log_prob = tf.reduce_sum(z_copy.log_prob(dict_swap[z]))\n",
    "                if z in scale:\n",
    "                    z_log_prob *= scale[z]\n",
    "\n",
    "                p_log_prob += z_log_prob\n",
    "\n",
    "            for x in data.keys():\n",
    "                if isinstance(x, ed.RandomVariable):\n",
    "                    x_copy = copy(x, dict_swap, scope=scope)\n",
    "                    x_log_prob = tf.reduce_sum(x_copy.log_prob(dict_swap[x]))\n",
    "                if x in scale:\n",
    "                    x_log_prob *= scale[x]\n",
    "\n",
    "                p_log_prob += x_log_prob\n",
    "\n",
    "    return tf.reduce_mean(p_log_prob - q_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = make_ELBO({A: q_A, B: q_B, C: q_C, \n",
    "                  muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                  sigA: q_sigA, sigB: q_sigB, sigC: q_sigC,\n",
    "                  Z: q_Z, sig: q_sig, pi: q_pi, lam: q_lam}, \n",
    "                 {cnt: tf.cast(batch_counts, 'float32')}, \n",
    "                 {lam: N/NB, cnt: N/NB})\n",
    "\n",
    "tf.scalar_summary('ELBO', elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $z_{sk} \\sim \\mathrm{Ber}(\\pi_k)$ and $\\log \\Lambda \\equiv \\lambda_{us} \\sim \\mathcal{N}(A_u + (B \\cdot X)_{us} + (C \\cdot Z)_{us}, \\sigma^2)$, we can calculate closed-form updates for $Z$. However, above, we have flattened $\\lambda$ into a single vector for each trial, with vectors $S$ and $U$ listing the stimulus and unit corresponding to each observation.\n",
    "\n",
    "Collecting the pieces involving $\\xi_{ks} = \\mathbb{E}[Z_{ks}]$ in the ELBO (prior to expectations wrt $q(A, B, C, \\lambda)$), we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &=\n",
    "\\sum_{ks}\\xi_{ks}\\log \\frac{\\pi_k}{1 - \\pi_k} + \\mathcal{M} + \\sum_{ks} \\mathcal{H}(\\xi_{ks})\\\\\n",
    "\\mathcal{H}(\\xi_{ks}) &= -\\xi_{ks}\\log \\xi_{ks} - (1 - \\xi_{ks})\\log (1 - \\xi_{ks}) \\\\\n",
    "\\mathcal{M} &= -\\frac{1}{2\\sigma^2}\\sum_{u,s,j=1\\ldots N_{us}} \\mathbb{E}_z \n",
    "\\left[\n",
    "-2(C \\cdot Z)_{us}(\\lambda^{(j)}_{us} - A_u - (B\\cdot X)_{us}) + (C \\cdot Z)_{us}^2\n",
    "\\right] \\\\\n",
    "&= -\\frac{1}{2\\sigma^2}\\sum_{usj} \\left[\n",
    "-2(C \\cdot \\xi)_{us} (\\lambda^{(j)}_{us} - A_u - (B\\cdot X)_{us}) \n",
    "+ \\sum_{k\\neq k'} C_{uk}C_{uk'} \\xi_{ks}\\xi_{k's} + \\sum_k C^2_{uk}\\xi_{ks}\n",
    "\\right] \\\\\n",
    "&= -\\frac{1}{2\\sigma^2}\\sum_{usj} \\left[\n",
    "-2(C \\cdot \\xi)_{us} (\\lambda^{(j)}_{us} - A_u - (B\\cdot X)_{us}) \n",
    "+ (C \\cdot \\xi)^2_{us} + \\sum_k C^2_{uk}\\xi_{ks}(1 - \\xi_{ks})\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the derivation expands the square in the log of the normal distribution and makes use of the fact that $z_{ks} \\in \\{ 0, 1\\} \\Rightarrow z_{ks}^2 = z_{ks}$. In addition, we have let $N_{us}$ be the number of observations of stimulus $s$ by unit $u$, indexed by $j$.\n",
    "\n",
    "Given this result, differentiating wrt $\\xi$ gives the result\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\frac{\\xi_{ks}}{1 - \\xi_{ks}} &= \n",
    "\\log \\frac{\\pi_k}{1 - \\pi_k}\n",
    "-\\frac{1}{2\\sigma^2} \\sum_{uj} \\left[\n",
    "-2C_{uk} (\\lambda^{(j)}_{us} - A_u - (B\\cdot X)_{us} - (C \\cdot \\xi_{ks})) \n",
    "+ C^2_{uk}(1 - 2\\xi_{ks})\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that despite appearances, the right hand side does *not* depend on $\\xi_{ks}$. This means that we can \n",
    "solve for $\\xi_{ks}$ given $\\xi_{k's}$ for $k' \\neq k$. Solving for all these equations simultaneously is not possible in general in closed form, but it leaves two options:\n",
    "1. Solve for all $\\xi^{(t)}_{ks}$ given $\\xi^{(t - 1)}_{ks}$, the values at the previous iteration.\n",
    "1. Iterate through $k$, updating each $\\xi_{ks}$ in turn given the current value of the others. In this method, the first $k$ chosen uses the values from the previous iteration, while the last $k$ uses updated values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if we make the assumption that the posterior over $C$ is normally distributed, we can take the expectation of the above update rule wrt $C$. (The expectations wrt $A$, $B$, and $\\lambda$ are trivial. Since each only appears linearly, we only need to know the mean of the approximate posterior. For $\\sigma$, we would need a posterior that allows us to compute $\\mathbb{E}[\\sigma^{-2}]$.)\n",
    "\n",
    "In this case, it is straightforward to work out that\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\frac{\\xi_{ks}}{1 - \\xi_{ks}} &= \n",
    "\\log \\frac{\\pi_k}{1 - \\pi_k}\n",
    "-\\frac{1}{2\\sigma^2} \\sum_{uj} \\left[\n",
    "-2\\overline{C}_{uk} (\\lambda^{(j)}_{us} - A_u - (B\\cdot X)_{us} - (\\overline{C} \\cdot \\xi_{ks})) \n",
    "+ \\overline{C}^2_{uk}(1 - 2\\xi_{ks}) + \\mathrm{var}[C]_{uk}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "using \n",
    "$$\n",
    "\\mathbb{E}_z \\left[\n",
    "2 C_{uk}(C \\cdot \\xi)_{us} + C^2_{uk}(1 - 2\\xi_{ks})\n",
    "\\right]\n",
    "=\n",
    "2 \\overline{C}_{uk}(\\overline{C}\\cdot \\xi)_{us} + \\overline{C}^2_{uk}(1 - 2\\xi_{ks}) + \\mathrm{var}[C]_{uk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_z():\n",
    "    # update binary feature matrix based on exact conjugate update\n",
    "    with tf.variable_scope(\"update_z\"):\n",
    "        # logit piece\n",
    "        pi_val = q_pi.value()\n",
    "        logit_pi = tf.log(pi_val) - tf.log(1. - pi_val)\n",
    "        \n",
    "        # C^2 piece\n",
    "        C = q_C.value()  # NU x K\n",
    "        quad_piece = tf.matmul(NSU, tf.square(C), name='quadratic_piece')\n",
    "        \n",
    "        # sample log firing rate values\n",
    "        lam_val = q_lam.value()  \n",
    "        \n",
    "        # now get \\lam_{su} summed over observations\n",
    "        lam_su_flat = tf.unsorted_segment_sum(lam_val, codes_SU, NS * NU, name='lam_collect')\n",
    "        lam_su = tf.reshape(lam_su_flat, (NS, NU), name='lam_su')\n",
    "        \n",
    "        # get A and BX (summed over observations)\n",
    "        # recall that X is in \"long\" format, and so must \n",
    "        # be handled with unsorted_segment_sum, like lambda\n",
    "        A = tf.expand_dims(q_A.value(), 0) * NSU\n",
    "        BX_long = tf.reduce_sum(tf.gather(B, U) * X, 1)\n",
    "        BX_summed = tf.unsorted_segment_sum(BX_long, codes_SU, NS * NU)\n",
    "        BX = tf.reshape(BX_summed, (NS, NU), name='BX')\n",
    "        \n",
    "        # lam_su * C_{uk}\n",
    "        lin_piece = tf.matmul(lam_su - A - BX, C, name='linear_piece')\n",
    "        \n",
    "        prec = 1/tf.square(q_sig.value())\n",
    "        \n",
    "        z_coeff = logit_pi - prec * (0.5 * quad_piece - lin_piece)\n",
    "        \n",
    "        update_z_op = Zlogit.assign(z_coeff)\n",
    "        \n",
    "        return update_z_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_pi():\n",
    "    # update probabilites for features based on exact conjugate update\n",
    "    with tf.variable_scope(\"update_pi\"):\n",
    "        xi = q_Z.mean()\n",
    "        new_a = tf.reduce_sum(xi, 0) + a_prior\n",
    "        new_b = tf.reduce_sum(1. - xi, 0) + b_prior\n",
    "        update_a_op = a_var.assign(new_a)\n",
    "        update_b_op = b_var.assign(new_b)\n",
    "        \n",
    "    return [update_a_op, update_b_op]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference_lam = ed.KLqp({lam: q_lam, sig: q_sig}, \n",
    "                        data={cnt: batch_counts, \n",
    "                              Z: q_Z, pi: q_pi, \n",
    "                              A: q_A, B: q_B, C: q_C,\n",
    "                              muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                              sigA: q_sigA, sigB: q_sigB, sigC: q_sigC})\n",
    "inference_coeffs = ed.KLqp({A: q_A, B: q_B, C: q_C}, \n",
    "                           data={cnt: batch_counts, \n",
    "                                 lam: q_lam, sig: q_sig,\n",
    "                                 Z: q_Z, pi: q_pi, \n",
    "                                 muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                                 sigA: q_sigA, sigB: q_sigB, sigC: q_sigC})\n",
    "inference_pop = ed.KLqp({muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                        sigA: q_sigA, sigB: q_sigB, sigC: q_sigC},\n",
    "                       data={cnt: batch_counts, \n",
    "                             lam: q_lam, sig: q_sig,\n",
    "                             Z: q_Z, pi: q_pi, \n",
    "                             A: q_A, B: q_B, C: q_C})\n",
    "inference_latents = update_z()\n",
    "inference_probs = update_pi()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference_lam = ed.KLqp({lam: q_lam, sig: q_sig}, \n",
    "                        data={cnt: batch_counts, \n",
    "                              Z: q_Z, pi: q_pi, \n",
    "                              A: q_A, B: q_B, C: q_C,\n",
    "                              muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                              sigA: q_sigA, sigB: q_sigB, sigC: q_sigC})\n",
    "inference_coeffs = ed.KLqp({A: q_A, B: q_B, C: q_C, \n",
    "                            muA: q_muA, muB: q_muB, muC: q_muC,\n",
    "                            sigA: q_sigA, sigB: q_sigB, sigC: q_sigC}, \n",
    "                           data={cnt: batch_counts, \n",
    "                                 lam: q_lam, sig: q_sig,\n",
    "                                 Z: q_Z, pi: q_pi})\n",
    "\n",
    "inference_latents = update_z()\n",
    "inference_probs = update_pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- The learning rate is a difficult tradeoff: 1e-2 drastically speeds convergence but can run into NaNs; 1e-3 (the default) is much slower.\n",
    "    - **TO DO**: Does regularizing emergence of `NaN`s help with this?\n",
    "- I'm currently using \"all\" the data, which appears to be faster (run-time, wise) than using minibatches. (Not entirely sure why this is, except perhaps that switching data into and out of the graph has a cost.) I've also found that minibatches need to be fairly substantial to be effective, since most variables ($\\lambda$, $A$, $B$, $C$) are unit-specific (i.e., local), so unless you have several observations from that unit, convergence can be slow.\n",
    "    - Ultimately, it might speed things to have a smarter minibatch selection (i.e., all observations for a single unit) when updating the local variables.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. \n",
    "- I've noticed no difference below in how many steps one takes along each coordinate before switching (number of inner loop iterations), either in runtime or convergence. Perhaps this matters in the final stages, but I would suspect that then it favors tighter inner loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize each (Edward) inference step\n",
    "debug = False\n",
    "inf_list = [inference_lam, inference_coeffs, inference_pop]\n",
    "for inf in inf_list:\n",
    "    if inf is inference_lam:\n",
    "        logdir = 'data/run1'\n",
    "    else:\n",
    "        logdir = None\n",
    "        \n",
    "    inf.initialize(n_print=100, n_samples=1,  \n",
    "                  logdir=logdir,\n",
    "                  optimizer=tf.train.AdamOptimizer(1e-1),\n",
    "                  scale={lam: N/NB, cnt: N/NB},\n",
    "                  debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "init.run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 240100 [24010%]: Loss = 373069.219\n",
      "Iteration 240200 [24020%]: Loss = 364673.438\n",
      "Iteration 240300 [24030%]: Loss = 359114.125\n",
      "Iteration 240400 [24040%]: Loss = 362114.875\n",
      "Iteration 240500 [24050%]: Loss = 363884.562\n",
      "Iteration 240600 [24060%]: Loss = 356831.531\n",
      "Iteration 240700 [24070%]: Loss = 361441.062\n",
      "Iteration 240800 [24080%]: Loss = 357246.312\n",
      "Iteration 240900 [24090%]: Loss = 370905.938\n",
      "Iteration 241000 [24100%]: Loss = 366204.531\n",
      "Iteration 241100 [24110%]: Loss = 401237.938\n",
      "Iteration 241200 [24120%]: Loss = 368650.875\n",
      "Iteration 241300 [24130%]: Loss = 372243.281\n",
      "Iteration 241400 [24140%]: Loss = 382306.438\n",
      "Iteration 241500 [24150%]: Loss = 359443.562\n",
      "Iteration 241600 [24160%]: Loss = 366023.562\n",
      "Iteration 241700 [24170%]: Loss = 369442.594\n",
      "Iteration 241800 [24180%]: Loss = 360584.750\n",
      "Iteration 241900 [24190%]: Loss = 377516.281\n",
      "Iteration 242000 [24200%]: Loss = 360623.219\n",
      "Iteration 242100 [24210%]: Loss = 378490.688\n",
      "Iteration 242200 [24220%]: Loss = 361778.656\n",
      "Iteration 242300 [24230%]: Loss = 364259.906\n",
      "Iteration 242400 [24240%]: Loss = 371037.406\n",
      "Iteration 242500 [24250%]: Loss = 367244.938\n",
      "Iteration 242600 [24260%]: Loss = 368146.062\n",
      "Iteration 242700 [24270%]: Loss = 369605.969\n",
      "Iteration 242800 [24280%]: Loss = 388535.656\n",
      "Iteration 242900 [24290%]: Loss = 367427.719\n",
      "Iteration 243000 [24300%]: Loss = 503602.906\n",
      "Iteration 243100 [24310%]: Loss = 827618.250\n",
      "Iteration 243200 [24320%]: Loss = 375123.000\n",
      "Iteration 243300 [24330%]: Loss = 369351.344\n",
      "Iteration 243400 [24340%]: Loss = 369688.250\n",
      "Iteration 243500 [24350%]: Loss = 360218.812\n",
      "Iteration 243600 [24360%]: Loss = 364356.719\n",
      "Iteration 243700 [24370%]: Loss = 361698.156\n",
      "Iteration 243800 [24380%]: Loss = 365052.844\n",
      "Iteration 243900 [24390%]: Loss = 364532.844\n",
      "Iteration 244000 [24400%]: Loss = 370457.375\n",
      "Iteration 244100 [24410%]: Loss = 360827.750\n",
      "Iteration 244200 [24420%]: Loss = 365165.281\n",
      "Iteration 244300 [24430%]: Loss = 365988.188\n",
      "Iteration 244400 [24440%]: Loss = 359247.625\n",
      "Iteration 244500 [24450%]: Loss = 356980.375\n",
      "Iteration 244600 [24460%]: Loss = 366367.500\n",
      "Iteration 244700 [24470%]: Loss = 358667.750\n",
      "Iteration 244800 [24480%]: Loss = 398092.531\n",
      "Iteration 244900 [24490%]: Loss = 383321.094\n",
      "Iteration 245000 [24500%]: Loss = 366263.750\n",
      "Iteration 245100 [24510%]: Loss = 369882.844\n",
      "Iteration 245200 [24520%]: Loss = 376142.938\n",
      "Iteration 245300 [24530%]: Loss = 361494.094\n",
      "Iteration 245400 [24540%]: Loss = 365726.844\n",
      "Iteration 245500 [24550%]: Loss = 361686.844\n",
      "Iteration 245600 [24560%]: Loss = 357247.406\n",
      "Iteration 245700 [24570%]: Loss = 366181.281\n",
      "Iteration 245800 [24580%]: Loss = 419309.500\n",
      "Iteration 245900 [24590%]: Loss = 365488.125\n",
      "Iteration 246000 [24600%]: Loss = 360860.125\n",
      "Iteration 246100 [24610%]: Loss = 411435.031\n",
      "Iteration 246200 [24620%]: Loss = 358970.125\n",
      "Iteration 246300 [24630%]: Loss = 401935.875\n",
      "Iteration 246400 [24640%]: Loss = 383127.156\n",
      "Iteration 246500 [24650%]: Loss = 410202.625\n",
      "Iteration 246600 [24660%]: Loss = 390853.188\n",
      "Iteration 246700 [24670%]: Loss = 359083.500\n",
      "Iteration 246800 [24680%]: Loss = 363118.750\n",
      "Iteration 246900 [24690%]: Loss = 365073.656\n",
      "Iteration 247000 [24700%]: Loss = 360269.625\n",
      "Iteration 247100 [24710%]: Loss = 400681.750\n",
      "Iteration 247200 [24720%]: Loss = 372158.656\n",
      "Iteration 247300 [24730%]: Loss = 386797.812\n",
      "Iteration 247400 [24740%]: Loss = 390985.906\n",
      "Iteration 247500 [24750%]: Loss = 385075.250\n",
      "Iteration 247600 [24760%]: Loss = 377520.375\n",
      "Iteration 247700 [24770%]: Loss = 381595.688\n",
      "Iteration 247800 [24780%]: Loss = 368456.281\n",
      "Iteration 247900 [24790%]: Loss = 376587.688\n",
      "Iteration 248000 [24800%]: Loss = 367358.344\n",
      "Iteration 248100 [24810%]: Loss = 358342.062\n",
      "Iteration 248200 [24820%]: Loss = 358447.531\n",
      "Iteration 248300 [24830%]: Loss = 359126.125\n",
      "Iteration 248400 [24840%]: Loss = 366199.875\n",
      "Iteration 248500 [24850%]: Loss = 365951.031\n",
      "Iteration 248600 [24860%]: Loss = 379276.312\n",
      "Iteration 248700 [24870%]: Loss = 361749.219\n",
      "Iteration 248800 [24880%]: Loss = 364268.531\n",
      "Iteration 248900 [24890%]: Loss = 366348.062\n",
      "Iteration 249000 [24900%]: Loss = 377634.125\n",
      "Iteration 249100 [24910%]: Loss = 370705.094\n",
      "Iteration 249200 [24920%]: Loss = 363393.875\n",
      "Iteration 249300 [24930%]: Loss = 360363.125\n",
      "Iteration 249400 [24940%]: Loss = 374359.750\n",
      "Iteration 249500 [24950%]: Loss = 369132.000\n",
      "Iteration 249600 [24960%]: Loss = 369789.312\n",
      "Iteration 249700 [24970%]: Loss = 373163.500\n",
      "Iteration 249800 [24980%]: Loss = 363704.031\n",
      "Iteration 249900 [24990%]: Loss = 375859.625\n",
      "Iteration 250000 [25000%]: Loss = 363840.625\n",
      "Iteration 250100 [25010%]: Loss = 368218.906\n",
      "Iteration 250200 [25020%]: Loss = 375785.406\n",
      "Iteration 250300 [25030%]: Loss = 367985.469\n",
      "Iteration 250400 [25040%]: Loss = 363420.312\n",
      "Iteration 250500 [25050%]: Loss = 368740.000\n",
      "Iteration 250600 [25060%]: Loss = 368061.188\n",
      "Iteration 250700 [25070%]: Loss = 366822.156\n",
      "Iteration 250800 [25080%]: Loss = 366715.094\n",
      "Iteration 250900 [25090%]: Loss = 371273.156\n",
      "Iteration 251000 [25100%]: Loss = 379397.844\n",
      "Iteration 251100 [25110%]: Loss = 403191.062\n",
      "Iteration 251200 [25120%]: Loss = 373512.250\n",
      "Iteration 251300 [25130%]: Loss = 375595.750\n",
      "Iteration 251400 [25140%]: Loss = 371487.375\n",
      "Iteration 251500 [25150%]: Loss = 364780.188\n",
      "Iteration 251600 [25160%]: Loss = 365705.969\n",
      "Iteration 251700 [25170%]: Loss = 360319.406\n",
      "Iteration 251800 [25180%]: Loss = 363336.562\n",
      "Iteration 251900 [25190%]: Loss = 370181.250\n",
      "Iteration 252000 [25200%]: Loss = 366004.750\n",
      "Iteration 252100 [25210%]: Loss = 362780.750\n",
      "Iteration 252200 [25220%]: Loss = 361648.375\n",
      "Iteration 252300 [25230%]: Loss = 377007.062\n",
      "Iteration 252400 [25240%]: Loss = 379261.375\n",
      "Iteration 252500 [25250%]: Loss = 383332.531\n",
      "Iteration 252600 [25260%]: Loss = 401260.625\n",
      "Iteration 252700 [25270%]: Loss = 436229.594\n",
      "Iteration 252800 [25280%]: Loss = 492014.844\n",
      "Iteration 252900 [25290%]: Loss = 454908.438\n",
      "Iteration 253000 [25300%]: Loss = 400784.344\n",
      "Iteration 253100 [25310%]: Loss = 457920.719\n",
      "Iteration 253200 [25320%]: Loss = 436012.000\n",
      "Iteration 253300 [25330%]: Loss = 418002.031\n",
      "Iteration 253400 [25340%]: Loss = 457632.094\n",
      "Iteration 253500 [25350%]: Loss = 425357.000\n",
      "Iteration 253600 [25360%]: Loss = 391634.875\n",
      "Iteration 253700 [25370%]: Loss = 385880.906\n",
      "Iteration 253800 [25380%]: Loss = 377998.406\n",
      "Iteration 253900 [25390%]: Loss = 375915.156\n",
      "Iteration 254000 [25400%]: Loss = 386356.969\n",
      "Iteration 254100 [25410%]: Loss = 368112.781\n",
      "Iteration 254200 [25420%]: Loss = 366425.438\n",
      "Iteration 254300 [25430%]: Loss = 369615.250\n",
      "Iteration 254400 [25440%]: Loss = 364095.844\n",
      "Iteration 254500 [25450%]: Loss = 363730.719\n",
      "Iteration 254600 [25460%]: Loss = 362038.312\n",
      "Iteration 254700 [25470%]: Loss = 363499.062\n",
      "Iteration 254800 [25480%]: Loss = 366669.094\n",
      "Iteration 254900 [25490%]: Loss = 366126.906\n",
      "Iteration 255000 [25500%]: Loss = 388833.625\n",
      "Iteration 255100 [25510%]: Loss = 373339.375\n",
      "Iteration 255200 [25520%]: Loss = 366424.312\n",
      "Iteration 255300 [25530%]: Loss = 362924.750\n",
      "Iteration 255400 [25540%]: Loss = 373066.281\n",
      "Iteration 255500 [25550%]: Loss = 375214.750\n",
      "Iteration 255600 [25560%]: Loss = 407022.625\n",
      "Iteration 255700 [25570%]: Loss = 381395.594\n",
      "Iteration 255800 [25580%]: Loss = 412072.125\n",
      "Iteration 255900 [25590%]: Loss = 429915.000\n",
      "Iteration 256000 [25600%]: Loss = 434497.562\n",
      "Iteration 256100 [25610%]: Loss = 401136.406\n",
      "Iteration 256200 [25620%]: Loss = 417016.594\n",
      "Iteration 256300 [25630%]: Loss = 386773.938\n",
      "Iteration 256400 [25639%]: Loss = 386310.938\n",
      "Iteration 256500 [25650%]: Loss = 455845.125\n",
      "Iteration 256600 [25660%]: Loss = 472244.188\n",
      "Iteration 256700 [25670%]: Loss = 420555.469\n",
      "Iteration 256800 [25680%]: Loss = 395551.656\n",
      "Iteration 256900 [25689%]: Loss = 388515.031\n",
      "Iteration 257000 [25700%]: Loss = 405765.906\n",
      "Iteration 257100 [25710%]: Loss = 409541.688\n",
      "Iteration 257200 [25720%]: Loss = 375153.719\n",
      "Iteration 257300 [25730%]: Loss = 396442.625\n",
      "Iteration 257400 [25739%]: Loss = 406117.906\n",
      "Iteration 257500 [25750%]: Loss = 393712.594\n",
      "Iteration 257600 [25760%]: Loss = 431796.875\n",
      "Iteration 257700 [25770%]: Loss = 386453.875\n",
      "Iteration 257800 [25780%]: Loss = 421493.469\n",
      "Iteration 257900 [25789%]: Loss = 382987.375\n",
      "Iteration 258000 [25800%]: Loss = 380428.281\n",
      "Iteration 258100 [25810%]: Loss = 375386.906\n",
      "Iteration 258200 [25820%]: Loss = 375743.531\n",
      "Iteration 258300 [25830%]: Loss = 368718.719\n",
      "Iteration 258400 [25839%]: Loss = 376996.781\n",
      "Iteration 258500 [25850%]: Loss = 369398.156\n",
      "Iteration 258600 [25860%]: Loss = 378220.156\n",
      "Iteration 258700 [25870%]: Loss = 375401.688\n",
      "Iteration 258800 [25880%]: Loss = 377653.938\n",
      "Iteration 258900 [25889%]: Loss = 391505.344\n",
      "Iteration 259000 [25900%]: Loss = 371201.938\n",
      "Iteration 259100 [25910%]: Loss = 377347.656\n",
      "Iteration 259200 [25920%]: Loss = 378336.875\n",
      "Iteration 259300 [25930%]: Loss = 393534.281\n",
      "Iteration 259400 [25939%]: Loss = 382942.375\n",
      "Iteration 259500 [25950%]: Loss = 393027.062\n",
      "Iteration 259600 [25960%]: Loss = 386074.375\n",
      "Iteration 259700 [25970%]: Loss = 371129.750\n",
      "Iteration 259800 [25980%]: Loss = 376883.594\n",
      "Iteration 259900 [25989%]: Loss = 359406.125\n",
      "Iteration 260000 [26000%]: Loss = 398212.469\n",
      "Iteration 260100 [26010%]: Loss = 388590.594\n",
      "Iteration 260200 [26020%]: Loss = 360735.281\n",
      "Iteration 260300 [26030%]: Loss = 364431.500\n",
      "Iteration 260400 [26039%]: Loss = 366634.000\n",
      "Iteration 260500 [26050%]: Loss = 370185.781\n",
      "Iteration 260600 [26060%]: Loss = 373475.375\n",
      "Iteration 260700 [26070%]: Loss = 365671.375\n",
      "Iteration 260800 [26080%]: Loss = 382533.812\n",
      "Iteration 260900 [26089%]: Loss = 366980.281\n",
      "Iteration 261000 [26100%]: Loss = 361626.656\n",
      "Iteration 261100 [26110%]: Loss = 365819.125\n",
      "Iteration 261200 [26120%]: Loss = 370136.812\n",
      "Iteration 261300 [26130%]: Loss = 368547.750\n",
      "Iteration 261400 [26139%]: Loss = 364920.875\n",
      "Iteration 261500 [26150%]: Loss = 365886.969\n",
      "Iteration 261600 [26160%]: Loss = 365635.719\n",
      "Iteration 261700 [26170%]: Loss = 367331.531\n",
      "Iteration 261800 [26180%]: Loss = 540683.188\n",
      "Iteration 261900 [26189%]: Loss = 499956.844\n",
      "Iteration 262000 [26200%]: Loss = 460882.000\n",
      "Iteration 262100 [26210%]: Loss = 394517.812\n",
      "Iteration 262200 [26220%]: Loss = 386168.000\n",
      "Iteration 262300 [26230%]: Loss = 370804.000\n",
      "Iteration 262400 [26239%]: Loss = 369069.844\n",
      "Iteration 262500 [26250%]: Loss = 369816.531\n",
      "Iteration 262600 [26260%]: Loss = 363589.156\n",
      "Iteration 262700 [26270%]: Loss = 375072.312\n",
      "Iteration 262800 [26280%]: Loss = 371703.312\n",
      "Iteration 262900 [26289%]: Loss = 370360.125\n",
      "Iteration 263000 [26300%]: Loss = 385528.125\n",
      "Iteration 263100 [26310%]: Loss = 406089.344\n",
      "Iteration 263200 [26320%]: Loss = 369928.000\n",
      "Iteration 263300 [26330%]: Loss = 373037.125\n",
      "Iteration 263400 [26339%]: Loss = 383305.531\n",
      "Iteration 263500 [26350%]: Loss = 376585.094\n",
      "Iteration 263600 [26360%]: Loss = 368451.250\n",
      "Iteration 263700 [26370%]: Loss = 372553.406\n",
      "Iteration 263800 [26380%]: Loss = 373980.375\n",
      "Iteration 263900 [26389%]: Loss = 366753.438\n",
      "Iteration 264000 [26400%]: Loss = 374163.906\n",
      "Iteration 264100 [26410%]: Loss = 389579.000\n",
      "Iteration 264200 [26420%]: Loss = 374001.188\n",
      "Iteration 264300 [26430%]: Loss = 374606.281\n",
      "Iteration 264400 [26439%]: Loss = 368867.594\n",
      "Iteration 264500 [26450%]: Loss = 376067.938\n",
      "Iteration 264600 [26460%]: Loss = 373902.750\n",
      "Iteration 264700 [26470%]: Loss = 396876.719\n",
      "Iteration 264800 [26480%]: Loss = 414368.312\n",
      "Iteration 264900 [26489%]: Loss = 376759.625\n",
      "Iteration 265000 [26500%]: Loss = 373234.781\n",
      "Iteration 265100 [26510%]: Loss = 372392.875\n",
      "Iteration 265200 [26520%]: Loss = 401284.438\n",
      "Iteration 265300 [26530%]: Loss = 401474.781\n",
      "Iteration 265400 [26539%]: Loss = 400752.062\n",
      "Iteration 265500 [26550%]: Loss = 462614.156\n",
      "Iteration 265600 [26560%]: Loss = 371973.000\n",
      "Iteration 265700 [26570%]: Loss = 368377.906\n",
      "Iteration 265800 [26580%]: Loss = 376795.312\n",
      "Iteration 265900 [26589%]: Loss = 390158.000\n",
      "Iteration 266000 [26600%]: Loss = 378345.125\n",
      "Iteration 266100 [26610%]: Loss = 377567.750\n",
      "Iteration 266200 [26620%]: Loss = 373737.375\n",
      "Iteration 266300 [26630%]: Loss = 381747.594\n",
      "Iteration 266400 [26639%]: Loss = 385335.156\n",
      "Iteration 266500 [26650%]: Loss = 373398.219\n",
      "Iteration 266600 [26660%]: Loss = 385588.062\n",
      "Iteration 266700 [26670%]: Loss = 408792.594\n",
      "Iteration 266800 [26680%]: Loss = 408096.062\n",
      "Iteration 266900 [26689%]: Loss = 415134.406\n",
      "Iteration 267000 [26700%]: Loss = 426522.938\n",
      "Iteration 267100 [26710%]: Loss = 401858.438\n",
      "Iteration 267200 [26720%]: Loss = 386018.125\n",
      "Iteration 267300 [26730%]: Loss = 405782.750\n",
      "Iteration 267400 [26739%]: Loss = 446099.438\n",
      "Iteration 267500 [26750%]: Loss = 391727.438\n",
      "Iteration 267600 [26760%]: Loss = 414266.156\n",
      "Iteration 267700 [26770%]: Loss = 401998.156\n",
      "Iteration 267800 [26780%]: Loss = 365764.062\n",
      "Iteration 267900 [26789%]: Loss = 370500.281\n",
      "Iteration 268000 [26800%]: Loss = 370193.000\n",
      "Iteration 268100 [26810%]: Loss = 367557.750\n",
      "Iteration 268200 [26820%]: Loss = 372913.156\n",
      "Iteration 268300 [26830%]: Loss = 363060.875\n",
      "Iteration 268400 [26839%]: Loss = 367041.250\n",
      "Iteration 268500 [26850%]: Loss = 365641.594\n",
      "Iteration 268600 [26860%]: Loss = 362548.000\n",
      "Iteration 268700 [26870%]: Loss = 363957.594\n",
      "Iteration 268800 [26880%]: Loss = 364118.375\n",
      "Iteration 268900 [26889%]: Loss = 359728.031\n",
      "Iteration 269000 [26900%]: Loss = 366487.062\n",
      "Iteration 269100 [26910%]: Loss = 368395.219\n",
      "Iteration 269200 [26920%]: Loss = 411130.781\n",
      "Iteration 269300 [26930%]: Loss = 864278.875\n",
      "Iteration 269400 [26939%]: Loss = 618576.875\n",
      "Iteration 269500 [26950%]: Loss = 550690.125\n",
      "Iteration 269600 [26960%]: Loss = 608084.188\n",
      "Iteration 269700 [26970%]: Loss = 495612.875\n",
      "Iteration 269800 [26980%]: Loss = 450364.688\n",
      "Iteration 269900 [26989%]: Loss = 419875.750\n",
      "Iteration 270000 [27000%]: Loss = 431555.375\n",
      "Iteration 270100 [27010%]: Loss = 411785.438\n",
      "Iteration 270200 [27020%]: Loss = 437475.969\n",
      "Iteration 270300 [27030%]: Loss = 429685.375\n",
      "Iteration 270400 [27039%]: Loss = 438098.781\n",
      "Iteration 270500 [27050%]: Loss = 433242.562\n",
      "Iteration 270600 [27060%]: Loss = 538944.500\n",
      "Iteration 270700 [27070%]: Loss = 549253.688\n",
      "Iteration 270800 [27080%]: Loss = 557672.375\n",
      "Iteration 270900 [27089%]: Loss = 404506.219\n",
      "Iteration 271000 [27100%]: Loss = 458381.344\n",
      "Iteration 271100 [27110%]: Loss = 458324.969\n",
      "Iteration 271200 [27120%]: Loss = 425097.281\n",
      "Iteration 271300 [27130%]: Loss = 403380.875\n",
      "Iteration 271400 [27139%]: Loss = 394014.344\n",
      "Iteration 271500 [27150%]: Loss = 382651.031\n",
      "Iteration 271600 [27160%]: Loss = 389695.031\n",
      "Iteration 271700 [27170%]: Loss = 471728.375\n",
      "Iteration 271800 [27180%]: Loss = 482595.875\n",
      "Iteration 271900 [27189%]: Loss = 423699.094\n",
      "Iteration 272000 [27200%]: Loss = 417527.219\n",
      "Iteration 272100 [27210%]: Loss = 379049.406\n",
      "Iteration 272200 [27220%]: Loss = 418965.219\n",
      "Iteration 272300 [27230%]: Loss = 426429.469\n",
      "Iteration 272400 [27239%]: Loss = 795433.312\n",
      "Iteration 272500 [27250%]: Loss = 1562150.875\n",
      "Iteration 272600 [27260%]: Loss = 514202.656\n",
      "Iteration 272700 [27270%]: Loss = 768644.500\n",
      "Iteration 272800 [27280%]: Loss = 513979.438\n",
      "Iteration 272900 [27289%]: Loss = 437942.312\n",
      "Iteration 273000 [27300%]: Loss = 457170.094\n",
      "Iteration 273100 [27310%]: Loss = 447623.094\n",
      "Iteration 273200 [27320%]: Loss = 476766.219\n",
      "Iteration 273300 [27330%]: Loss = 606302.750\n",
      "Iteration 273400 [27339%]: Loss = 542363.000\n",
      "Iteration 273500 [27350%]: Loss = 402985.594\n",
      "Iteration 273600 [27360%]: Loss = 400569.906\n",
      "Iteration 273700 [27370%]: Loss = 395071.562\n",
      "Iteration 273800 [27380%]: Loss = 368815.156\n",
      "Iteration 273900 [27389%]: Loss = 371397.562\n",
      "Iteration 274000 [27400%]: Loss = 366924.250\n",
      "Iteration 274100 [27410%]: Loss = 365044.344\n",
      "Iteration 274200 [27420%]: Loss = 363875.062\n",
      "Iteration 274300 [27430%]: Loss = 363273.625\n",
      "Iteration 274400 [27439%]: Loss = 363326.531\n",
      "Iteration 274500 [27450%]: Loss = 361567.688\n",
      "Iteration 274600 [27460%]: Loss = 361353.188\n",
      "Iteration 274700 [27470%]: Loss = 359860.250\n",
      "Iteration 274800 [27480%]: Loss = 369829.844\n",
      "Iteration 274900 [27489%]: Loss = 363193.875\n",
      "Iteration 275000 [27500%]: Loss = 371724.375\n",
      "Iteration 275100 [27510%]: Loss = 361978.531\n",
      "Iteration 275200 [27520%]: Loss = 363921.188\n",
      "Iteration 275300 [27530%]: Loss = 363738.406\n",
      "Iteration 275400 [27539%]: Loss = 362760.688\n",
      "Iteration 275500 [27550%]: Loss = 369635.906\n",
      "Iteration 275600 [27560%]: Loss = 364205.125\n",
      "Iteration 275700 [27570%]: Loss = 364519.125\n",
      "Iteration 275800 [27580%]: Loss = 359416.781\n",
      "Iteration 275900 [27589%]: Loss = 360061.438\n",
      "Iteration 276000 [27600%]: Loss = 365048.812\n",
      "Iteration 276100 [27610%]: Loss = 373166.281\n",
      "Iteration 276200 [27620%]: Loss = 362824.688\n",
      "Iteration 276300 [27630%]: Loss = 364918.594\n",
      "Iteration 276400 [27639%]: Loss = 366939.625\n",
      "Iteration 276500 [27650%]: Loss = 366040.000\n",
      "Iteration 276600 [27660%]: Loss = 375966.688\n",
      "Iteration 276700 [27670%]: Loss = 379899.344\n",
      "Iteration 276800 [27680%]: Loss = 376021.625\n",
      "Iteration 276900 [27689%]: Loss = 373428.438\n",
      "Iteration 277000 [27700%]: Loss = 379614.562\n",
      "Iteration 277100 [27710%]: Loss = 377981.594\n",
      "Iteration 277200 [27720%]: Loss = 372567.812\n",
      "Iteration 277300 [27730%]: Loss = 366646.625\n",
      "Iteration 277400 [27739%]: Loss = 406232.812\n",
      "Iteration 277500 [27750%]: Loss = 380812.625\n",
      "Iteration 277600 [27760%]: Loss = 385723.469\n",
      "Iteration 277700 [27770%]: Loss = 379695.000\n",
      "Iteration 277800 [27780%]: Loss = 380978.562\n",
      "Iteration 277900 [27789%]: Loss = 389259.938\n",
      "Iteration 278000 [27800%]: Loss = 395853.906\n",
      "Iteration 278100 [27810%]: Loss = 376773.156\n",
      "Iteration 278200 [27820%]: Loss = 381458.844\n",
      "Iteration 278300 [27830%]: Loss = 384748.906\n",
      "Iteration 278400 [27839%]: Loss = 391109.938\n",
      "Iteration 278500 [27850%]: Loss = 416072.906\n",
      "Iteration 278600 [27860%]: Loss = 376007.062\n",
      "Iteration 278700 [27870%]: Loss = 393629.656\n",
      "Iteration 278800 [27880%]: Loss = 385869.344\n",
      "Iteration 278900 [27889%]: Loss = 400477.344\n",
      "Iteration 279000 [27900%]: Loss = 399519.812\n",
      "Iteration 279100 [27910%]: Loss = 393621.750\n",
      "Iteration 279200 [27920%]: Loss = 386417.875\n",
      "Iteration 279300 [27930%]: Loss = 392590.406\n",
      "Iteration 279400 [27939%]: Loss = 387119.375\n",
      "Iteration 279500 [27950%]: Loss = 377935.062\n",
      "Iteration 279600 [27960%]: Loss = 378158.125\n",
      "Iteration 279700 [27970%]: Loss = 382525.312\n",
      "Iteration 279800 [27980%]: Loss = 383549.031\n",
      "Iteration 279900 [27989%]: Loss = 392089.125\n",
      "Iteration 280000 [28000%]: Loss = 377075.156\n",
      "Iteration 280100 [28010%]: Loss = 379438.062\n",
      "Iteration 280200 [28020%]: Loss = 376890.312\n",
      "Iteration 280300 [28030%]: Loss = 377479.375\n",
      "Iteration 280400 [28039%]: Loss = 384265.969\n",
      "Iteration 280500 [28050%]: Loss = 366388.438\n",
      "Iteration 280600 [28060%]: Loss = 374315.062\n",
      "Iteration 280700 [28070%]: Loss = 376223.406\n",
      "Iteration 280800 [28080%]: Loss = 368306.719\n",
      "Iteration 280900 [28089%]: Loss = 371307.125\n",
      "Iteration 281000 [28100%]: Loss = 378692.969\n",
      "Iteration 281100 [28110%]: Loss = 375033.625\n",
      "Iteration 281200 [28120%]: Loss = 368591.938\n",
      "Iteration 281300 [28130%]: Loss = 369176.688\n",
      "Iteration 281400 [28139%]: Loss = 372322.812\n",
      "Iteration 281500 [28150%]: Loss = 375565.094\n",
      "Iteration 281600 [28160%]: Loss = 377910.312\n",
      "Iteration 281700 [28170%]: Loss = 388805.531\n",
      "Iteration 281800 [28180%]: Loss = 373194.750\n",
      "Iteration 281900 [28189%]: Loss = 378976.875\n",
      "Iteration 282000 [28200%]: Loss = 375591.375\n",
      "Iteration 282100 [28210%]: Loss = 375210.938\n",
      "Iteration 282200 [28220%]: Loss = 381769.156\n",
      "Iteration 282300 [28230%]: Loss = 387213.062\n",
      "Iteration 282400 [28239%]: Loss = 377392.031\n",
      "Iteration 282500 [28250%]: Loss = 381886.844\n",
      "Iteration 282600 [28260%]: Loss = 381453.125\n",
      "Iteration 282700 [28270%]: Loss = 374964.312\n",
      "Iteration 282800 [28280%]: Loss = 380157.625\n",
      "Iteration 282900 [28289%]: Loss = 381675.719\n",
      "Iteration 283000 [28300%]: Loss = 383601.188\n",
      "Iteration 283100 [28310%]: Loss = 373818.719\n",
      "Iteration 283200 [28320%]: Loss = 389893.562\n",
      "Iteration 283300 [28330%]: Loss = 375749.062\n",
      "Iteration 283400 [28339%]: Loss = 381226.219\n",
      "Iteration 283500 [28350%]: Loss = 382758.094\n",
      "Iteration 283600 [28360%]: Loss = 375901.594\n",
      "Iteration 283700 [28370%]: Loss = 388841.281\n",
      "Iteration 283800 [28380%]: Loss = 378486.938\n",
      "Iteration 283900 [28389%]: Loss = 378731.531\n",
      "Iteration 284000 [28400%]: Loss = 403067.312\n",
      "Iteration 284100 [28410%]: Loss = 430422.250\n",
      "Iteration 284200 [28420%]: Loss = 396481.000\n",
      "Iteration 284300 [28430%]: Loss = 474983.094\n",
      "Iteration 284400 [28439%]: Loss = 390547.344\n",
      "Iteration 284500 [28450%]: Loss = 391816.031\n",
      "Iteration 284600 [28460%]: Loss = 402371.656\n",
      "Iteration 284700 [28470%]: Loss = 419134.344\n",
      "Iteration 284800 [28480%]: Loss = 416229.031\n",
      "Iteration 284900 [28489%]: Loss = 391391.469\n",
      "Iteration 285000 [28500%]: Loss = 408713.062\n",
      "Iteration 285100 [28510%]: Loss = 441732.812\n",
      "Iteration 285200 [28520%]: Loss = 435040.688\n",
      "Iteration 285300 [28530%]: Loss = 719634.312\n",
      "Iteration 285400 [28539%]: Loss = 486337.281\n",
      "Iteration 285500 [28550%]: Loss = 444687.094\n",
      "Iteration 285600 [28560%]: Loss = 394609.938\n",
      "Iteration 285700 [28570%]: Loss = 390562.281\n",
      "Iteration 285800 [28580%]: Loss = 390890.594\n",
      "Iteration 285900 [28589%]: Loss = 393152.531\n",
      "Iteration 286000 [28600%]: Loss = 383754.750\n",
      "Iteration 286100 [28610%]: Loss = 397656.625\n",
      "Iteration 286200 [28620%]: Loss = 403295.625\n",
      "Iteration 286300 [28630%]: Loss = 391574.438\n",
      "Iteration 286400 [28639%]: Loss = 400626.562\n",
      "Iteration 286500 [28650%]: Loss = 386957.312\n",
      "Iteration 286600 [28660%]: Loss = 382796.344\n",
      "Iteration 286700 [28670%]: Loss = 388517.125\n",
      "Iteration 286800 [28680%]: Loss = 397087.062\n",
      "Iteration 286900 [28689%]: Loss = 399552.125\n",
      "Iteration 287000 [28700%]: Loss = 390640.219\n",
      "Iteration 287100 [28710%]: Loss = 378032.938\n",
      "Iteration 287200 [28720%]: Loss = 391613.531\n",
      "Iteration 287300 [28730%]: Loss = 377081.094\n",
      "Iteration 287400 [28739%]: Loss = 387835.469\n",
      "Iteration 287500 [28750%]: Loss = 379929.375\n",
      "Iteration 287600 [28760%]: Loss = 373120.750\n",
      "Iteration 287700 [28770%]: Loss = 377213.375\n",
      "Iteration 287800 [28780%]: Loss = 396561.594\n",
      "Iteration 287900 [28789%]: Loss = 400306.406\n",
      "Iteration 288000 [28800%]: Loss = 409803.125\n",
      "Iteration 288100 [28810%]: Loss = 382281.500\n",
      "Iteration 288200 [28820%]: Loss = 384604.750\n",
      "Iteration 288300 [28830%]: Loss = 378429.094\n",
      "Iteration 288400 [28839%]: Loss = 396871.406\n",
      "Iteration 288500 [28850%]: Loss = 387529.656\n",
      "Iteration 288600 [28860%]: Loss = 408166.469\n",
      "Iteration 288700 [28870%]: Loss = 408489.469\n",
      "Iteration 288800 [28880%]: Loss = 392803.562\n",
      "Iteration 288900 [28889%]: Loss = 406904.938\n",
      "Iteration 289000 [28900%]: Loss = 403735.531\n",
      "Iteration 289100 [28910%]: Loss = 409508.719\n",
      "Iteration 289200 [28920%]: Loss = 401482.969\n",
      "Iteration 289300 [28930%]: Loss = 452313.031\n",
      "Iteration 289400 [28939%]: Loss = 437368.625\n",
      "Iteration 289500 [28950%]: Loss = 430493.281\n",
      "Iteration 289600 [28960%]: Loss = 428649.875\n",
      "Iteration 289700 [28970%]: Loss = 443342.438\n",
      "Iteration 289800 [28980%]: Loss = 570016.375\n",
      "Iteration 289900 [28989%]: Loss = 472846.531\n",
      "Iteration 290000 [29000%]: Loss = 479790.844\n",
      "Iteration 290100 [29010%]: Loss = 541786.625\n",
      "Iteration 290200 [29020%]: Loss = 470202.406\n",
      "Iteration 290300 [29030%]: Loss = 425042.656\n",
      "Iteration 290400 [29039%]: Loss = 417938.469\n",
      "Iteration 290500 [29050%]: Loss = 447350.469\n",
      "Iteration 290600 [29060%]: Loss = 468022.156\n",
      "Iteration 290700 [29070%]: Loss = 487080.750\n",
      "Iteration 290800 [29080%]: Loss = 456075.562\n",
      "Iteration 290900 [29089%]: Loss = 446665.312\n",
      "Iteration 291000 [29100%]: Loss = 426507.688\n",
      "Iteration 291100 [29110%]: Loss = 477341.000\n",
      "Iteration 291200 [29120%]: Loss = 424933.750\n",
      "Iteration 291300 [29130%]: Loss = 411505.594\n",
      "Iteration 291400 [29139%]: Loss = 482842.438\n",
      "Iteration 291500 [29150%]: Loss = 432485.250\n",
      "Iteration 291600 [29160%]: Loss = 410145.250\n",
      "Iteration 291700 [29170%]: Loss = 406882.781\n",
      "Iteration 291800 [29180%]: Loss = 404529.469\n",
      "Iteration 291900 [29189%]: Loss = 470917.156\n",
      "Iteration 292000 [29200%]: Loss = 415485.875\n",
      "Iteration 292100 [29210%]: Loss = 414338.219\n",
      "Iteration 292200 [29220%]: Loss = 451508.594\n",
      "Iteration 292300 [29230%]: Loss = 489559.125\n",
      "Iteration 292400 [29239%]: Loss = 504325.156\n",
      "Iteration 292500 [29250%]: Loss = 463690.750\n",
      "Iteration 292600 [29260%]: Loss = 470415.094\n",
      "Iteration 292700 [29270%]: Loss = 448078.375\n",
      "Iteration 292800 [29280%]: Loss = 440805.906\n",
      "Iteration 292900 [29289%]: Loss = 427985.406\n",
      "Iteration 293000 [29300%]: Loss = 411408.406\n",
      "Iteration 293100 [29310%]: Loss = 408378.906\n",
      "Iteration 293200 [29320%]: Loss = 459085.125\n",
      "Iteration 293300 [29330%]: Loss = 418675.031\n",
      "Iteration 293400 [29339%]: Loss = 433682.469\n",
      "Iteration 293500 [29350%]: Loss = 386240.562\n",
      "Iteration 293600 [29360%]: Loss = 400097.844\n",
      "Iteration 293700 [29370%]: Loss = 400931.250\n",
      "Iteration 293800 [29380%]: Loss = 379129.438\n",
      "Iteration 293900 [29389%]: Loss = 398737.688\n",
      "Iteration 294000 [29400%]: Loss = 404153.031\n",
      "Iteration 294100 [29410%]: Loss = 393428.875\n",
      "Iteration 294200 [29420%]: Loss = 405513.531\n",
      "Iteration 294300 [29430%]: Loss = 407614.062\n",
      "Iteration 294400 [29439%]: Loss = 392189.250\n",
      "Iteration 294500 [29450%]: Loss = 388700.500\n",
      "Iteration 294600 [29460%]: Loss = 405896.844\n",
      "Iteration 294700 [29470%]: Loss = 414419.875\n",
      "Iteration 294800 [29480%]: Loss = 1793114.875\n",
      "Iteration 294900 [29489%]: Loss = 769092.500\n",
      "Iteration 295000 [29500%]: Loss = 638435.562\n",
      "Iteration 295100 [29510%]: Loss = 470985.750\n",
      "Iteration 295200 [29520%]: Loss = 490193.594\n",
      "Iteration 295300 [29530%]: Loss = 398959.906\n",
      "Iteration 295400 [29539%]: Loss = 393130.875\n",
      "Iteration 295500 [29550%]: Loss = 379468.531\n",
      "Iteration 295600 [29560%]: Loss = 378584.188\n",
      "Iteration 295700 [29570%]: Loss = 381145.312\n",
      "Iteration 295800 [29580%]: Loss = 390029.062\n",
      "Iteration 295900 [29589%]: Loss = 382064.625\n",
      "Iteration 296000 [29600%]: Loss = 381351.031\n",
      "Iteration 296100 [29610%]: Loss = 387477.406\n",
      "Iteration 296200 [29620%]: Loss = 386928.188\n",
      "Iteration 296300 [29630%]: Loss = 379056.219\n",
      "Iteration 296400 [29639%]: Loss = 386116.875\n",
      "Iteration 296500 [29650%]: Loss = 381488.906\n",
      "Iteration 296600 [29660%]: Loss = 388575.438\n",
      "Iteration 296700 [29670%]: Loss = 384692.875\n",
      "Iteration 296800 [29680%]: Loss = 385165.781\n",
      "Iteration 296900 [29689%]: Loss = 389264.312\n",
      "Iteration 297000 [29700%]: Loss = 643909.750\n",
      "Iteration 297100 [29710%]: Loss = 384221.562\n",
      "Iteration 297200 [29720%]: Loss = 385405.781\n",
      "Iteration 297300 [29730%]: Loss = 376462.250\n",
      "Iteration 297400 [29739%]: Loss = 373984.594\n",
      "Iteration 297500 [29750%]: Loss = 377711.906\n",
      "Iteration 297600 [29760%]: Loss = 376909.000\n",
      "Iteration 297700 [29770%]: Loss = 379660.812\n",
      "Iteration 297800 [29780%]: Loss = 374933.781\n",
      "Iteration 297900 [29789%]: Loss = 382001.531\n",
      "Iteration 298000 [29800%]: Loss = 450018.406\n",
      "Iteration 298100 [29810%]: Loss = 394257.562\n",
      "Iteration 298200 [29820%]: Loss = 410527.125\n",
      "Iteration 298300 [29830%]: Loss = 405493.844\n",
      "Iteration 298400 [29839%]: Loss = 446342.281\n",
      "Iteration 298500 [29850%]: Loss = 711273.062\n",
      "Iteration 298600 [29860%]: Loss = 477003.781\n",
      "Iteration 298700 [29870%]: Loss = 457002.312\n",
      "Iteration 298800 [29880%]: Loss = 459253.500\n",
      "Iteration 298900 [29889%]: Loss = 387584.438\n",
      "Iteration 299000 [29900%]: Loss = 378863.531\n",
      "Iteration 299100 [29910%]: Loss = 377301.938\n",
      "Iteration 299200 [29920%]: Loss = 382018.094\n",
      "Iteration 299300 [29930%]: Loss = 378711.062\n",
      "Iteration 299400 [29939%]: Loss = 374615.781\n",
      "Iteration 299500 [29950%]: Loss = 370346.312\n",
      "Iteration 299600 [29960%]: Loss = 377147.812\n",
      "Iteration 299700 [29970%]: Loss = 370565.938\n",
      "Iteration 299800 [29980%]: Loss = 376047.031\n",
      "Iteration 299900 [29989%]: Loss = 456660.125\n",
      "Iteration 300000 [30000%]: Loss = 602780.562\n",
      "Iteration 300100 [30010%]: Loss = 737612.000\n",
      "Iteration 300200 [30020%]: Loss = 508381.438\n",
      "Iteration 300300 [30030%]: Loss = 557800.688\n",
      "Iteration 300400 [30039%]: Loss = 1035419.062\n",
      "Iteration 300500 [30050%]: Loss = 1020874.312\n",
      "Iteration 300600 [30060%]: Loss = 519434.219\n",
      "Iteration 300700 [30070%]: Loss = 499947.094\n",
      "Iteration 300800 [30080%]: Loss = 471983.688\n",
      "Iteration 300900 [30089%]: Loss = 409509.188\n",
      "Iteration 301000 [30100%]: Loss = 428946.406\n",
      "Iteration 301100 [30110%]: Loss = 434259.719\n",
      "Iteration 301200 [30120%]: Loss = 436563.562\n",
      "Iteration 301300 [30130%]: Loss = 398609.312\n",
      "Iteration 301400 [30139%]: Loss = 409819.250\n",
      "Iteration 301500 [30150%]: Loss = 390346.500\n",
      "Iteration 301600 [30160%]: Loss = 380187.188\n",
      "Iteration 301700 [30170%]: Loss = 379190.281\n",
      "Iteration 301800 [30180%]: Loss = 374413.125\n",
      "Iteration 301900 [30189%]: Loss = 377105.438\n",
      "Iteration 302000 [30200%]: Loss = 380850.594\n",
      "Iteration 302100 [30210%]: Loss = 382692.906\n",
      "Iteration 302200 [30220%]: Loss = 390690.594\n",
      "Iteration 302300 [30230%]: Loss = 394113.062\n",
      "Iteration 302400 [30239%]: Loss = 382427.625\n",
      "Iteration 302500 [30250%]: Loss = 382692.844\n",
      "Iteration 302600 [30260%]: Loss = 394375.094\n",
      "Iteration 302700 [30270%]: Loss = 389044.031\n",
      "Iteration 302800 [30280%]: Loss = 384758.875\n",
      "Iteration 302900 [30289%]: Loss = 388225.000\n",
      "Iteration 303000 [30300%]: Loss = 411762.469\n",
      "Iteration 303100 [30310%]: Loss = 411592.531\n",
      "Iteration 303200 [30320%]: Loss = 396997.562\n",
      "Iteration 303300 [30330%]: Loss = 389596.938\n",
      "Iteration 303400 [30339%]: Loss = 387779.375\n",
      "Iteration 303500 [30350%]: Loss = 390559.719\n",
      "Iteration 303600 [30360%]: Loss = 385357.188\n",
      "Iteration 303700 [30370%]: Loss = 386864.125\n",
      "Iteration 303800 [30380%]: Loss = 380950.562\n",
      "Iteration 303900 [30389%]: Loss = 381564.719\n",
      "Iteration 304000 [30400%]: Loss = 383156.156\n",
      "Iteration 304100 [30410%]: Loss = 390242.281\n",
      "Iteration 304200 [30420%]: Loss = 408823.156\n",
      "Iteration 304300 [30430%]: Loss = 409868.094\n",
      "Iteration 304400 [30439%]: Loss = 420172.000\n",
      "Iteration 304500 [30450%]: Loss = 412218.531\n",
      "Iteration 304600 [30460%]: Loss = 439231.062\n",
      "Iteration 304700 [30470%]: Loss = 409817.875\n",
      "Iteration 304800 [30480%]: Loss = 429373.750\n",
      "Iteration 304900 [30489%]: Loss = 414868.594\n",
      "Iteration 305000 [30500%]: Loss = 390181.344\n",
      "Iteration 305100 [30510%]: Loss = 389610.031\n",
      "Iteration 305200 [30520%]: Loss = 413325.656\n",
      "Iteration 305300 [30530%]: Loss = 514935.844\n",
      "Iteration 305400 [30539%]: Loss = 400526.594\n",
      "Iteration 305500 [30550%]: Loss = 389875.562\n",
      "Iteration 305600 [30560%]: Loss = 382871.781\n",
      "Iteration 305700 [30570%]: Loss = 377388.562\n",
      "Iteration 305800 [30580%]: Loss = 397587.406\n",
      "Iteration 305900 [30589%]: Loss = 403723.844\n",
      "Iteration 306000 [30600%]: Loss = 381994.031\n",
      "Iteration 306100 [30610%]: Loss = 385498.344\n",
      "Iteration 306200 [30620%]: Loss = 390429.031\n",
      "Iteration 306300 [30630%]: Loss = 391888.156\n",
      "Iteration 306400 [30639%]: Loss = 390951.500\n",
      "Iteration 306500 [30650%]: Loss = 402420.750\n",
      "Iteration 306600 [30660%]: Loss = 381049.344\n",
      "Iteration 306700 [30670%]: Loss = 387974.000\n",
      "Iteration 306800 [30680%]: Loss = 388981.906\n",
      "Iteration 306900 [30689%]: Loss = 395067.906\n",
      "Iteration 307000 [30700%]: Loss = 386882.125\n",
      "Iteration 307100 [30710%]: Loss = 394960.531\n",
      "Iteration 307200 [30720%]: Loss = 409222.125\n",
      "Iteration 307300 [30730%]: Loss = 454455.969\n",
      "Iteration 307400 [30739%]: Loss = 398779.688\n",
      "Iteration 307500 [30750%]: Loss = 383116.375\n",
      "Iteration 307600 [30760%]: Loss = 382820.844\n",
      "Iteration 307700 [30770%]: Loss = 399308.656\n",
      "Iteration 307800 [30780%]: Loss = 382593.750\n",
      "Iteration 307900 [30789%]: Loss = 396983.719\n",
      "Iteration 308000 [30800%]: Loss = 412204.938\n",
      "Iteration 308100 [30810%]: Loss = 431098.000\n",
      "Iteration 308200 [30820%]: Loss = 530318.125\n",
      "Iteration 308300 [30830%]: Loss = 512235.531\n",
      "Iteration 308400 [30839%]: Loss = 408413.688\n",
      "Iteration 308500 [30850%]: Loss = 414733.875\n",
      "Iteration 308600 [30860%]: Loss = 398268.219\n",
      "Iteration 308700 [30870%]: Loss = 395338.312\n",
      "Iteration 308800 [30880%]: Loss = 394525.594\n",
      "Iteration 308900 [30889%]: Loss = 406741.344\n",
      "Iteration 309000 [30900%]: Loss = 390209.656\n",
      "Iteration 309100 [30910%]: Loss = 389326.312\n",
      "Iteration 309200 [30920%]: Loss = 385551.094\n",
      "Iteration 309300 [30930%]: Loss = 392795.688\n",
      "Iteration 309400 [30939%]: Loss = 391021.375\n",
      "Iteration 309500 [30950%]: Loss = 390819.594\n",
      "Iteration 309600 [30960%]: Loss = 393574.406\n",
      "Iteration 309700 [30970%]: Loss = 397848.438\n",
      "Iteration 309800 [30980%]: Loss = 407680.500\n",
      "Iteration 309900 [30989%]: Loss = 407300.875\n",
      "Iteration 310000 [31000%]: Loss = 396495.031\n",
      "Iteration 310100 [31010%]: Loss = 414424.562\n",
      "Iteration 310200 [31020%]: Loss = 398591.406\n",
      "Iteration 310300 [31030%]: Loss = 417578.438\n",
      "Iteration 310400 [31039%]: Loss = 394334.000\n",
      "Iteration 310500 [31050%]: Loss = 390376.281\n",
      "Iteration 310600 [31060%]: Loss = 394250.312\n",
      "Iteration 310700 [31070%]: Loss = 392827.969\n",
      "Iteration 310800 [31080%]: Loss = 389529.125\n",
      "Iteration 310900 [31089%]: Loss = 408798.281\n",
      "Iteration 311000 [31100%]: Loss = 401132.031\n",
      "Iteration 311100 [31110%]: Loss = 433163.812\n",
      "Iteration 311200 [31120%]: Loss = 402753.281\n",
      "Iteration 311300 [31130%]: Loss = 398226.406\n",
      "Iteration 311400 [31139%]: Loss = 391234.312\n",
      "Iteration 311500 [31150%]: Loss = 434689.562\n",
      "Iteration 311600 [31160%]: Loss = 407175.812\n",
      "Iteration 311700 [31170%]: Loss = 406156.250\n",
      "Iteration 311800 [31180%]: Loss = 406485.406\n",
      "Iteration 311900 [31189%]: Loss = 485571.312\n",
      "Iteration 312000 [31200%]: Loss = 551736.438\n",
      "Iteration 312100 [31210%]: Loss = 443604.906\n",
      "Iteration 312200 [31220%]: Loss = 488062.812\n",
      "Iteration 312300 [31230%]: Loss = 475179.625\n",
      "Iteration 312400 [31239%]: Loss = 783979.688\n",
      "Iteration 312500 [31250%]: Loss = 434981.250\n",
      "Iteration 312600 [31260%]: Loss = 430208.812\n",
      "Iteration 312700 [31270%]: Loss = 399941.688\n",
      "Iteration 312800 [31280%]: Loss = 390595.844\n",
      "Iteration 312900 [31289%]: Loss = 413995.750\n",
      "Iteration 313000 [31300%]: Loss = 388141.281\n",
      "Iteration 313100 [31310%]: Loss = 423890.562\n",
      "Iteration 313200 [31320%]: Loss = 441643.281\n",
      "Iteration 313300 [31330%]: Loss = 446284.344\n",
      "Iteration 313400 [31339%]: Loss = 422014.375\n",
      "Iteration 313500 [31350%]: Loss = 402699.594\n",
      "Iteration 313600 [31360%]: Loss = 405754.031\n",
      "Iteration 313700 [31370%]: Loss = 407742.938\n",
      "Iteration 313800 [31380%]: Loss = 399430.969\n",
      "Iteration 313900 [31389%]: Loss = 403931.844\n",
      "Iteration 314000 [31400%]: Loss = 406289.719\n",
      "Iteration 314100 [31410%]: Loss = 458316.875\n",
      "Iteration 314200 [31420%]: Loss = 450013.406\n",
      "Iteration 314300 [31430%]: Loss = 408323.375\n",
      "Iteration 314400 [31439%]: Loss = 388981.875\n",
      "Iteration 314500 [31450%]: Loss = 384074.969\n",
      "Iteration 314600 [31460%]: Loss = 379103.844\n",
      "Iteration 314700 [31470%]: Loss = 397839.281\n",
      "Iteration 314800 [31480%]: Loss = 406463.250\n",
      "Iteration 314900 [31489%]: Loss = 407399.750\n",
      "Iteration 315000 [31500%]: Loss = 414206.812\n",
      "Iteration 315100 [31510%]: Loss = 453029.688\n",
      "Iteration 315200 [31520%]: Loss = 408779.500\n",
      "Iteration 315300 [31530%]: Loss = 394683.875\n",
      "Iteration 315400 [31539%]: Loss = 390323.844\n",
      "Iteration 315500 [31550%]: Loss = 398323.625\n",
      "Iteration 315600 [31560%]: Loss = 380976.562\n",
      "Iteration 315700 [31570%]: Loss = 384523.625\n",
      "Iteration 315800 [31580%]: Loss = 375002.344\n",
      "Iteration 315900 [31589%]: Loss = 385255.656\n",
      "Iteration 316000 [31600%]: Loss = 383746.312\n",
      "Iteration 316100 [31610%]: Loss = 479818.625\n",
      "Iteration 316200 [31620%]: Loss = 427705.312\n",
      "Iteration 316300 [31630%]: Loss = 432456.312\n",
      "Iteration 316400 [31639%]: Loss = 482793.406\n",
      "Iteration 316500 [31650%]: Loss = 794830.625\n",
      "Iteration 316600 [31660%]: Loss = 442419.281\n",
      "Iteration 316700 [31670%]: Loss = 427301.562\n",
      "Iteration 316800 [31680%]: Loss = 391895.406\n",
      "Iteration 316900 [31689%]: Loss = 389860.688\n",
      "Iteration 317000 [31700%]: Loss = 382432.281\n",
      "Iteration 317100 [31710%]: Loss = 396827.969\n",
      "Iteration 317200 [31720%]: Loss = 383042.812\n",
      "Iteration 317300 [31730%]: Loss = 388495.531\n",
      "Iteration 317400 [31739%]: Loss = 388450.594\n",
      "Iteration 317500 [31750%]: Loss = 388705.406\n",
      "Iteration 317600 [31760%]: Loss = 381623.719\n",
      "Iteration 317700 [31770%]: Loss = 374914.469\n",
      "Iteration 317800 [31780%]: Loss = 381082.438\n",
      "Iteration 317900 [31789%]: Loss = 378533.125\n",
      "Iteration 318000 [31800%]: Loss = 378687.781\n",
      "Iteration 318100 [31810%]: Loss = 374287.062\n",
      "Iteration 318200 [31820%]: Loss = 383417.969\n",
      "Iteration 318300 [31830%]: Loss = 382698.344\n",
      "Iteration 318400 [31839%]: Loss = 380209.562\n",
      "Iteration 318500 [31850%]: Loss = 381311.062\n",
      "Iteration 318600 [31860%]: Loss = 391634.844\n",
      "Iteration 318700 [31870%]: Loss = 444541.625\n",
      "Iteration 318800 [31880%]: Loss = 384459.562\n",
      "Iteration 318900 [31889%]: Loss = 374721.250\n",
      "Iteration 319000 [31900%]: Loss = 378834.906\n",
      "Iteration 319100 [31910%]: Loss = 380088.656\n",
      "Iteration 319200 [31920%]: Loss = 382272.156\n",
      "Iteration 319300 [31930%]: Loss = 378750.469\n",
      "Iteration 319400 [31939%]: Loss = 376575.625\n",
      "Iteration 319500 [31950%]: Loss = 382722.094\n",
      "Iteration 319600 [31960%]: Loss = 386955.656\n",
      "Iteration 319700 [31970%]: Loss = 389101.031\n",
      "Iteration 319800 [31980%]: Loss = 384968.625\n",
      "Iteration 319900 [31989%]: Loss = 386559.969\n",
      "Iteration 320000 [32000%]: Loss = 384668.750\n",
      "Iteration 320100 [32010%]: Loss = 385029.062\n",
      "Iteration 320200 [32020%]: Loss = 380791.062\n",
      "Iteration 320300 [32030%]: Loss = 435896.969\n",
      "Iteration 320400 [32039%]: Loss = 388006.719\n",
      "Iteration 320500 [32050%]: Loss = 481939.938\n",
      "Iteration 320600 [32060%]: Loss = 378245.250\n",
      "Iteration 320700 [32070%]: Loss = 383554.906\n",
      "Iteration 320800 [32080%]: Loss = 393665.531\n",
      "Iteration 320900 [32089%]: Loss = 379125.875\n",
      "Iteration 321000 [32100%]: Loss = 383140.312\n",
      "Iteration 321100 [32110%]: Loss = 376493.375\n",
      "Iteration 321200 [32120%]: Loss = 385666.219\n",
      "Iteration 321300 [32130%]: Loss = 424847.844\n",
      "Iteration 321400 [32139%]: Loss = 381190.906\n",
      "Iteration 321500 [32150%]: Loss = 385504.312\n",
      "Iteration 321600 [32160%]: Loss = 387778.625\n",
      "Iteration 321700 [32170%]: Loss = 377709.531\n",
      "Iteration 321800 [32180%]: Loss = 379935.125\n",
      "Iteration 321900 [32189%]: Loss = 387892.875\n",
      "Iteration 322000 [32200%]: Loss = 397030.031\n",
      "Iteration 322100 [32210%]: Loss = 410128.250\n",
      "Iteration 322200 [32220%]: Loss = 372102.594\n",
      "Iteration 322300 [32230%]: Loss = 379069.375\n",
      "Iteration 322400 [32239%]: Loss = 383775.094\n",
      "Iteration 322500 [32250%]: Loss = 371262.500\n",
      "Iteration 322600 [32260%]: Loss = 398708.812\n",
      "Iteration 322700 [32270%]: Loss = 382312.062\n",
      "Iteration 322800 [32280%]: Loss = 374529.312\n",
      "Iteration 322900 [32289%]: Loss = 383486.844\n",
      "Iteration 323000 [32300%]: Loss = 379858.562\n",
      "Iteration 323100 [32310%]: Loss = 394495.562\n",
      "Iteration 323200 [32320%]: Loss = 380385.094\n",
      "Iteration 323300 [32330%]: Loss = 396505.500\n",
      "Iteration 323400 [32339%]: Loss = 394834.969\n",
      "Iteration 323500 [32350%]: Loss = 423255.156\n",
      "Iteration 323600 [32360%]: Loss = 401616.250\n",
      "Iteration 323700 [32370%]: Loss = 377225.875\n",
      "Iteration 323800 [32380%]: Loss = 382094.656\n",
      "Iteration 323900 [32389%]: Loss = 457979.844\n",
      "Iteration 324000 [32400%]: Loss = 467733.750\n",
      "Iteration 324100 [32410%]: Loss = 423740.625\n",
      "Iteration 324200 [32420%]: Loss = 402552.312\n",
      "Iteration 324300 [32430%]: Loss = 395598.625\n",
      "Iteration 324400 [32439%]: Loss = 416318.750\n",
      "Iteration 324500 [32450%]: Loss = 389944.312\n",
      "Iteration 324600 [32460%]: Loss = 410820.156\n",
      "Iteration 324700 [32470%]: Loss = 371430.906\n",
      "Iteration 324800 [32480%]: Loss = 377186.875\n",
      "Iteration 324900 [32489%]: Loss = 381643.750\n",
      "Iteration 325000 [32500%]: Loss = 538191.750\n",
      "Iteration 325100 [32510%]: Loss = 431798.344\n",
      "Iteration 325200 [32520%]: Loss = 421943.719\n",
      "Iteration 325300 [32530%]: Loss = 441333.219\n",
      "Iteration 325400 [32539%]: Loss = 546771.625\n",
      "Iteration 325500 [32550%]: Loss = 505571.438\n",
      "Iteration 325600 [32560%]: Loss = 494954.188\n",
      "Iteration 325700 [32570%]: Loss = 428331.750\n",
      "Iteration 325800 [32580%]: Loss = 409325.156\n",
      "Iteration 325900 [32589%]: Loss = 402297.469\n",
      "Iteration 326000 [32600%]: Loss = 412288.125\n",
      "Iteration 326100 [32610%]: Loss = 422846.625\n",
      "Iteration 326200 [32620%]: Loss = 434437.469\n",
      "Iteration 326300 [32630%]: Loss = 409351.969\n",
      "Iteration 326400 [32639%]: Loss = 437256.281\n",
      "Iteration 326500 [32650%]: Loss = 444038.875\n",
      "Iteration 326600 [32660%]: Loss = 425083.844\n",
      "Iteration 326700 [32670%]: Loss = 558105.438\n",
      "Iteration 326800 [32680%]: Loss = 503278.875\n",
      "Iteration 326900 [32689%]: Loss = 433311.375\n",
      "Iteration 327000 [32700%]: Loss = 431048.594\n",
      "Iteration 327100 [32710%]: Loss = 431275.938\n",
      "Iteration 327200 [32720%]: Loss = 543171.625\n",
      "Iteration 327300 [32730%]: Loss = 494152.969\n",
      "Iteration 327400 [32739%]: Loss = 550857.812\n",
      "Iteration 327500 [32750%]: Loss = 456094.281\n",
      "Iteration 327600 [32760%]: Loss = 513792.938\n",
      "Iteration 327700 [32770%]: Loss = 1004839.938\n",
      "Iteration 327800 [32780%]: Loss = 439972.312\n",
      "Iteration 327900 [32790%]: Loss = 486989.688\n",
      "Iteration 328000 [32800%]: Loss = 561163.812\n",
      "Iteration 328100 [32810%]: Loss = 458508.938\n",
      "Iteration 328200 [32820%]: Loss = 449861.406\n",
      "Iteration 328300 [32830%]: Loss = 643420.250\n",
      "Iteration 328400 [32840%]: Loss = 470619.250\n",
      "Iteration 328500 [32850%]: Loss = 417223.969\n",
      "Iteration 328600 [32860%]: Loss = 435855.438\n",
      "Iteration 328700 [32870%]: Loss = 430492.750\n",
      "Iteration 328800 [32880%]: Loss = 448511.625\n",
      "Iteration 328900 [32890%]: Loss = 481030.281\n",
      "Iteration 329000 [32900%]: Loss = 404459.312\n",
      "Iteration 329100 [32910%]: Loss = 437668.125\n",
      "Iteration 329200 [32920%]: Loss = 471484.969\n",
      "Iteration 329300 [32930%]: Loss = 486169.531\n",
      "Iteration 329400 [32940%]: Loss = 414675.469\n",
      "Iteration 329500 [32950%]: Loss = 482329.406\n",
      "Iteration 329600 [32960%]: Loss = 620341.625\n",
      "Iteration 329700 [32970%]: Loss = 459971.625\n",
      "Iteration 329800 [32980%]: Loss = 448232.812\n",
      "Iteration 329900 [32990%]: Loss = 401875.500\n",
      "Iteration 330000 [33000%]: Loss = 411703.344\n",
      "Iteration 330100 [33010%]: Loss = 481776.188\n",
      "Iteration 330200 [33020%]: Loss = 510992.969\n",
      "Iteration 330300 [33030%]: Loss = 687558.250\n",
      "Iteration 330400 [33040%]: Loss = 670404.250\n",
      "Iteration 330500 [33050%]: Loss = 467543.312\n",
      "Iteration 330600 [33060%]: Loss = 473369.156\n",
      "Iteration 330700 [33070%]: Loss = 447100.188\n",
      "Iteration 330800 [33080%]: Loss = 671289.125\n",
      "Iteration 330900 [33090%]: Loss = 449287.031\n",
      "Iteration 331000 [33100%]: Loss = 429971.094\n",
      "Iteration 331100 [33110%]: Loss = 452877.406\n",
      "Iteration 331200 [33120%]: Loss = 522255.938\n",
      "Iteration 331300 [33130%]: Loss = 499852.344\n",
      "Iteration 331400 [33140%]: Loss = 465787.719\n",
      "Iteration 331500 [33150%]: Loss = 456011.156\n",
      "Iteration 331600 [33160%]: Loss = 466393.969\n",
      "Iteration 331700 [33170%]: Loss = 415126.781\n",
      "Iteration 331800 [33180%]: Loss = 396525.562\n",
      "Iteration 331900 [33190%]: Loss = 409725.875\n",
      "Iteration 332000 [33200%]: Loss = 409667.375\n",
      "Iteration 332100 [33210%]: Loss = 396915.875\n",
      "Iteration 332200 [33220%]: Loss = 398782.406\n",
      "Iteration 332300 [33230%]: Loss = 387351.062\n",
      "Iteration 332400 [33240%]: Loss = 399729.125\n",
      "Iteration 332500 [33250%]: Loss = 459124.594\n",
      "Iteration 332600 [33260%]: Loss = 529056.375\n",
      "Iteration 332700 [33270%]: Loss = 437457.812\n",
      "Iteration 332800 [33280%]: Loss = 414338.094\n",
      "Iteration 332900 [33290%]: Loss = 425483.656\n",
      "Iteration 333000 [33300%]: Loss = 422366.188\n",
      "Iteration 333100 [33310%]: Loss = 449420.375\n",
      "Iteration 333200 [33320%]: Loss = 436142.594\n",
      "Iteration 333300 [33330%]: Loss = 440198.656\n",
      "Iteration 333400 [33340%]: Loss = 420824.000\n",
      "Iteration 333500 [33350%]: Loss = 399576.469\n",
      "Iteration 333600 [33360%]: Loss = 414351.156\n",
      "Iteration 333700 [33370%]: Loss = 409951.094\n",
      "Iteration 333800 [33380%]: Loss = 387383.094\n",
      "Iteration 333900 [33390%]: Loss = 391407.500\n",
      "Iteration 334000 [33400%]: Loss = 438142.500\n",
      "Iteration 334100 [33410%]: Loss = 402668.969\n",
      "Iteration 334200 [33420%]: Loss = 472151.062\n",
      "Iteration 334300 [33430%]: Loss = 492194.094\n",
      "Iteration 334400 [33440%]: Loss = 451560.156\n",
      "Iteration 334500 [33450%]: Loss = 512298.062\n",
      "Iteration 334600 [33460%]: Loss = 423506.312\n",
      "Iteration 334700 [33470%]: Loss = 395106.875\n",
      "Iteration 334800 [33480%]: Loss = 389532.812\n",
      "Iteration 334900 [33490%]: Loss = 376681.281\n",
      "Iteration 335000 [33500%]: Loss = 382459.125\n",
      "Iteration 335100 [33510%]: Loss = 377715.062\n",
      "Iteration 335200 [33520%]: Loss = 384505.594\n",
      "Iteration 335300 [33530%]: Loss = 378193.469\n",
      "Iteration 335400 [33540%]: Loss = 387594.750\n",
      "Iteration 335500 [33550%]: Loss = 383625.219\n",
      "Iteration 335600 [33560%]: Loss = 383969.938\n",
      "Iteration 335700 [33570%]: Loss = 382212.312\n",
      "Iteration 335800 [33580%]: Loss = 397415.531\n",
      "Iteration 335900 [33590%]: Loss = 390764.344\n",
      "Iteration 336000 [33600%]: Loss = 388881.656\n",
      "Iteration 336100 [33610%]: Loss = 390236.062\n",
      "Iteration 336200 [33620%]: Loss = 401793.438\n",
      "Iteration 336300 [33630%]: Loss = 432613.219\n",
      "Iteration 336400 [33640%]: Loss = 438793.688\n",
      "Iteration 336500 [33650%]: Loss = 526714.250\n",
      "Iteration 336600 [33660%]: Loss = 421580.625\n",
      "Iteration 336700 [33670%]: Loss = 719208.812\n",
      "Iteration 336800 [33680%]: Loss = 420002.250\n",
      "Iteration 336900 [33690%]: Loss = 469537.312\n",
      "Iteration 337000 [33700%]: Loss = 411692.156\n",
      "Iteration 337100 [33710%]: Loss = 379632.219\n",
      "Iteration 337200 [33720%]: Loss = 423117.219\n",
      "Iteration 337300 [33730%]: Loss = 401085.844\n",
      "Iteration 337400 [33740%]: Loss = 392383.625\n",
      "Iteration 337500 [33750%]: Loss = 417973.750\n",
      "Iteration 337600 [33760%]: Loss = 420903.406\n",
      "Iteration 337700 [33770%]: Loss = 446221.625\n",
      "Iteration 337800 [33780%]: Loss = 385861.562\n",
      "Iteration 337900 [33790%]: Loss = 381998.281\n",
      "Iteration 338000 [33800%]: Loss = 375050.219\n",
      "Iteration 338100 [33810%]: Loss = 371884.438\n",
      "Iteration 338200 [33820%]: Loss = 383932.062\n",
      "Iteration 338300 [33830%]: Loss = 376434.438\n",
      "Iteration 338400 [33840%]: Loss = 372279.938\n",
      "Iteration 338500 [33850%]: Loss = 371203.531\n",
      "Iteration 338600 [33860%]: Loss = 381616.375\n",
      "Iteration 338700 [33870%]: Loss = 431383.000\n",
      "Iteration 338800 [33880%]: Loss = 374475.375\n",
      "Iteration 338900 [33890%]: Loss = 374293.000\n",
      "Iteration 339000 [33900%]: Loss = 383572.406\n",
      "Iteration 339100 [33910%]: Loss = 374990.656\n",
      "Iteration 339200 [33920%]: Loss = 380198.312\n",
      "Iteration 339300 [33930%]: Loss = 376732.469\n",
      "Iteration 339400 [33940%]: Loss = 378065.312\n",
      "Iteration 339500 [33950%]: Loss = 375569.250\n",
      "Iteration 339600 [33960%]: Loss = 381180.094\n",
      "Iteration 339700 [33970%]: Loss = 374351.125\n",
      "Iteration 339800 [33980%]: Loss = 379121.875\n",
      "Iteration 339900 [33990%]: Loss = 407684.000\n",
      "Iteration 340000 [34000%]: Loss = 407881.312\n",
      "Iteration 340100 [34010%]: Loss = 378542.938\n",
      "Iteration 340200 [34020%]: Loss = 376371.156\n",
      "Iteration 340300 [34030%]: Loss = 369953.188\n",
      "Iteration 340400 [34040%]: Loss = 374307.875\n",
      "Iteration 340500 [34050%]: Loss = 376838.812\n",
      "Iteration 340600 [34060%]: Loss = 387824.500\n",
      "Iteration 340700 [34070%]: Loss = 400332.750\n",
      "Iteration 340800 [34080%]: Loss = 385200.875\n",
      "Iteration 340900 [34090%]: Loss = 395398.500\n",
      "Iteration 341000 [34100%]: Loss = 492848.875\n",
      "Iteration 341100 [34110%]: Loss = 494211.625\n",
      "Iteration 341200 [34120%]: Loss = 415371.781\n",
      "Iteration 341300 [34130%]: Loss = 397712.000\n",
      "Iteration 341400 [34140%]: Loss = 397023.719\n",
      "Iteration 341500 [34150%]: Loss = 378661.438\n",
      "Iteration 341600 [34160%]: Loss = 385217.781\n",
      "Iteration 341700 [34170%]: Loss = 380848.594\n",
      "Iteration 341800 [34180%]: Loss = 381136.625\n",
      "Iteration 341900 [34190%]: Loss = 384030.406\n",
      "Iteration 342000 [34200%]: Loss = 396637.000\n",
      "Iteration 342100 [34210%]: Loss = 394680.344\n",
      "Iteration 342200 [34220%]: Loss = 381299.906\n",
      "Iteration 342300 [34230%]: Loss = 384831.250\n",
      "Iteration 342400 [34240%]: Loss = 373755.625\n",
      "Iteration 342500 [34250%]: Loss = 385752.750\n",
      "Iteration 342600 [34260%]: Loss = 404382.719\n",
      "Iteration 342700 [34270%]: Loss = 371687.250\n",
      "Iteration 342800 [34280%]: Loss = 374580.125\n",
      "Iteration 342900 [34290%]: Loss = 378298.594\n",
      "Iteration 343000 [34300%]: Loss = 376380.812\n",
      "Iteration 343100 [34310%]: Loss = 405437.125\n",
      "Iteration 343200 [34320%]: Loss = 379518.531\n",
      "Iteration 343300 [34330%]: Loss = 381427.438\n",
      "Iteration 343400 [34340%]: Loss = 381073.469\n",
      "Iteration 343500 [34350%]: Loss = 382216.656\n",
      "Iteration 343600 [34360%]: Loss = 382047.750\n",
      "Iteration 343700 [34370%]: Loss = 393248.094\n",
      "Iteration 343800 [34380%]: Loss = 384736.469\n",
      "Iteration 343900 [34390%]: Loss = 444991.219\n",
      "Iteration 344000 [34400%]: Loss = 399152.031\n",
      "Iteration 344100 [34410%]: Loss = 422831.875\n",
      "Iteration 344200 [34420%]: Loss = 405584.406\n",
      "Iteration 344300 [34430%]: Loss = 399489.062\n",
      "Iteration 344400 [34440%]: Loss = 418905.438\n",
      "Iteration 344500 [34450%]: Loss = 405564.312\n",
      "Iteration 344600 [34460%]: Loss = 399515.500\n",
      "Iteration 344700 [34470%]: Loss = 401098.719\n",
      "Iteration 344800 [34480%]: Loss = 459167.312\n",
      "Iteration 344900 [34490%]: Loss = 439006.969\n",
      "Iteration 345000 [34500%]: Loss = 459068.188\n",
      "Iteration 345100 [34510%]: Loss = 523201.562\n",
      "Iteration 345200 [34520%]: Loss = 420752.062\n",
      "Iteration 345300 [34530%]: Loss = 386560.250\n",
      "Iteration 345400 [34540%]: Loss = 382462.875\n",
      "Iteration 345500 [34550%]: Loss = 378088.469\n",
      "Iteration 345600 [34560%]: Loss = 399142.500\n",
      "Iteration 345700 [34570%]: Loss = 396945.656\n",
      "Iteration 345800 [34580%]: Loss = 399265.094\n",
      "Iteration 345900 [34590%]: Loss = 396025.688\n",
      "Iteration 346000 [34600%]: Loss = 406285.219\n",
      "Iteration 346100 [34610%]: Loss = 748884.875\n",
      "Iteration 346200 [34620%]: Loss = 386642.750\n",
      "Iteration 346300 [34630%]: Loss = 405329.844\n",
      "Iteration 346400 [34640%]: Loss = 455881.250\n",
      "Iteration 346500 [34650%]: Loss = 449420.469\n",
      "Iteration 346600 [34660%]: Loss = 486834.125\n",
      "Iteration 346700 [34670%]: Loss = 401093.156\n",
      "Iteration 346800 [34680%]: Loss = 396015.156\n",
      "Iteration 346900 [34690%]: Loss = 407888.469\n",
      "Iteration 347000 [34700%]: Loss = 432003.219\n",
      "Iteration 347100 [34710%]: Loss = 446984.719\n",
      "Iteration 347200 [34720%]: Loss = 401097.844\n",
      "Iteration 347300 [34730%]: Loss = 397955.406\n",
      "Iteration 347400 [34740%]: Loss = 446715.438\n",
      "Iteration 347500 [34750%]: Loss = 442529.625\n",
      "Iteration 347600 [34760%]: Loss = 538703.625\n",
      "Iteration 347700 [34770%]: Loss = 491237.438\n",
      "Iteration 347800 [34780%]: Loss = 441153.656\n",
      "Iteration 347900 [34790%]: Loss = 396579.938\n",
      "Iteration 348000 [34800%]: Loss = 676388.062\n",
      "Iteration 348100 [34810%]: Loss = 486664.750\n",
      "Iteration 348200 [34820%]: Loss = 404808.500\n",
      "Iteration 348300 [34830%]: Loss = 486859.750\n",
      "Iteration 348400 [34840%]: Loss = 457306.156\n",
      "Iteration 348500 [34850%]: Loss = 507531.906\n",
      "Iteration 348600 [34860%]: Loss = 509518.156\n",
      "Iteration 348700 [34870%]: Loss = 431050.562\n",
      "Iteration 348800 [34880%]: Loss = 494950.688\n",
      "Iteration 348900 [34890%]: Loss = 451402.781\n",
      "Iteration 349000 [34900%]: Loss = 452033.125\n",
      "Iteration 349100 [34910%]: Loss = 466390.562\n",
      "Iteration 349200 [34920%]: Loss = 429971.000\n",
      "Iteration 349300 [34930%]: Loss = 496697.656\n",
      "Iteration 349400 [34940%]: Loss = 512674.438\n",
      "Iteration 349500 [34950%]: Loss = 1151311.625\n",
      "Iteration 349600 [34960%]: Loss = 551364.875\n",
      "Iteration 349700 [34970%]: Loss = 465016.500\n",
      "Iteration 349800 [34980%]: Loss = 446272.875\n",
      "Iteration 349900 [34990%]: Loss = 456079.812\n",
      "Iteration 350000 [35000%]: Loss = 456879.125\n",
      "Iteration 350100 [35010%]: Loss = 459939.688\n",
      "Iteration 350200 [35020%]: Loss = 570822.750\n",
      "Iteration 350300 [35030%]: Loss = 462525.156\n",
      "Iteration 350400 [35040%]: Loss = 536928.000\n",
      "Iteration 350500 [35050%]: Loss = 498920.375\n",
      "Iteration 350600 [35060%]: Loss = 564014.500\n",
      "Iteration 350700 [35070%]: Loss = 477251.062\n",
      "Iteration 350800 [35080%]: Loss = 559564.500\n",
      "Iteration 350900 [35090%]: Loss = 501713.875\n",
      "Iteration 351000 [35100%]: Loss = 487522.938\n",
      "Iteration 351100 [35110%]: Loss = 447968.406\n",
      "Iteration 351200 [35120%]: Loss = 437531.938\n",
      "Iteration 351300 [35130%]: Loss = 453579.594\n",
      "Iteration 351400 [35140%]: Loss = 421845.188\n",
      "Iteration 351500 [35150%]: Loss = 456072.312\n",
      "Iteration 351600 [35160%]: Loss = 433462.125\n",
      "Iteration 351700 [35170%]: Loss = 456195.906\n",
      "Iteration 351800 [35180%]: Loss = 499370.812\n",
      "Iteration 351900 [35190%]: Loss = 447917.844\n",
      "Iteration 352000 [35200%]: Loss = 411834.438\n",
      "Iteration 352100 [35210%]: Loss = 423313.875\n",
      "Iteration 352200 [35220%]: Loss = 435335.875\n",
      "Iteration 352300 [35230%]: Loss = 427510.969\n",
      "Iteration 352400 [35240%]: Loss = 448172.438\n",
      "Iteration 352500 [35250%]: Loss = 471088.688\n",
      "Iteration 352600 [35260%]: Loss = 465314.688\n",
      "Iteration 352700 [35270%]: Loss = 478281.906\n",
      "Iteration 352800 [35280%]: Loss = 407158.625\n",
      "Iteration 352900 [35290%]: Loss = 390995.281\n",
      "Iteration 353000 [35300%]: Loss = 384283.219\n",
      "Iteration 353100 [35310%]: Loss = 387985.875\n",
      "Iteration 353200 [35320%]: Loss = 424178.500\n",
      "Iteration 353300 [35330%]: Loss = 429158.406\n",
      "Iteration 353400 [35340%]: Loss = 426531.969\n",
      "Iteration 353500 [35350%]: Loss = 397511.094\n",
      "Iteration 353600 [35360%]: Loss = 425823.750\n",
      "Iteration 353700 [35370%]: Loss = 424763.531\n",
      "Iteration 353800 [35380%]: Loss = 441663.219\n",
      "Iteration 353900 [35390%]: Loss = 428630.781\n",
      "Iteration 354000 [35400%]: Loss = 404934.469\n",
      "Iteration 354100 [35410%]: Loss = 432963.812\n",
      "Iteration 354200 [35420%]: Loss = 389300.188\n",
      "Iteration 354300 [35430%]: Loss = 395992.344\n",
      "Iteration 354400 [35440%]: Loss = 440544.812\n",
      "Iteration 354500 [35450%]: Loss = 464424.844\n",
      "Iteration 354600 [35460%]: Loss = 596102.000\n",
      "Iteration 354700 [35470%]: Loss = 437717.281\n",
      "Iteration 354800 [35480%]: Loss = 392644.281\n",
      "Iteration 354900 [35490%]: Loss = 420123.500\n",
      "Iteration 355000 [35500%]: Loss = 424979.094\n",
      "Iteration 355100 [35510%]: Loss = 402593.031\n",
      "Iteration 355200 [35520%]: Loss = 402968.094\n",
      "Iteration 355300 [35530%]: Loss = 418747.000\n",
      "Iteration 355400 [35540%]: Loss = 393313.469\n",
      "Iteration 355500 [35550%]: Loss = 410331.312\n",
      "Iteration 355600 [35560%]: Loss = 390949.531\n",
      "Iteration 355700 [35570%]: Loss = 416597.969\n",
      "Iteration 355800 [35580%]: Loss = 539301.875\n",
      "Iteration 355900 [35590%]: Loss = 432788.375\n",
      "Iteration 356000 [35600%]: Loss = 458203.125\n",
      "Iteration 356100 [35610%]: Loss = 488542.625\n",
      "Iteration 356200 [35620%]: Loss = 459685.531\n",
      "Iteration 356300 [35630%]: Loss = 451767.500\n",
      "Iteration 356400 [35640%]: Loss = 468796.625\n",
      "Iteration 356500 [35650%]: Loss = 451070.281\n",
      "Iteration 356600 [35660%]: Loss = 457328.688\n",
      "Iteration 356700 [35670%]: Loss = 443157.312\n",
      "Iteration 356800 [35680%]: Loss = 446120.125\n",
      "Iteration 356900 [35690%]: Loss = 441241.094\n",
      "Iteration 357000 [35700%]: Loss = 619961.750\n",
      "Iteration 357100 [35710%]: Loss = 547019.875\n",
      "Iteration 357200 [35720%]: Loss = 450797.531\n",
      "Iteration 357300 [35730%]: Loss = 448344.312\n",
      "Iteration 357400 [35740%]: Loss = 441295.594\n",
      "Iteration 357500 [35750%]: Loss = 448756.000\n",
      "Iteration 357600 [35760%]: Loss = 464013.812\n",
      "Iteration 357700 [35770%]: Loss = 431503.938\n",
      "Iteration 357800 [35780%]: Loss = 477441.312\n",
      "Iteration 357900 [35790%]: Loss = 1118864.875\n",
      "Iteration 358000 [35800%]: Loss = 470503.281\n",
      "Iteration 358100 [35810%]: Loss = 481286.094\n",
      "Iteration 358200 [35820%]: Loss = 525299.562\n",
      "Iteration 358300 [35830%]: Loss = 478455.969\n",
      "Iteration 358400 [35840%]: Loss = 467982.188\n",
      "Iteration 358500 [35850%]: Loss = 472750.000\n",
      "Iteration 358600 [35860%]: Loss = 474573.281\n",
      "Iteration 358700 [35870%]: Loss = 545018.312\n",
      "Iteration 358800 [35880%]: Loss = 475577.594\n",
      "Iteration 358900 [35890%]: Loss = 467491.094\n",
      "Iteration 359000 [35900%]: Loss = 567912.000\n",
      "Iteration 359100 [35910%]: Loss = 488464.719\n",
      "Iteration 359200 [35920%]: Loss = 473035.062\n",
      "Iteration 359300 [35930%]: Loss = 648483.750\n",
      "Iteration 359400 [35940%]: Loss = 483649.250\n",
      "Iteration 359500 [35950%]: Loss = 486666.531\n",
      "Iteration 359600 [35960%]: Loss = 481419.969\n",
      "Iteration 359700 [35970%]: Loss = 507924.125\n",
      "Iteration 359800 [35980%]: Loss = 507890.938\n",
      "Iteration 359900 [35990%]: Loss = 513826.969\n",
      "Iteration 360000 [36000%]: Loss = 474092.781\n"
     ]
    }
   ],
   "source": [
    "n_iter = 120000\n",
    "sess = tf.get_default_session()\n",
    "for _ in range(n_iter):\n",
    "    for inf in inf_list:\n",
    "        for _ in range(1):  # make multiple steps along each set of coords\n",
    "            info_dict = inf.update()\n",
    "        if inf is inference_lam:\n",
    "            inf.print_progress(info_dict)\n",
    "    sess.run(inference_latents)\n",
    "    sess.run(inference_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1324af160>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1327a5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD9jdF8L02Ty8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1327a5828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Zmat = q_Z.mean().eval()\n",
    "\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')\n",
    "plt.matshow(Zmat.T, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13247e898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_A.mean().eval().ravel(), label='inferred'); plt.hist(dA.ravel(), label='actual'), plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132461048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_B.mean().eval().ravel(), 200, label='inferred'), plt.hist(dB.ravel(), 200, label='actual'), plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1328edc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_C.mean().eval().ravel(), 200, label='inferred'), plt.hist(dC.ravel(), 200, label='actual'), plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf96UhQfmw+UsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13245bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lam_mu.eval(), 200, label='inferred'), plt.hist(dlam, 200, label='actual'), plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.95266223,  2.20943737,  0.82715327, ...,  2.98056245,\n",
       "        2.72723484,  2.19961119], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_mu.value().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.95266223,  2.20943737,  0.82715327, ...,  2.98056245,\n",
       "        2.72723484,  2.19961119], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lam.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27815196], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sig.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97660524,  0.99759662,  0.99464697,  0.0045814 ], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pi._value.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01886443,  0.01886443,  0.01886443,  0.01886443], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pi.std().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98076922,  0.98076922,  0.98076922,  0.01923077], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pi.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
