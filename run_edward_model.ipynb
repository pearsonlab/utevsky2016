{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687276, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim</th>\n",
       "      <th>unit</th>\n",
       "      <th>isfirst</th>\n",
       "      <th>isrewarded</th>\n",
       "      <th>count</th>\n",
       "      <th>trial</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim  unit  isfirst  isrewarded  count  trial  time\n",
       "0     0  1068        0           0      4  57277     0\n",
       "1     0  1069        0           0      1  57277     0\n",
       "2     0  1070        0           0      6  57277     0\n",
       "3     0  1071        0           0      0  57277     0\n",
       "4     0  1072        1           0      3  60250     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('data/prepared_data.csv')\n",
    "print(dat.shape)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are:\n",
    "- stim: original stimulus number\n",
    "- unit: recorded neuron number\n",
    "- isfirst, isrewarded: potential regressors of interest\n",
    "- count: spike count during stimulus display period\n",
    "- trial: original trial number\n",
    "- time: unique stimulus code in trimmed dataset (very rare stims removed)\n",
    "\n",
    "We need to do a couple of things to prep the data:\n",
    "- turn unit codes into a 1-based unit index\n",
    "- turn time into a 1-based stim index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27038, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset data for testing\n",
    "dat = dat.query(\"unit < 1050 & time < 300\")\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 48 49\n",
      "0 283 284\n"
     ]
    }
   ],
   "source": [
    "_, unit = np.unique(dat.unit, return_inverse=True)\n",
    "_, stim = np.unique(dat.time, return_inverse=True)\n",
    "\n",
    "print(np.min(unit), np.max(unit), len(np.unique(unit)))\n",
    "\n",
    "print(np.min(stim), np.max(stim), len(np.unique(stim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27038, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = dat['count'].values\n",
    "Xdat = dat[['isfirst', 'isrewarded']].values\n",
    "Xdat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = dat.shape[0]  # number of trials\n",
    "NB = 10  # number of trials in minibatch\n",
    "NU = len(np.unique(unit))  # number of units\n",
    "NS = len(np.unique(stim))  # number of stims\n",
    "P = Xdat.shape[1]  # number of specified regressors\n",
    "K = 5  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.train.range_input_producer(N).dequeue_many(NB, name='batch_inds')\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB, name='batch_counts')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=tf.ones((NU, P)), name='B')\n",
    "    C = ed.models.Normal(mu=tf.zeros((NU, K)), sigma=tf.ones((NU, K)), name='C')  \n",
    "    \n",
    "    delta = ed.models.Beta(a=3 * tf.ones(K), b=tf.ones(K), name='delta')\n",
    "    tf.scalar_summary('mean_delta', tf.reduce_mean(delta))\n",
    "    log_delta = tf.log(delta)\n",
    "    tf.scalar_summary('min_log_delta', tf.reduce_min(log_delta))\n",
    "    tf.scalar_summary('mean_log_delta', tf.reduce_mean(log_delta))\n",
    "\n",
    "    pi = tf.exp(tf.cumsum(log_delta), name='pi')\n",
    "    tf.scalar_summary('min_pi', tf.reduce_min(pi))\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "    tf.scalar_summary('mean_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-0.1], sigma=[0.1], name='sig')\n",
    "\n",
    "    lam_vars = (tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, U) * tf.gather(tf.to_float(Z), S), 1))\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "    tf.scalar_summary('mean_lam', tf.reduce_mean(lam))\n",
    "\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.nn.softplus(lam), value=tf.ones(NB), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(tf.random_normal((NS, K))), name='Z')\n",
    "    tf.scalar_summary('mean_q_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    q_delta = ed.models.BetaWithSoftplusAB(a=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           b=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           name='delta')\n",
    "    tf.scalar_summary('mean_q_delta', tf.reduce_mean(q_delta))\n",
    "\n",
    "    lam_mu = tf.Variable(tf.random_normal((N,)))\n",
    "    lam_sig = tf.Variable(tf.random_uniform((N,)))\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "    tf.scalar_summary('mean_q_lam', tf.reduce_mean(q_lam))\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')\n",
    "    tf.scalar_summary('mean_q_sig', tf.reduce_mean(q_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, \n",
    "                    data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      1 [  0%]: Loss = 32376.336\n",
      "Iteration    100 [  0%]: Loss = 367329.406\n",
      "Iteration    200 [  0%]: Loss = 359012.062\n",
      "Iteration    300 [  0%]: Loss = 284934.562\n",
      "Iteration    400 [  0%]: Loss = 403948.594\n",
      "Iteration    500 [  0%]: Loss = 78713.344\n",
      "Iteration    600 [  0%]: Loss = 50482.652\n",
      "Iteration    700 [  0%]: Loss = 1473515.500\n",
      "Iteration    800 [  0%]: Loss = 105033.516\n",
      "Iteration    900 [  0%]: Loss = 872569.125\n",
      "Iteration   1000 [  1%]: Loss = 52607.586\n",
      "Iteration   1100 [  1%]: Loss = 4033088.250\n",
      "Iteration   1200 [  1%]: Loss = 748272.000\n",
      "Iteration   1300 [  1%]: Loss = 29906.422\n",
      "Iteration   1400 [  1%]: Loss = 36473.871\n",
      "Iteration   1500 [  1%]: Loss = 49276.605\n",
      "Iteration   1600 [  1%]: Loss = 43245.121\n",
      "Iteration   1700 [  1%]: Loss = 28256.328\n",
      "Iteration   1800 [  1%]: Loss = 27379.195\n",
      "Iteration   1900 [  1%]: Loss = 250368.062\n",
      "Iteration   2000 [  2%]: Loss = 113161.391\n",
      "Iteration   2100 [  2%]: Loss = 74971.477\n",
      "Iteration   2200 [  2%]: Loss = 45243.535\n",
      "Iteration   2300 [  2%]: Loss = 28554.004\n",
      "Iteration   2400 [  2%]: Loss = 24987.508\n",
      "Iteration   2500 [  2%]: Loss = 1428583.750\n",
      "Iteration   2600 [  2%]: Loss = 48799.762\n",
      "Iteration   2700 [  2%]: Loss = 27056.969\n",
      "Iteration   2800 [  2%]: Loss = 303678.438\n",
      "Iteration   2900 [  2%]: Loss = 1750557.625\n",
      "Iteration   3000 [  3%]: Loss = 248701.016\n",
      "Iteration   3100 [  3%]: Loss = 94856.930\n",
      "Iteration   3200 [  3%]: Loss = 78193.633\n",
      "Iteration   3300 [  3%]: Loss = 109910.719\n",
      "Iteration   3400 [  3%]: Loss = 527317.000\n",
      "Iteration   3500 [  3%]: Loss = 30478.078\n",
      "Iteration   3600 [  3%]: Loss = 1084258.250\n",
      "Iteration   3700 [  3%]: Loss = 25618.691\n",
      "Iteration   3800 [  3%]: Loss = 44271.398\n",
      "Iteration   3900 [  3%]: Loss = 36094.680\n",
      "Iteration   4000 [  4%]: Loss = 41206.473\n",
      "Iteration   4100 [  4%]: Loss = 26066.734\n",
      "Iteration   4200 [  4%]: Loss = 139540448.000\n",
      "Iteration   4300 [  4%]: Loss = 33701.168\n",
      "Iteration   4400 [  4%]: Loss = 62423.254\n",
      "Iteration   4500 [  4%]: Loss = 68934.172\n",
      "Iteration   4600 [  4%]: Loss = 3824461.750\n",
      "Iteration   4700 [  4%]: Loss = 49812.609\n",
      "Iteration   4800 [  4%]: Loss = 76785.328\n",
      "Iteration   4900 [  4%]: Loss = 285559.656\n",
      "Iteration   5000 [  5%]: Loss = 56537.250\n",
      "Iteration   5100 [  5%]: Loss = 20997.086\n",
      "Iteration   5200 [  5%]: Loss = 34979.352\n",
      "Iteration   5300 [  5%]: Loss = 25391.914\n",
      "Iteration   5400 [  5%]: Loss = 415099.094\n",
      "Iteration   5500 [  5%]: Loss = 34990.977\n",
      "Iteration   5600 [  5%]: Loss = 746610.625\n",
      "Iteration   5700 [  5%]: Loss = 423003.688\n",
      "Iteration   5800 [  5%]: Loss = 46811.199\n",
      "Iteration   5900 [  5%]: Loss = 300598.938\n",
      "Iteration   6000 [  6%]: Loss = 90540.375\n",
      "Iteration   6100 [  6%]: Loss = 42424.727\n",
      "Iteration   6200 [  6%]: Loss = 26674.457\n",
      "Iteration   6300 [  6%]: Loss = 31161.988\n",
      "Iteration   6400 [  6%]: Loss = 36997.930\n",
      "Iteration   6500 [  6%]: Loss = 656726.062\n",
      "Iteration   6600 [  6%]: Loss = 214640.062\n",
      "Iteration   6700 [  6%]: Loss = 128261.141\n",
      "Iteration   6800 [  6%]: Loss = 31824.238\n",
      "Iteration   6900 [  6%]: Loss = 20797.062\n",
      "Iteration   7000 [  7%]: Loss = 76540.078\n",
      "Iteration   7100 [  7%]: Loss = 51959.875\n",
      "Iteration   7200 [  7%]: Loss = 1320809.500\n",
      "Iteration   7300 [  7%]: Loss = 28943.648\n",
      "Iteration   7400 [  7%]: Loss = 37916.223\n",
      "Iteration   7500 [  7%]: Loss = 513284.250\n",
      "Iteration   7600 [  7%]: Loss = 38663.625\n",
      "Iteration   7700 [  7%]: Loss = 58149.316\n",
      "Iteration   7800 [  7%]: Loss = 40837.887\n",
      "Iteration   7900 [  7%]: Loss = 34846.078\n",
      "Iteration   8000 [  8%]: Loss = 58597.492\n",
      "Iteration   8100 [  8%]: Loss = 28364.492\n",
      "Iteration   8200 [  8%]: Loss = 35046.523\n",
      "Iteration   8300 [  8%]: Loss = 42352.992\n",
      "Iteration   8400 [  8%]: Loss = 38904.438\n",
      "Iteration   8500 [  8%]: Loss = 57224.566\n",
      "Iteration   8600 [  8%]: Loss = 68314.320\n",
      "Iteration   8700 [  8%]: Loss = 433415.125\n",
      "Iteration   8800 [  8%]: Loss = 39529.965\n",
      "Iteration   8900 [  8%]: Loss = 365356.594\n",
      "Iteration   9000 [  9%]: Loss = 733515.250\n",
      "Iteration   9100 [  9%]: Loss = 28801.453\n",
      "Iteration   9200 [  9%]: Loss = 1890617.500\n",
      "Iteration   9300 [  9%]: Loss = 48033.477\n",
      "Iteration   9400 [  9%]: Loss = 32229.336\n",
      "Iteration   9500 [  9%]: Loss = 429986.562\n",
      "Iteration   9600 [  9%]: Loss = 32953.359\n",
      "Iteration   9700 [  9%]: Loss = 98280.328\n",
      "Iteration   9800 [  9%]: Loss = 32569.930\n",
      "Iteration   9900 [  9%]: Loss = 15157.207\n",
      "Iteration  10000 [ 10%]: Loss = 298390.969\n",
      "Iteration  10100 [ 10%]: Loss = 108818.508\n",
      "Iteration  10200 [ 10%]: Loss = 19647.004\n",
      "Iteration  10300 [ 10%]: Loss = 97767.703\n",
      "Iteration  10400 [ 10%]: Loss = 35659.555\n",
      "Iteration  10500 [ 10%]: Loss = 214648.812\n",
      "Iteration  10600 [ 10%]: Loss = 43834.578\n",
      "Iteration  10700 [ 10%]: Loss = 51207.613\n",
      "Iteration  10800 [ 10%]: Loss = 145323.594\n",
      "Iteration  10900 [ 10%]: Loss = 74101.898\n",
      "Iteration  11000 [ 11%]: Loss = 42911.680\n",
      "Iteration  11100 [ 11%]: Loss = 35275.473\n",
      "Iteration  11200 [ 11%]: Loss = 8518.293\n",
      "Iteration  11300 [ 11%]: Loss = 49070.984\n",
      "Iteration  11400 [ 11%]: Loss = 29876.410\n",
      "Iteration  11500 [ 11%]: Loss = 30494.426\n",
      "Iteration  11600 [ 11%]: Loss = 55455.309\n",
      "Iteration  11700 [ 11%]: Loss = 32594.832\n",
      "Iteration  11800 [ 11%]: Loss = 38378.742\n",
      "Iteration  11900 [ 11%]: Loss = 39514.762\n",
      "Iteration  12000 [ 12%]: Loss = 79254.688\n",
      "Iteration  12100 [ 12%]: Loss = 49222.324\n",
      "Iteration  12200 [ 12%]: Loss = 33375.348\n",
      "Iteration  12300 [ 12%]: Loss = 69620.922\n",
      "Iteration  12400 [ 12%]: Loss = 36903.777\n",
      "Iteration  12500 [ 12%]: Loss = 50566.449\n",
      "Iteration  12600 [ 12%]: Loss = 33576.203\n",
      "Iteration  12700 [ 12%]: Loss = 34103.035\n",
      "Iteration  12800 [ 12%]: Loss = 31618.250\n",
      "Iteration  12900 [ 12%]: Loss = 37466.582\n",
      "Iteration  13000 [ 13%]: Loss = 45066.523\n",
      "Iteration  13100 [ 13%]: Loss = 69411.180\n",
      "Iteration  13200 [ 13%]: Loss = 33296.188\n",
      "Iteration  13300 [ 13%]: Loss = 37000.742\n",
      "Iteration  13400 [ 13%]: Loss = 30749.953\n",
      "Iteration  13500 [ 13%]: Loss = 36325.223\n",
      "Iteration  13600 [ 13%]: Loss = 86607.609\n",
      "Iteration  13700 [ 13%]: Loss = 33567.305\n",
      "Iteration  13800 [ 13%]: Loss = 78874.586\n",
      "Iteration  13900 [ 13%]: Loss = 52292.547\n",
      "Iteration  14000 [ 14%]: Loss = 40395.238\n",
      "Iteration  14100 [ 14%]: Loss = 43812.008\n",
      "Iteration  14200 [ 14%]: Loss = 19613.637\n",
      "Iteration  14300 [ 14%]: Loss = 24246.477\n",
      "Iteration  14400 [ 14%]: Loss = 37460.844\n",
      "Iteration  14500 [ 14%]: Loss = 28163.828\n",
      "Iteration  14600 [ 14%]: Loss = 45398.406\n",
      "Iteration  14700 [ 14%]: Loss = 61616.863\n",
      "Iteration  14800 [ 14%]: Loss = 23277.879\n",
      "Iteration  14900 [ 14%]: Loss = 23874.566\n",
      "Iteration  15000 [ 15%]: Loss = 42833.723\n",
      "Iteration  15100 [ 15%]: Loss = 604878.062\n",
      "Iteration  15200 [ 15%]: Loss = 74874.781\n",
      "Iteration  15300 [ 15%]: Loss = 38239.027\n",
      "Iteration  15400 [ 15%]: Loss = 24804.180\n",
      "Iteration  15500 [ 15%]: Loss = 53265.953\n",
      "Iteration  15600 [ 15%]: Loss = 21070.777\n",
      "Iteration  15700 [ 15%]: Loss = 34440.633\n",
      "Iteration  15800 [ 15%]: Loss = 44631.383\n",
      "Iteration  15900 [ 15%]: Loss = 31650.641\n",
      "Iteration  16000 [ 16%]: Loss = 30644.348\n",
      "Iteration  16100 [ 16%]: Loss = 55868.516\n",
      "Iteration  16200 [ 16%]: Loss = 413303.000\n",
      "Iteration  16300 [ 16%]: Loss = 34502.523\n",
      "Iteration  16400 [ 16%]: Loss = 44992.402\n",
      "Iteration  16500 [ 16%]: Loss = 52534.910\n",
      "Iteration  16600 [ 16%]: Loss = 40304.383\n",
      "Iteration  16700 [ 16%]: Loss = 34843.191\n",
      "Iteration  16800 [ 16%]: Loss = 64280.637\n",
      "Iteration  16900 [ 16%]: Loss = 34370.461\n",
      "Iteration  17000 [ 17%]: Loss = 12486.605\n",
      "Iteration  17100 [ 17%]: Loss = 46275.234\n",
      "Iteration  17200 [ 17%]: Loss = 37235.961\n",
      "Iteration  17300 [ 17%]: Loss = 45109.078\n",
      "Iteration  17400 [ 17%]: Loss = 48622.629\n",
      "Iteration  17500 [ 17%]: Loss = 37268.816\n",
      "Iteration  17600 [ 17%]: Loss = 44014.117\n",
      "Iteration  17700 [ 17%]: Loss = 33511.043\n",
      "Iteration  17800 [ 17%]: Loss = 74020.102\n",
      "Iteration  17900 [ 17%]: Loss = 44230.578\n",
      "Iteration  18000 [ 18%]: Loss = 33520.512\n",
      "Iteration  18100 [ 18%]: Loss = 33204.520\n",
      "Iteration  18200 [ 18%]: Loss = 46404.734\n",
      "Iteration  18300 [ 18%]: Loss = 44804.602\n",
      "Iteration  18400 [ 18%]: Loss = 36916.762\n",
      "Iteration  18500 [ 18%]: Loss = 44984.117\n",
      "Iteration  18600 [ 18%]: Loss = 58391.316\n",
      "Iteration  18700 [ 18%]: Loss = 39246.523\n",
      "Iteration  18800 [ 18%]: Loss = 32345.750\n",
      "Iteration  18900 [ 18%]: Loss = 37530.547\n",
      "Iteration  19000 [ 19%]: Loss = 56278.836\n",
      "Iteration  19100 [ 19%]: Loss = 35077.605\n",
      "Iteration  19200 [ 19%]: Loss = 39568.383\n",
      "Iteration  19300 [ 19%]: Loss = 32818.410\n",
      "Iteration  19400 [ 19%]: Loss = 70589.406\n",
      "Iteration  19500 [ 19%]: Loss = 34618.914\n",
      "Iteration  19600 [ 19%]: Loss = 55658.387\n",
      "Iteration  19700 [ 19%]: Loss = 45083.762\n",
      "Iteration  19800 [ 19%]: Loss = 40574.211\n",
      "Iteration  19900 [ 19%]: Loss = 31773.465\n",
      "Iteration  20000 [ 20%]: Loss = 44170.645\n",
      "Iteration  20100 [ 20%]: Loss = 39551.945\n",
      "Iteration  20200 [ 20%]: Loss = 40521.832\n",
      "Iteration  20300 [ 20%]: Loss = 38308.250\n",
      "Iteration  20400 [ 20%]: Loss = 46215.156\n",
      "Iteration  20500 [ 20%]: Loss = 41078.977\n",
      "Iteration  20600 [ 20%]: Loss = 32914.078\n",
      "Iteration  20700 [ 20%]: Loss = 53032.344\n",
      "Iteration  20800 [ 20%]: Loss = 41676.672\n",
      "Iteration  20900 [ 20%]: Loss = 49350.055\n",
      "Iteration  21000 [ 21%]: Loss = 55398.918\n",
      "Iteration  21100 [ 21%]: Loss = 44083.164\n",
      "Iteration  21200 [ 21%]: Loss = 37768.031\n",
      "Iteration  21300 [ 21%]: Loss = 36893.832\n",
      "Iteration  21400 [ 21%]: Loss = 25793.926\n",
      "Iteration  21500 [ 21%]: Loss = 74432.109\n",
      "Iteration  21600 [ 21%]: Loss = 30947.059\n",
      "Iteration  21700 [ 21%]: Loss = 23819.816\n",
      "Iteration  21800 [ 21%]: Loss = 36545.613\n",
      "Iteration  21900 [ 21%]: Loss = 22719.148\n",
      "Iteration  22000 [ 22%]: Loss = 39693.496\n",
      "Iteration  22100 [ 22%]: Loss = 40322.562\n",
      "Iteration  22200 [ 22%]: Loss = 29153.492\n",
      "Iteration  22300 [ 22%]: Loss = 47361.762\n",
      "Iteration  22400 [ 22%]: Loss = 30295.703\n",
      "Iteration  22500 [ 22%]: Loss = 29377.082\n",
      "Iteration  22600 [ 22%]: Loss = 32669.441\n",
      "Iteration  22700 [ 22%]: Loss = 74807.891\n",
      "Iteration  22800 [ 22%]: Loss = 29480.098\n",
      "Iteration  22900 [ 22%]: Loss = 46215.789\n",
      "Iteration  23000 [ 23%]: Loss = 33110.266\n",
      "Iteration  23100 [ 23%]: Loss = 31194.480\n",
      "Iteration  23200 [ 23%]: Loss = 25057.883\n",
      "Iteration  23300 [ 23%]: Loss = 32489.457\n",
      "Iteration  23400 [ 23%]: Loss = 36926.785\n",
      "Iteration  23500 [ 23%]: Loss = 36360.117\n",
      "Iteration  23600 [ 23%]: Loss = 45370.859\n",
      "Iteration  23700 [ 23%]: Loss = 52873.414\n",
      "Iteration  23800 [ 23%]: Loss = 41848.742\n",
      "Iteration  23900 [ 23%]: Loss = 23254.238\n",
      "Iteration  24000 [ 24%]: Loss = 81100.828\n",
      "Iteration  24100 [ 24%]: Loss = 7841.477\n",
      "Iteration  24200 [ 24%]: Loss = 28133.191\n",
      "Iteration  24300 [ 24%]: Loss = 51165.219\n",
      "Iteration  24400 [ 24%]: Loss = 24324.867\n",
      "Iteration  24500 [ 24%]: Loss = 20119.293\n",
      "Iteration  24600 [ 24%]: Loss = 20549.078\n",
      "Iteration  24700 [ 24%]: Loss = 20049.957\n",
      "Iteration  24800 [ 24%]: Loss = 42968.406\n",
      "Iteration  24900 [ 24%]: Loss = 56154.609\n",
      "Iteration  25000 [ 25%]: Loss = 42845.688\n",
      "Iteration  25100 [ 25%]: Loss = 39383.727\n",
      "Iteration  25200 [ 25%]: Loss = 56761.938\n",
      "Iteration  25300 [ 25%]: Loss = 34560.883\n",
      "Iteration  25400 [ 25%]: Loss = 25814.367\n",
      "Iteration  25500 [ 25%]: Loss = 26138.914\n",
      "Iteration  25600 [ 25%]: Loss = 38883.508\n",
      "Iteration  25700 [ 25%]: Loss = 35505.527\n",
      "Iteration  25800 [ 25%]: Loss = 42200.477\n",
      "Iteration  25900 [ 25%]: Loss = 38752.941\n",
      "Iteration  26000 [ 26%]: Loss = 30636.988\n",
      "Iteration  26100 [ 26%]: Loss = 41682.797\n",
      "Iteration  26200 [ 26%]: Loss = 54223.227\n",
      "Iteration  26300 [ 26%]: Loss = 53966.188\n",
      "Iteration  26400 [ 26%]: Loss = 42583.180\n",
      "Iteration  26500 [ 26%]: Loss = 28619.590\n",
      "Iteration  26600 [ 26%]: Loss = 47494.965\n",
      "Iteration  26700 [ 26%]: Loss = 49300.969\n",
      "Iteration  26800 [ 26%]: Loss = 51004.520\n",
      "Iteration  26900 [ 26%]: Loss = 48154.008\n",
      "Iteration  27000 [ 27%]: Loss = 39117.027\n",
      "Iteration  27100 [ 27%]: Loss = 47390.719\n",
      "Iteration  27200 [ 27%]: Loss = 46243.129\n",
      "Iteration  27300 [ 27%]: Loss = 49010.168\n",
      "Iteration  27400 [ 27%]: Loss = 36541.449\n",
      "Iteration  27500 [ 27%]: Loss = 46133.605\n",
      "Iteration  27600 [ 27%]: Loss = 32588.125\n",
      "Iteration  27700 [ 27%]: Loss = 28648.465\n",
      "Iteration  27800 [ 27%]: Loss = 41692.934\n",
      "Iteration  27900 [ 27%]: Loss = 40261.488\n",
      "Iteration  28000 [ 28%]: Loss = 35127.977\n",
      "Iteration  28100 [ 28%]: Loss = 24268.680\n",
      "Iteration  28200 [ 28%]: Loss = 29717.168\n",
      "Iteration  28300 [ 28%]: Loss = 45914.855\n",
      "Iteration  28400 [ 28%]: Loss = 31599.457\n",
      "Iteration  28500 [ 28%]: Loss = 65788.617\n",
      "Iteration  28600 [ 28%]: Loss = 52654.590\n",
      "Iteration  28700 [ 28%]: Loss = 25918.781\n",
      "Iteration  28800 [ 28%]: Loss = 35021.977\n",
      "Iteration  28900 [ 28%]: Loss = 44938.078\n",
      "Iteration  29000 [ 28%]: Loss = 30373.832\n",
      "Iteration  29100 [ 29%]: Loss = 20874.289\n",
      "Iteration  29200 [ 29%]: Loss = 45886.750\n",
      "Iteration  29300 [ 29%]: Loss = 31452.625\n",
      "Iteration  29400 [ 29%]: Loss = 52735.430\n",
      "Iteration  29500 [ 29%]: Loss = 34572.656\n",
      "Iteration  29600 [ 29%]: Loss = 38577.461\n",
      "Iteration  29700 [ 29%]: Loss = 38141.746\n",
      "Iteration  29800 [ 29%]: Loss = 39089.730\n",
      "Iteration  29900 [ 29%]: Loss = 33791.754\n",
      "Iteration  30000 [ 30%]: Loss = 34185.633\n",
      "Iteration  30100 [ 30%]: Loss = 47524.797\n",
      "Iteration  30200 [ 30%]: Loss = 63911.223\n",
      "Iteration  30300 [ 30%]: Loss = 62509.605\n",
      "Iteration  30400 [ 30%]: Loss = 45236.688\n",
      "Iteration  30500 [ 30%]: Loss = 44746.574\n",
      "Iteration  30600 [ 30%]: Loss = 50720.543\n",
      "Iteration  30700 [ 30%]: Loss = 26359.703\n",
      "Iteration  30800 [ 30%]: Loss = 33494.496\n",
      "Iteration  30900 [ 30%]: Loss = 48776.027\n",
      "Iteration  31000 [ 31%]: Loss = 46189.137\n",
      "Iteration  31100 [ 31%]: Loss = 34160.883\n",
      "Iteration  31200 [ 31%]: Loss = 42769.867\n",
      "Iteration  31300 [ 31%]: Loss = 40477.445\n",
      "Iteration  31400 [ 31%]: Loss = 38747.258\n",
      "Iteration  31500 [ 31%]: Loss = 43005.547\n",
      "Iteration  31600 [ 31%]: Loss = 28561.516\n",
      "Iteration  31700 [ 31%]: Loss = 36942.094\n",
      "Iteration  31800 [ 31%]: Loss = 25976.539\n",
      "Iteration  31900 [ 31%]: Loss = 26349.578\n",
      "Iteration  32000 [ 32%]: Loss = 27227.750\n",
      "Iteration  32100 [ 32%]: Loss = 42620.594\n",
      "Iteration  32200 [ 32%]: Loss = 41468.004\n",
      "Iteration  32300 [ 32%]: Loss = 44603.895\n",
      "Iteration  32400 [ 32%]: Loss = 16738.250\n",
      "Iteration  32500 [ 32%]: Loss = 27070.047\n",
      "Iteration  32600 [ 32%]: Loss = 32737.555\n",
      "Iteration  32700 [ 32%]: Loss = 32642.273\n",
      "Iteration  32800 [ 32%]: Loss = 37865.844\n",
      "Iteration  32900 [ 32%]: Loss = 40930.590\n",
      "Iteration  33000 [ 33%]: Loss = 32381.133\n",
      "Iteration  33100 [ 33%]: Loss = 18724.547\n",
      "Iteration  33200 [ 33%]: Loss = 42197.262\n",
      "Iteration  33300 [ 33%]: Loss = 65251.633\n",
      "Iteration  33400 [ 33%]: Loss = 32991.617\n",
      "Iteration  33500 [ 33%]: Loss = 56411.703\n",
      "Iteration  33600 [ 33%]: Loss = 36025.309\n",
      "Iteration  33700 [ 33%]: Loss = 34170.281\n",
      "Iteration  33800 [ 33%]: Loss = 38734.309\n",
      "Iteration  33900 [ 33%]: Loss = 44940.953\n",
      "Iteration  34000 [ 34%]: Loss = 60822.242\n",
      "Iteration  34100 [ 34%]: Loss = 28836.664\n",
      "Iteration  34200 [ 34%]: Loss = 33456.449\n",
      "Iteration  34300 [ 34%]: Loss = 36459.574\n",
      "Iteration  34400 [ 34%]: Loss = 22744.215\n",
      "Iteration  34500 [ 34%]: Loss = 40408.562\n",
      "Iteration  34600 [ 34%]: Loss = 30148.207\n",
      "Iteration  34700 [ 34%]: Loss = 35913.430\n",
      "Iteration  34800 [ 34%]: Loss = 46206.383\n",
      "Iteration  34900 [ 34%]: Loss = 44480.996\n",
      "Iteration  35000 [ 35%]: Loss = 24010.688\n",
      "Iteration  35100 [ 35%]: Loss = 26451.484\n",
      "Iteration  35200 [ 35%]: Loss = 25690.680\n",
      "Iteration  35300 [ 35%]: Loss = 41147.750\n",
      "Iteration  35400 [ 35%]: Loss = 44253.879\n",
      "Iteration  35500 [ 35%]: Loss = 38058.730\n",
      "Iteration  35600 [ 35%]: Loss = 17517.312\n",
      "Iteration  35700 [ 35%]: Loss = 60464.477\n",
      "Iteration  35800 [ 35%]: Loss = 41122.359\n",
      "Iteration  35900 [ 35%]: Loss = 36392.367\n",
      "Iteration  36000 [ 36%]: Loss = 36175.258\n",
      "Iteration  36100 [ 36%]: Loss = 35464.832\n",
      "Iteration  36200 [ 36%]: Loss = 18696.781\n",
      "Iteration  36300 [ 36%]: Loss = 23124.109\n",
      "Iteration  36400 [ 36%]: Loss = 31901.141\n",
      "Iteration  36500 [ 36%]: Loss = 44056.617\n",
      "Iteration  36600 [ 36%]: Loss = 25958.148\n",
      "Iteration  36700 [ 36%]: Loss = 46554.430\n",
      "Iteration  36800 [ 36%]: Loss = 35563.824\n",
      "Iteration  36900 [ 36%]: Loss = 16637.367\n",
      "Iteration  37000 [ 37%]: Loss = 33709.840\n",
      "Iteration  37100 [ 37%]: Loss = 48929.770\n",
      "Iteration  37200 [ 37%]: Loss = 27643.289\n",
      "Iteration  37300 [ 37%]: Loss = 43746.500\n",
      "Iteration  37400 [ 37%]: Loss = 48254.539\n",
      "Iteration  37500 [ 37%]: Loss = 48450.965\n",
      "Iteration  37600 [ 37%]: Loss = 30587.742\n",
      "Iteration  37700 [ 37%]: Loss = 28441.965\n",
      "Iteration  37800 [ 37%]: Loss = 35392.305\n",
      "Iteration  37900 [ 37%]: Loss = 36840.594\n",
      "Iteration  38000 [ 38%]: Loss = 27860.418\n",
      "Iteration  38100 [ 38%]: Loss = 43982.203\n",
      "Iteration  38200 [ 38%]: Loss = 30368.898\n",
      "Iteration  38300 [ 38%]: Loss = 22369.625\n",
      "Iteration  38400 [ 38%]: Loss = 26044.852\n",
      "Iteration  38500 [ 38%]: Loss = 33962.863\n",
      "Iteration  38600 [ 38%]: Loss = 26214.328\n",
      "Iteration  38700 [ 38%]: Loss = 42226.148\n",
      "Iteration  38800 [ 38%]: Loss = 18983.934\n",
      "Iteration  38900 [ 38%]: Loss = 35011.090\n",
      "Iteration  39000 [ 39%]: Loss = 44889.312\n",
      "Iteration  39100 [ 39%]: Loss = 31718.176\n",
      "Iteration  39200 [ 39%]: Loss = 45473.469\n",
      "Iteration  39300 [ 39%]: Loss = 33573.309\n",
      "Iteration  39400 [ 39%]: Loss = 32828.730\n",
      "Iteration  39500 [ 39%]: Loss = 38955.121\n",
      "Iteration  39600 [ 39%]: Loss = 38622.812\n",
      "Iteration  39700 [ 39%]: Loss = 42364.660\n",
      "Iteration  39800 [ 39%]: Loss = 43967.762\n",
      "Iteration  39900 [ 39%]: Loss = 80530.844\n",
      "Iteration  40000 [ 40%]: Loss = 29735.320\n",
      "Iteration  40100 [ 40%]: Loss = 33756.664\n",
      "Iteration  40200 [ 40%]: Loss = 30579.887\n",
      "Iteration  40300 [ 40%]: Loss = 20944.766\n",
      "Iteration  40400 [ 40%]: Loss = 52725.547\n",
      "Iteration  40500 [ 40%]: Loss = 29324.926\n",
      "Iteration  40600 [ 40%]: Loss = 41904.082\n",
      "Iteration  40700 [ 40%]: Loss = 26613.477\n",
      "Iteration  40800 [ 40%]: Loss = 28211.465\n",
      "Iteration  40900 [ 40%]: Loss = 32360.934\n",
      "Iteration  41000 [ 41%]: Loss = 25555.234\n",
      "Iteration  41100 [ 41%]: Loss = 42791.121\n",
      "Iteration  41200 [ 41%]: Loss = 49153.305\n",
      "Iteration  41300 [ 41%]: Loss = 36285.824\n",
      "Iteration  41400 [ 41%]: Loss = 36052.785\n",
      "Iteration  41500 [ 41%]: Loss = 36846.750\n",
      "Iteration  41600 [ 41%]: Loss = 33862.797\n",
      "Iteration  41700 [ 41%]: Loss = 32770.312\n",
      "Iteration  41800 [ 41%]: Loss = 38594.359\n",
      "Iteration  41900 [ 41%]: Loss = 27475.871\n",
      "Iteration  42000 [ 42%]: Loss = 28685.352\n",
      "Iteration  42100 [ 42%]: Loss = 25854.633\n",
      "Iteration  42200 [ 42%]: Loss = 30517.238\n",
      "Iteration  42300 [ 42%]: Loss = 38839.410\n",
      "Iteration  42400 [ 42%]: Loss = 27850.809\n",
      "Iteration  42500 [ 42%]: Loss = 40554.074\n",
      "Iteration  42600 [ 42%]: Loss = 28941.324\n",
      "Iteration  42700 [ 42%]: Loss = 33639.203\n",
      "Iteration  42800 [ 42%]: Loss = 31269.055\n",
      "Iteration  42900 [ 42%]: Loss = 23538.695\n",
      "Iteration  43000 [ 43%]: Loss = 34259.660\n",
      "Iteration  43100 [ 43%]: Loss = 23777.227\n",
      "Iteration  43200 [ 43%]: Loss = 20345.859\n",
      "Iteration  43300 [ 43%]: Loss = 24474.887\n",
      "Iteration  43400 [ 43%]: Loss = 45157.434\n",
      "Iteration  43500 [ 43%]: Loss = 20994.492\n",
      "Iteration  43600 [ 43%]: Loss = 37506.750\n",
      "Iteration  43700 [ 43%]: Loss = 20945.934\n",
      "Iteration  43800 [ 43%]: Loss = 32752.754\n",
      "Iteration  43900 [ 43%]: Loss = 39414.773\n",
      "Iteration  44000 [ 44%]: Loss = 23916.156\n",
      "Iteration  44100 [ 44%]: Loss = 35412.027\n",
      "Iteration  44200 [ 44%]: Loss = 24982.531\n",
      "Iteration  44300 [ 44%]: Loss = 32913.129\n",
      "Iteration  44400 [ 44%]: Loss = 30358.340\n",
      "Iteration  44500 [ 44%]: Loss = 24546.234\n",
      "Iteration  44600 [ 44%]: Loss = 40509.000\n",
      "Iteration  44700 [ 44%]: Loss = 47771.543\n",
      "Iteration  44800 [ 44%]: Loss = 60662.707\n",
      "Iteration  44900 [ 44%]: Loss = 16389.902\n",
      "Iteration  45000 [ 45%]: Loss = 27107.523\n",
      "Iteration  45100 [ 45%]: Loss = 40641.797\n",
      "Iteration  45200 [ 45%]: Loss = 32452.711\n",
      "Iteration  45300 [ 45%]: Loss = 31725.527\n",
      "Iteration  45400 [ 45%]: Loss = 32798.230\n",
      "Iteration  45500 [ 45%]: Loss = 41851.293\n",
      "Iteration  45600 [ 45%]: Loss = 31135.008\n",
      "Iteration  45700 [ 45%]: Loss = 31772.512\n",
      "Iteration  45800 [ 45%]: Loss = 43157.258\n",
      "Iteration  45900 [ 45%]: Loss = 30276.047\n",
      "Iteration  46000 [ 46%]: Loss = 54251.473\n",
      "Iteration  46100 [ 46%]: Loss = 24266.988\n",
      "Iteration  46200 [ 46%]: Loss = 22939.910\n",
      "Iteration  46300 [ 46%]: Loss = 54106.992\n",
      "Iteration  46400 [ 46%]: Loss = 39746.754\n",
      "Iteration  46500 [ 46%]: Loss = 41221.492\n",
      "Iteration  46600 [ 46%]: Loss = 29399.699\n",
      "Iteration  46700 [ 46%]: Loss = 27021.918\n",
      "Iteration  46800 [ 46%]: Loss = 28096.801\n",
      "Iteration  46900 [ 46%]: Loss = 25987.723\n",
      "Iteration  47000 [ 47%]: Loss = 31448.297\n",
      "Iteration  47100 [ 47%]: Loss = 28419.754\n",
      "Iteration  47200 [ 47%]: Loss = 18142.355\n",
      "Iteration  47300 [ 47%]: Loss = 36216.863\n",
      "Iteration  47400 [ 47%]: Loss = 25703.898\n",
      "Iteration  47500 [ 47%]: Loss = 26882.707\n",
      "Iteration  47600 [ 47%]: Loss = 30472.785\n",
      "Iteration  47700 [ 47%]: Loss = 29135.234\n",
      "Iteration  47800 [ 47%]: Loss = 27835.746\n",
      "Iteration  47900 [ 47%]: Loss = 24475.559\n",
      "Iteration  48000 [ 48%]: Loss = 28762.277\n",
      "Iteration  48100 [ 48%]: Loss = 26227.500\n",
      "Iteration  48200 [ 48%]: Loss = 41565.574\n",
      "Iteration  48300 [ 48%]: Loss = 32584.422\n",
      "Iteration  48400 [ 48%]: Loss = 23823.848\n",
      "Iteration  48500 [ 48%]: Loss = 39942.754\n",
      "Iteration  48600 [ 48%]: Loss = 32951.910\n",
      "Iteration  48700 [ 48%]: Loss = 17277.199\n",
      "Iteration  48800 [ 48%]: Loss = 22054.145\n",
      "Iteration  48900 [ 48%]: Loss = 36446.426\n",
      "Iteration  49000 [ 49%]: Loss = 29306.965\n",
      "Iteration  49100 [ 49%]: Loss = 40892.105\n",
      "Iteration  49200 [ 49%]: Loss = 32584.043\n",
      "Iteration  49300 [ 49%]: Loss = 28638.805\n",
      "Iteration  49400 [ 49%]: Loss = 35839.262\n",
      "Iteration  49500 [ 49%]: Loss = 16349.129\n",
      "Iteration  49600 [ 49%]: Loss = 58504.500\n",
      "Iteration  49700 [ 49%]: Loss = 21029.902\n",
      "Iteration  49800 [ 49%]: Loss = 31959.562\n",
      "Iteration  49900 [ 49%]: Loss = 41049.996\n",
      "Iteration  50000 [ 50%]: Loss = 31184.432\n",
      "Iteration  50100 [ 50%]: Loss = 21515.363\n",
      "Iteration  50200 [ 50%]: Loss = 38278.051\n",
      "Iteration  50300 [ 50%]: Loss = 24766.785\n",
      "Iteration  50400 [ 50%]: Loss = 19854.930\n",
      "Iteration  50500 [ 50%]: Loss = 21238.996\n",
      "Iteration  50600 [ 50%]: Loss = 45730.387\n",
      "Iteration  50700 [ 50%]: Loss = 39810.727\n",
      "Iteration  50800 [ 50%]: Loss = 32331.020\n",
      "Iteration  50900 [ 50%]: Loss = 32972.684\n",
      "Iteration  51000 [ 51%]: Loss = 38534.230\n",
      "Iteration  51100 [ 51%]: Loss = 31386.930\n",
      "Iteration  51200 [ 51%]: Loss = 24272.625\n",
      "Iteration  51300 [ 51%]: Loss = 32179.145\n",
      "Iteration  51400 [ 51%]: Loss = 37624.477\n",
      "Iteration  51500 [ 51%]: Loss = 27731.156\n",
      "Iteration  51600 [ 51%]: Loss = 25032.602\n",
      "Iteration  51700 [ 51%]: Loss = 32606.949\n",
      "Iteration  51800 [ 51%]: Loss = 26005.961\n",
      "Iteration  51900 [ 51%]: Loss = 31973.871\n",
      "Iteration  52000 [ 52%]: Loss = 22390.699\n",
      "Iteration  52100 [ 52%]: Loss = 40236.633\n",
      "Iteration  52200 [ 52%]: Loss = 15162.680\n",
      "Iteration  52300 [ 52%]: Loss = 26022.965\n",
      "Iteration  52400 [ 52%]: Loss = 20944.910\n",
      "Iteration  52500 [ 52%]: Loss = 29756.008\n",
      "Iteration  52600 [ 52%]: Loss = 22709.816\n",
      "Iteration  52700 [ 52%]: Loss = 31958.805\n",
      "Iteration  52800 [ 52%]: Loss = 35743.961\n",
      "Iteration  52900 [ 52%]: Loss = 29481.527\n",
      "Iteration  53000 [ 53%]: Loss = 26950.020\n",
      "Iteration  53100 [ 53%]: Loss = 28803.270\n",
      "Iteration  53200 [ 53%]: Loss = 35453.961\n",
      "Iteration  53300 [ 53%]: Loss = 36152.324\n",
      "Iteration  53400 [ 53%]: Loss = 41504.602\n",
      "Iteration  53500 [ 53%]: Loss = 42082.969\n",
      "Iteration  53600 [ 53%]: Loss = 38964.258\n",
      "Iteration  53700 [ 53%]: Loss = 18783.785\n",
      "Iteration  53800 [ 53%]: Loss = 29850.508\n",
      "Iteration  53900 [ 53%]: Loss = 38132.012\n",
      "Iteration  54000 [ 54%]: Loss = 34435.199\n",
      "Iteration  54100 [ 54%]: Loss = 28197.016\n",
      "Iteration  54200 [ 54%]: Loss = 44163.602\n",
      "Iteration  54300 [ 54%]: Loss = 38926.434\n",
      "Iteration  54400 [ 54%]: Loss = 23493.012\n",
      "Iteration  54500 [ 54%]: Loss = 41533.352\n",
      "Iteration  54600 [ 54%]: Loss = 33682.598\n",
      "Iteration  54700 [ 54%]: Loss = 31212.352\n",
      "Iteration  54800 [ 54%]: Loss = 26534.207\n",
      "Iteration  54900 [ 54%]: Loss = 43943.719\n",
      "Iteration  55000 [ 55%]: Loss = 25287.242\n",
      "Iteration  55100 [ 55%]: Loss = 39689.176\n",
      "Iteration  55200 [ 55%]: Loss = 36715.582\n",
      "Iteration  55300 [ 55%]: Loss = 38414.199\n",
      "Iteration  55400 [ 55%]: Loss = 36104.250\n",
      "Iteration  55500 [ 55%]: Loss = 34075.602\n",
      "Iteration  55600 [ 55%]: Loss = 29360.246\n",
      "Iteration  55700 [ 55%]: Loss = 30644.070\n",
      "Iteration  55800 [ 55%]: Loss = 31936.254\n",
      "Iteration  55900 [ 55%]: Loss = 44493.723\n",
      "Iteration  56000 [ 56%]: Loss = 33140.086\n",
      "Iteration  56100 [ 56%]: Loss = 26827.895\n",
      "Iteration  56200 [ 56%]: Loss = 13879.582\n",
      "Iteration  56300 [ 56%]: Loss = 26543.629\n",
      "Iteration  56400 [ 56%]: Loss = 20623.234\n",
      "Iteration  56500 [ 56%]: Loss = 15109.867\n",
      "Iteration  56600 [ 56%]: Loss = 42040.863\n",
      "Iteration  56700 [ 56%]: Loss = 22911.090\n",
      "Iteration  56800 [ 56%]: Loss = 40627.277\n",
      "Iteration  56900 [ 56%]: Loss = 23290.789\n",
      "Iteration  57000 [ 56%]: Loss = 28250.605\n",
      "Iteration  57100 [ 57%]: Loss = 31161.152\n",
      "Iteration  57200 [ 57%]: Loss = 40689.184\n",
      "Iteration  57300 [ 57%]: Loss = 28582.844\n",
      "Iteration  57400 [ 57%]: Loss = 24707.570\n",
      "Iteration  57500 [ 57%]: Loss = 31123.375\n",
      "Iteration  57600 [ 57%]: Loss = 36052.945\n",
      "Iteration  57700 [ 57%]: Loss = 25149.465\n",
      "Iteration  57800 [ 57%]: Loss = 25884.008\n",
      "Iteration  57900 [ 57%]: Loss = 32067.605\n",
      "Iteration  58000 [ 57%]: Loss = 29109.195\n",
      "Iteration  58100 [ 58%]: Loss = 40248.688\n",
      "Iteration  58200 [ 58%]: Loss = 34752.090\n",
      "Iteration  58300 [ 58%]: Loss = 21367.207\n",
      "Iteration  58400 [ 58%]: Loss = 28428.637\n",
      "Iteration  58500 [ 58%]: Loss = 33774.125\n",
      "Iteration  58600 [ 58%]: Loss = 30078.273\n",
      "Iteration  58700 [ 58%]: Loss = 44131.609\n",
      "Iteration  58800 [ 58%]: Loss = 38176.043\n",
      "Iteration  58900 [ 58%]: Loss = 19871.137\n",
      "Iteration  59000 [ 59%]: Loss = 22997.043\n",
      "Iteration  59100 [ 59%]: Loss = 28112.488\n",
      "Iteration  59200 [ 59%]: Loss = 36569.258\n",
      "Iteration  59300 [ 59%]: Loss = 31079.348\n",
      "Iteration  59400 [ 59%]: Loss = 25655.762\n",
      "Iteration  59500 [ 59%]: Loss = 18929.301\n",
      "Iteration  59600 [ 59%]: Loss = 29680.203\n",
      "Iteration  59700 [ 59%]: Loss = 32114.496\n",
      "Iteration  59800 [ 59%]: Loss = 37577.574\n",
      "Iteration  59900 [ 59%]: Loss = 25396.137\n",
      "Iteration  60000 [ 60%]: Loss = 24228.633\n",
      "Iteration  60100 [ 60%]: Loss = 19888.441\n",
      "Iteration  60200 [ 60%]: Loss = 26614.352\n",
      "Iteration  60300 [ 60%]: Loss = 36963.160\n",
      "Iteration  60400 [ 60%]: Loss = 43784.020\n",
      "Iteration  60500 [ 60%]: Loss = 37937.184\n",
      "Iteration  60600 [ 60%]: Loss = 24846.422\n",
      "Iteration  60700 [ 60%]: Loss = 52706.312\n",
      "Iteration  60800 [ 60%]: Loss = 42915.461\n",
      "Iteration  60900 [ 60%]: Loss = 42349.043\n",
      "Iteration  61000 [ 61%]: Loss = 31532.352\n",
      "Iteration  61100 [ 61%]: Loss = 33717.645\n",
      "Iteration  61200 [ 61%]: Loss = 23229.539\n",
      "Iteration  61300 [ 61%]: Loss = 27336.367\n",
      "Iteration  61400 [ 61%]: Loss = 33876.520\n",
      "Iteration  61500 [ 61%]: Loss = 28556.082\n",
      "Iteration  61600 [ 61%]: Loss = 33753.668\n",
      "Iteration  61700 [ 61%]: Loss = 10820.289\n",
      "Iteration  61800 [ 61%]: Loss = 16241.406\n",
      "Iteration  61900 [ 61%]: Loss = 20619.852\n",
      "Iteration  62000 [ 62%]: Loss = 26450.348\n",
      "Iteration  62100 [ 62%]: Loss = 64421.027\n",
      "Iteration  62200 [ 62%]: Loss = 25815.703\n",
      "Iteration  62300 [ 62%]: Loss = 32147.609\n",
      "Iteration  62400 [ 62%]: Loss = 33380.953\n",
      "Iteration  62500 [ 62%]: Loss = 43943.684\n",
      "Iteration  62600 [ 62%]: Loss = 39490.477\n",
      "Iteration  62700 [ 62%]: Loss = 30631.789\n",
      "Iteration  62800 [ 62%]: Loss = 31123.574\n",
      "Iteration  62900 [ 62%]: Loss = 27332.770\n",
      "Iteration  63000 [ 63%]: Loss = 28037.395\n",
      "Iteration  63100 [ 63%]: Loss = 34235.465\n",
      "Iteration  63200 [ 63%]: Loss = 42091.922\n",
      "Iteration  63300 [ 63%]: Loss = 32952.520\n",
      "Iteration  63400 [ 63%]: Loss = 31855.371\n",
      "Iteration  63500 [ 63%]: Loss = 41258.551\n",
      "Iteration  63600 [ 63%]: Loss = 42923.117\n",
      "Iteration  63700 [ 63%]: Loss = 41803.629\n",
      "Iteration  63800 [ 63%]: Loss = 14869.398\n",
      "Iteration  63900 [ 63%]: Loss = 22899.809\n",
      "Iteration  64000 [ 64%]: Loss = 43596.016\n",
      "Iteration  64100 [ 64%]: Loss = 31036.145\n",
      "Iteration  64200 [ 64%]: Loss = 33115.402\n",
      "Iteration  64300 [ 64%]: Loss = 20309.996\n",
      "Iteration  64400 [ 64%]: Loss = 38722.590\n",
      "Iteration  64500 [ 64%]: Loss = 38899.879\n",
      "Iteration  64600 [ 64%]: Loss = 31594.875\n",
      "Iteration  64700 [ 64%]: Loss = 40540.945\n",
      "Iteration  64800 [ 64%]: Loss = 43819.281\n",
      "Iteration  64900 [ 64%]: Loss = 42669.402\n",
      "Iteration  65000 [ 65%]: Loss = 25100.109\n",
      "Iteration  65100 [ 65%]: Loss = 32476.277\n",
      "Iteration  65200 [ 65%]: Loss = 27137.336\n",
      "Iteration  65300 [ 65%]: Loss = 40711.578\n",
      "Iteration  65400 [ 65%]: Loss = 32095.727\n",
      "Iteration  65500 [ 65%]: Loss = 38310.531\n",
      "Iteration  65600 [ 65%]: Loss = 29692.645\n",
      "Iteration  65700 [ 65%]: Loss = 35301.707\n",
      "Iteration  65800 [ 65%]: Loss = 35420.461\n",
      "Iteration  65900 [ 65%]: Loss = 24144.312\n",
      "Iteration  66000 [ 66%]: Loss = 23821.855\n",
      "Iteration  66100 [ 66%]: Loss = 23425.637\n",
      "Iteration  66200 [ 66%]: Loss = 35356.941\n",
      "Iteration  66300 [ 66%]: Loss = 26326.391\n",
      "Iteration  66400 [ 66%]: Loss = 36261.770\n",
      "Iteration  66500 [ 66%]: Loss = 37777.422\n",
      "Iteration  66600 [ 66%]: Loss = 23837.445\n",
      "Iteration  66700 [ 66%]: Loss = 33923.602\n",
      "Iteration  66800 [ 66%]: Loss = 47331.324\n",
      "Iteration  66900 [ 66%]: Loss = 38495.297\n",
      "Iteration  67000 [ 67%]: Loss = 23742.055\n",
      "Iteration  67100 [ 67%]: Loss = 24447.586\n",
      "Iteration  67200 [ 67%]: Loss = 17448.000\n",
      "Iteration  67300 [ 67%]: Loss = 34147.316\n",
      "Iteration  67400 [ 67%]: Loss = 21884.141\n",
      "Iteration  67500 [ 67%]: Loss = 23151.453\n",
      "Iteration  67600 [ 67%]: Loss = 42639.344\n",
      "Iteration  67700 [ 67%]: Loss = 29184.305\n",
      "Iteration  67800 [ 67%]: Loss = 40173.090\n",
      "Iteration  67900 [ 67%]: Loss = 26527.879\n",
      "Iteration  68000 [ 68%]: Loss = 27225.871\n",
      "Iteration  68100 [ 68%]: Loss = 29231.816\n",
      "Iteration  68200 [ 68%]: Loss = 34486.523\n",
      "Iteration  68300 [ 68%]: Loss = 33076.332\n",
      "Iteration  68400 [ 68%]: Loss = 32773.113\n",
      "Iteration  68500 [ 68%]: Loss = 26801.895\n",
      "Iteration  68600 [ 68%]: Loss = 29531.953\n",
      "Iteration  68700 [ 68%]: Loss = 28221.711\n",
      "Iteration  68800 [ 68%]: Loss = 24813.230\n",
      "Iteration  68900 [ 68%]: Loss = 37273.121\n",
      "Iteration  69000 [ 69%]: Loss = 17825.867\n",
      "Iteration  69100 [ 69%]: Loss = 20237.961\n",
      "Iteration  69200 [ 69%]: Loss = 36763.258\n",
      "Iteration  69300 [ 69%]: Loss = 43266.297\n",
      "Iteration  69400 [ 69%]: Loss = 34161.160\n",
      "Iteration  69500 [ 69%]: Loss = 32852.805\n",
      "Iteration  69600 [ 69%]: Loss = 33648.879\n",
      "Iteration  69700 [ 69%]: Loss = 26975.395\n",
      "Iteration  69800 [ 69%]: Loss = 34195.480\n",
      "Iteration  69900 [ 69%]: Loss = 37164.586\n",
      "Iteration  70000 [ 70%]: Loss = 20121.832\n",
      "Iteration  70100 [ 70%]: Loss = 35722.375\n",
      "Iteration  70200 [ 70%]: Loss = 37693.188\n",
      "Iteration  70300 [ 70%]: Loss = 40562.281\n",
      "Iteration  70400 [ 70%]: Loss = 37861.102\n",
      "Iteration  70500 [ 70%]: Loss = 40145.559\n",
      "Iteration  70600 [ 70%]: Loss = 32841.816\n",
      "Iteration  70700 [ 70%]: Loss = 32698.797\n",
      "Iteration  70800 [ 70%]: Loss = 36812.215\n",
      "Iteration  70900 [ 70%]: Loss = 41302.473\n",
      "Iteration  71000 [ 71%]: Loss = 24045.426\n",
      "Iteration  71100 [ 71%]: Loss = 33534.590\n",
      "Iteration  71200 [ 71%]: Loss = 33529.156\n",
      "Iteration  71300 [ 71%]: Loss = 21386.168\n",
      "Iteration  71400 [ 71%]: Loss = 32807.074\n",
      "Iteration  71500 [ 71%]: Loss = 23276.234\n",
      "Iteration  71600 [ 71%]: Loss = 37478.699\n",
      "Iteration  71700 [ 71%]: Loss = 52995.352\n",
      "Iteration  71800 [ 71%]: Loss = 28871.512\n",
      "Iteration  71900 [ 71%]: Loss = 25073.582\n",
      "Iteration  72000 [ 72%]: Loss = 21823.305\n",
      "Iteration  72100 [ 72%]: Loss = 25424.531\n",
      "Iteration  72200 [ 72%]: Loss = 30016.688\n",
      "Iteration  72300 [ 72%]: Loss = 34978.090\n",
      "Iteration  72400 [ 72%]: Loss = 40092.965\n",
      "Iteration  72500 [ 72%]: Loss = 35122.492\n",
      "Iteration  72600 [ 72%]: Loss = 21711.508\n",
      "Iteration  72700 [ 72%]: Loss = 26906.605\n",
      "Iteration  72800 [ 72%]: Loss = 38303.230\n",
      "Iteration  72900 [ 72%]: Loss = 15717.516\n",
      "Iteration  73000 [ 73%]: Loss = 44507.207\n",
      "Iteration  73100 [ 73%]: Loss = 31203.320\n",
      "Iteration  73200 [ 73%]: Loss = 43366.520\n",
      "Iteration  73300 [ 73%]: Loss = 31022.289\n",
      "Iteration  73400 [ 73%]: Loss = 24226.488\n",
      "Iteration  73500 [ 73%]: Loss = 23910.703\n",
      "Iteration  73600 [ 73%]: Loss = 47591.105\n",
      "Iteration  73700 [ 73%]: Loss = 31984.000\n",
      "Iteration  73800 [ 73%]: Loss = 25238.656\n",
      "Iteration  73900 [ 73%]: Loss = 28841.887\n",
      "Iteration  74000 [ 74%]: Loss = 48043.223\n",
      "Iteration  74100 [ 74%]: Loss = 26569.918\n",
      "Iteration  74200 [ 74%]: Loss = 20218.855\n",
      "Iteration  74300 [ 74%]: Loss = 57078.465\n",
      "Iteration  74400 [ 74%]: Loss = 29430.156\n",
      "Iteration  74500 [ 74%]: Loss = 23269.242\n",
      "Iteration  74600 [ 74%]: Loss = 12101.102\n",
      "Iteration  74700 [ 74%]: Loss = 30674.527\n",
      "Iteration  74800 [ 74%]: Loss = 46944.090\n",
      "Iteration  74900 [ 74%]: Loss = 42088.219\n",
      "Iteration  75000 [ 75%]: Loss = 8287.668\n",
      "Iteration  75100 [ 75%]: Loss = 6920.023\n",
      "Iteration  75200 [ 75%]: Loss = 31230.145\n",
      "Iteration  75300 [ 75%]: Loss = 21198.469\n",
      "Iteration  75400 [ 75%]: Loss = 44041.570\n",
      "Iteration  75500 [ 75%]: Loss = 27330.602\n",
      "Iteration  75600 [ 75%]: Loss = 34731.516\n",
      "Iteration  75700 [ 75%]: Loss = 32389.867\n",
      "Iteration  75800 [ 75%]: Loss = 28854.852\n",
      "Iteration  75900 [ 75%]: Loss = 21960.188\n",
      "Iteration  76000 [ 76%]: Loss = 31297.961\n",
      "Iteration  76100 [ 76%]: Loss = 27299.070\n",
      "Iteration  76200 [ 76%]: Loss = 33551.059\n",
      "Iteration  76300 [ 76%]: Loss = 38787.383\n",
      "Iteration  76400 [ 76%]: Loss = 25522.336\n",
      "Iteration  76500 [ 76%]: Loss = 26900.578\n",
      "Iteration  76600 [ 76%]: Loss = 42559.234\n",
      "Iteration  76700 [ 76%]: Loss = 31720.613\n",
      "Iteration  76800 [ 76%]: Loss = 37462.461\n",
      "Iteration  76900 [ 76%]: Loss = 26314.086\n",
      "Iteration  77000 [ 77%]: Loss = 19806.688\n",
      "Iteration  77100 [ 77%]: Loss = 37215.207\n",
      "Iteration  77200 [ 77%]: Loss = 36928.359\n",
      "Iteration  77300 [ 77%]: Loss = 12647.496\n",
      "Iteration  77400 [ 77%]: Loss = 27109.219\n",
      "Iteration  77500 [ 77%]: Loss = 31395.566\n",
      "Iteration  77600 [ 77%]: Loss = 31584.043\n",
      "Iteration  77700 [ 77%]: Loss = 42587.289\n",
      "Iteration  77800 [ 77%]: Loss = 21928.871\n",
      "Iteration  77900 [ 77%]: Loss = 43128.086\n",
      "Iteration  78000 [ 78%]: Loss = 30718.348\n",
      "Iteration  78100 [ 78%]: Loss = 29928.180\n",
      "Iteration  78200 [ 78%]: Loss = 38780.555\n",
      "Iteration  78300 [ 78%]: Loss = 32590.508\n",
      "Iteration  78400 [ 78%]: Loss = 35998.102\n",
      "Iteration  78500 [ 78%]: Loss = 21535.145\n",
      "Iteration  78600 [ 78%]: Loss = 35100.984\n",
      "Iteration  78700 [ 78%]: Loss = 51330.246\n",
      "Iteration  78800 [ 78%]: Loss = 14041.559\n",
      "Iteration  78900 [ 78%]: Loss = 35530.598\n",
      "Iteration  79000 [ 79%]: Loss = 14603.602\n",
      "Iteration  79100 [ 79%]: Loss = 35571.996\n",
      "Iteration  79200 [ 79%]: Loss = 41843.027\n",
      "Iteration  79300 [ 79%]: Loss = 30370.051\n",
      "Iteration  79400 [ 79%]: Loss = 30940.262\n",
      "Iteration  79500 [ 79%]: Loss = 34376.551\n",
      "Iteration  79600 [ 79%]: Loss = 36581.250\n",
      "Iteration  79700 [ 79%]: Loss = 21873.168\n",
      "Iteration  79800 [ 79%]: Loss = 39918.469\n",
      "Iteration  79900 [ 79%]: Loss = 17527.254\n",
      "Iteration  80000 [ 80%]: Loss = 21271.734\n",
      "Iteration  80100 [ 80%]: Loss = 36636.500\n",
      "Iteration  80200 [ 80%]: Loss = 21704.230\n",
      "Iteration  80300 [ 80%]: Loss = 28927.762\n",
      "Iteration  80400 [ 80%]: Loss = 20157.176\n",
      "Iteration  80500 [ 80%]: Loss = 19236.676\n",
      "Iteration  80600 [ 80%]: Loss = 39591.746\n",
      "Iteration  80700 [ 80%]: Loss = 35900.016\n",
      "Iteration  80800 [ 80%]: Loss = 25874.035\n",
      "Iteration  80900 [ 80%]: Loss = 35970.871\n",
      "Iteration  81000 [ 81%]: Loss = 33153.578\n",
      "Iteration  81100 [ 81%]: Loss = 41877.395\n",
      "Iteration  81200 [ 81%]: Loss = 46715.379\n",
      "Iteration  81300 [ 81%]: Loss = 30989.527\n",
      "Iteration  81400 [ 81%]: Loss = 30957.332\n",
      "Iteration  81500 [ 81%]: Loss = 25739.000\n",
      "Iteration  81600 [ 81%]: Loss = 57466.078\n",
      "Iteration  81700 [ 81%]: Loss = 25671.246\n",
      "Iteration  81800 [ 81%]: Loss = 30687.453\n",
      "Iteration  81900 [ 81%]: Loss = 29780.762\n",
      "Iteration  82000 [ 82%]: Loss = 29377.531\n",
      "Iteration  82100 [ 82%]: Loss = 24976.703\n",
      "Iteration  82200 [ 82%]: Loss = 13457.625\n",
      "Iteration  82300 [ 82%]: Loss = 12653.438\n",
      "Iteration  82400 [ 82%]: Loss = 21879.156\n",
      "Iteration  82500 [ 82%]: Loss = 28925.617\n",
      "Iteration  82600 [ 82%]: Loss = 17930.367\n",
      "Iteration  82700 [ 82%]: Loss = 60705.789\n",
      "Iteration  82800 [ 82%]: Loss = 39376.988\n",
      "Iteration  82900 [ 82%]: Loss = 30534.371\n",
      "Iteration  83000 [ 83%]: Loss = 33305.340\n",
      "Iteration  83100 [ 83%]: Loss = 26804.184\n",
      "Iteration  83200 [ 83%]: Loss = 30625.598\n",
      "Iteration  83300 [ 83%]: Loss = 35416.367\n",
      "Iteration  83400 [ 83%]: Loss = 17373.562\n",
      "Iteration  83500 [ 83%]: Loss = 40062.113\n",
      "Iteration  83600 [ 83%]: Loss = 29755.828\n",
      "Iteration  83700 [ 83%]: Loss = 25808.773\n",
      "Iteration  83800 [ 83%]: Loss = 35134.320\n",
      "Iteration  83900 [ 83%]: Loss = 32945.785\n",
      "Iteration  84000 [ 84%]: Loss = 24720.742\n",
      "Iteration  84100 [ 84%]: Loss = 27027.504\n",
      "Iteration  84200 [ 84%]: Loss = 37866.789\n",
      "Iteration  84300 [ 84%]: Loss = 29182.793\n",
      "Iteration  84400 [ 84%]: Loss = 24944.074\n",
      "Iteration  84500 [ 84%]: Loss = 35015.992\n",
      "Iteration  84600 [ 84%]: Loss = 37436.125\n",
      "Iteration  84700 [ 84%]: Loss = 25211.387\n",
      "Iteration  84800 [ 84%]: Loss = 28213.156\n",
      "Iteration  84900 [ 84%]: Loss = 40791.281\n",
      "Iteration  85000 [ 85%]: Loss = 44150.180\n",
      "Iteration  85100 [ 85%]: Loss = 22330.879\n",
      "Iteration  85200 [ 85%]: Loss = 37781.176\n",
      "Iteration  85300 [ 85%]: Loss = 32452.746\n",
      "Iteration  85400 [ 85%]: Loss = 45329.133\n",
      "Iteration  85500 [ 85%]: Loss = 31130.449\n",
      "Iteration  85600 [ 85%]: Loss = 27083.598\n",
      "Iteration  85700 [ 85%]: Loss = 49653.477\n",
      "Iteration  85800 [ 85%]: Loss = 25640.055\n",
      "Iteration  85900 [ 85%]: Loss = 25332.961\n",
      "Iteration  86000 [ 86%]: Loss = 25833.668\n",
      "Iteration  86100 [ 86%]: Loss = 33642.719\n",
      "Iteration  86200 [ 86%]: Loss = 10653.121\n",
      "Iteration  86300 [ 86%]: Loss = 26858.934\n",
      "Iteration  86400 [ 86%]: Loss = 43921.137\n",
      "Iteration  86500 [ 86%]: Loss = 29035.766\n",
      "Iteration  86600 [ 86%]: Loss = 31020.633\n",
      "Iteration  86700 [ 86%]: Loss = 35667.102\n",
      "Iteration  86800 [ 86%]: Loss = 28970.562\n",
      "Iteration  86900 [ 86%]: Loss = 29603.203\n",
      "Iteration  87000 [ 87%]: Loss = 13708.238\n",
      "Iteration  87100 [ 87%]: Loss = 16761.566\n",
      "Iteration  87200 [ 87%]: Loss = 26711.336\n",
      "Iteration  87300 [ 87%]: Loss = 19979.367\n",
      "Iteration  87400 [ 87%]: Loss = 19056.223\n",
      "Iteration  87500 [ 87%]: Loss = 30674.008\n",
      "Iteration  87600 [ 87%]: Loss = 35642.895\n",
      "Iteration  87700 [ 87%]: Loss = 21945.469\n",
      "Iteration  87800 [ 87%]: Loss = 30015.309\n",
      "Iteration  87900 [ 87%]: Loss = 25866.242\n",
      "Iteration  88000 [ 88%]: Loss = 42408.824\n",
      "Iteration  88100 [ 88%]: Loss = 60836.617\n",
      "Iteration  88200 [ 88%]: Loss = 41537.406\n",
      "Iteration  88300 [ 88%]: Loss = 30999.320\n",
      "Iteration  88400 [ 88%]: Loss = 52895.770\n",
      "Iteration  88500 [ 88%]: Loss = 28818.516\n",
      "Iteration  88600 [ 88%]: Loss = 22230.844\n",
      "Iteration  88700 [ 88%]: Loss = 17575.137\n",
      "Iteration  88800 [ 88%]: Loss = 32602.281\n",
      "Iteration  88900 [ 88%]: Loss = 27059.758\n",
      "Iteration  89000 [ 89%]: Loss = 29566.535\n",
      "Iteration  89100 [ 89%]: Loss = 37421.121\n",
      "Iteration  89200 [ 89%]: Loss = 28549.379\n",
      "Iteration  89300 [ 89%]: Loss = 33980.020\n",
      "Iteration  89400 [ 89%]: Loss = 16065.914\n",
      "Iteration  89500 [ 89%]: Loss = 24693.824\n",
      "Iteration  89600 [ 89%]: Loss = 24319.484\n",
      "Iteration  89700 [ 89%]: Loss = 31726.949\n",
      "Iteration  89800 [ 89%]: Loss = 14496.414\n",
      "Iteration  89900 [ 89%]: Loss = 18802.742\n",
      "Iteration  90000 [ 90%]: Loss = 25957.938\n",
      "Iteration  90100 [ 90%]: Loss = 22796.758\n",
      "Iteration  90200 [ 90%]: Loss = 19487.605\n",
      "Iteration  90300 [ 90%]: Loss = 17908.820\n",
      "Iteration  90400 [ 90%]: Loss = 23484.926\n",
      "Iteration  90500 [ 90%]: Loss = 24169.332\n",
      "Iteration  90600 [ 90%]: Loss = 22714.543\n",
      "Iteration  90700 [ 90%]: Loss = 26943.434\n",
      "Iteration  90800 [ 90%]: Loss = 22249.668\n",
      "Iteration  90900 [ 90%]: Loss = 10943.070\n",
      "Iteration  91000 [ 91%]: Loss = 14036.027\n",
      "Iteration  91100 [ 91%]: Loss = 33191.664\n",
      "Iteration  91200 [ 91%]: Loss = 20051.102\n",
      "Iteration  91300 [ 91%]: Loss = 36603.430\n",
      "Iteration  91400 [ 91%]: Loss = 12305.031\n",
      "Iteration  91500 [ 91%]: Loss = 24674.207\n",
      "Iteration  91600 [ 91%]: Loss = 19377.367\n",
      "Iteration  91700 [ 91%]: Loss = 35843.145\n",
      "Iteration  91800 [ 91%]: Loss = 21411.125\n",
      "Iteration  91900 [ 91%]: Loss = 31901.402\n",
      "Iteration  92000 [ 92%]: Loss = 23786.645\n",
      "Iteration  92100 [ 92%]: Loss = 27042.285\n",
      "Iteration  92200 [ 92%]: Loss = 36422.293\n",
      "Iteration  92300 [ 92%]: Loss = 29934.363\n",
      "Iteration  92400 [ 92%]: Loss = 25103.285\n",
      "Iteration  92500 [ 92%]: Loss = 36607.570\n",
      "Iteration  92600 [ 92%]: Loss = 18681.672\n",
      "Iteration  92700 [ 92%]: Loss = 33593.711\n",
      "Iteration  92800 [ 92%]: Loss = 29950.273\n",
      "Iteration  92900 [ 92%]: Loss = 43570.168\n",
      "Iteration  93000 [ 93%]: Loss = 41509.125\n",
      "Iteration  93100 [ 93%]: Loss = 35476.500\n",
      "Iteration  93200 [ 93%]: Loss = 27411.508\n",
      "Iteration  93300 [ 93%]: Loss = 20418.523\n",
      "Iteration  93400 [ 93%]: Loss = 32240.125\n",
      "Iteration  93500 [ 93%]: Loss = 32647.816\n",
      "Iteration  93600 [ 93%]: Loss = 38136.879\n",
      "Iteration  93700 [ 93%]: Loss = 32402.355\n",
      "Iteration  93800 [ 93%]: Loss = 29516.520\n",
      "Iteration  93900 [ 93%]: Loss = 24852.215\n",
      "Iteration  94000 [ 94%]: Loss = 18138.562\n",
      "Iteration  94100 [ 94%]: Loss = 29413.582\n",
      "Iteration  94200 [ 94%]: Loss = 33458.746\n",
      "Iteration  94300 [ 94%]: Loss = 25639.688\n",
      "Iteration  94400 [ 94%]: Loss = 28590.391\n",
      "Iteration  94500 [ 94%]: Loss = 36055.027\n",
      "Iteration  94600 [ 94%]: Loss = 30688.164\n",
      "Iteration  94700 [ 94%]: Loss = 22529.074\n",
      "Iteration  94800 [ 94%]: Loss = 22669.141\n",
      "Iteration  94900 [ 94%]: Loss = 27430.156\n",
      "Iteration  95000 [ 95%]: Loss = 20924.391\n",
      "Iteration  95100 [ 95%]: Loss = 29807.562\n",
      "Iteration  95200 [ 95%]: Loss = 13153.984\n",
      "Iteration  95300 [ 95%]: Loss = 27510.582\n",
      "Iteration  95400 [ 95%]: Loss = 21765.730\n",
      "Iteration  95500 [ 95%]: Loss = 38154.363\n",
      "Iteration  95600 [ 95%]: Loss = 34292.254\n",
      "Iteration  95700 [ 95%]: Loss = 28382.086\n",
      "Iteration  95800 [ 95%]: Loss = 22816.906\n",
      "Iteration  95900 [ 95%]: Loss = 28692.672\n",
      "Iteration  96000 [ 96%]: Loss = 29664.254\n",
      "Iteration  96100 [ 96%]: Loss = 24031.375\n",
      "Iteration  96200 [ 96%]: Loss = 33587.797\n",
      "Iteration  96300 [ 96%]: Loss = 39192.375\n",
      "Iteration  96400 [ 96%]: Loss = 33141.234\n",
      "Iteration  96500 [ 96%]: Loss = 42989.605\n",
      "Iteration  96600 [ 96%]: Loss = 31990.473\n",
      "Iteration  96700 [ 96%]: Loss = 28884.332\n",
      "Iteration  96800 [ 96%]: Loss = 34713.562\n",
      "Iteration  96900 [ 96%]: Loss = 28725.035\n",
      "Iteration  97000 [ 97%]: Loss = 22944.516\n",
      "Iteration  97100 [ 97%]: Loss = 47473.730\n",
      "Iteration  97200 [ 97%]: Loss = 37598.301\n",
      "Iteration  97300 [ 97%]: Loss = 19190.266\n",
      "Iteration  97400 [ 97%]: Loss = 30307.594\n",
      "Iteration  97500 [ 97%]: Loss = 27976.258\n",
      "Iteration  97600 [ 97%]: Loss = 19586.133\n",
      "Iteration  97700 [ 97%]: Loss = 13000.859\n",
      "Iteration  97800 [ 97%]: Loss = 30382.707\n",
      "Iteration  97900 [ 97%]: Loss = 24620.871\n",
      "Iteration  98000 [ 98%]: Loss = 16612.879\n",
      "Iteration  98100 [ 98%]: Loss = 35576.832\n",
      "Iteration  98200 [ 98%]: Loss = 27267.539\n",
      "Iteration  98300 [ 98%]: Loss = 32683.598\n",
      "Iteration  98400 [ 98%]: Loss = 21013.383\n",
      "Iteration  98500 [ 98%]: Loss = 22991.930\n",
      "Iteration  98600 [ 98%]: Loss = 35356.559\n",
      "Iteration  98700 [ 98%]: Loss = 34338.340\n",
      "Iteration  98800 [ 98%]: Loss = 11732.656\n",
      "Iteration  98900 [ 98%]: Loss = 34613.988\n",
      "Iteration  99000 [ 99%]: Loss = 26106.648\n",
      "Iteration  99100 [ 99%]: Loss = 25633.160\n",
      "Iteration  99200 [ 99%]: Loss = 32018.828\n",
      "Iteration  99300 [ 99%]: Loss = 30744.367\n",
      "Iteration  99400 [ 99%]: Loss = 27795.078\n",
      "Iteration  99500 [ 99%]: Loss = 23136.637\n",
      "Iteration  99600 [ 99%]: Loss = 39905.559\n",
      "Iteration  99700 [ 99%]: Loss = 36560.074\n",
      "Iteration  99800 [ 99%]: Loss = 26258.395\n",
      "Iteration  99900 [ 99%]: Loss = 32436.895\n",
      "Iteration 100000 [100%]: Loss = 33637.492\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=100000, n_print=100, n_samples=1,\n",
    "              logdir='data/run1',\n",
    "              optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "              scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'qmodel/A/sample/Reshape:0' shape=(49,) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_A.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Zmat = q_Z.value().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x122eb9278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.matshow(Zmat, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
