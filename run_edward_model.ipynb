{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687276, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim</th>\n",
       "      <th>unit</th>\n",
       "      <th>isfirst</th>\n",
       "      <th>isrewarded</th>\n",
       "      <th>count</th>\n",
       "      <th>trial</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim  unit  isfirst  isrewarded  count  trial  time\n",
       "0     0  1068        0           0      4  57277     0\n",
       "1     0  1069        0           0      1  57277     0\n",
       "2     0  1070        0           0      6  57277     0\n",
       "3     0  1071        0           0      0  57277     0\n",
       "4     0  1072        1           0      3  60250     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('data/prepared_data.csv')\n",
    "print(dat.shape)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are:\n",
    "- stim: original stimulus number\n",
    "- unit: recorded neuron number\n",
    "- isfirst, isrewarded: potential regressors of interest\n",
    "- count: spike count during stimulus display period\n",
    "- trial: original trial number\n",
    "- time: unique stimulus code in trimmed dataset (very rare stims removed)\n",
    "\n",
    "We need to do a couple of things to prep the data:\n",
    "- turn unit codes into a 1-based unit index\n",
    "- turn time into a 1-based stim index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27038, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset data for testing\n",
    "dat = dat.query(\"unit < 1050 & time < 300\")\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 48 49\n",
      "0 283 284\n"
     ]
    }
   ],
   "source": [
    "_, unit = np.unique(dat.unit, return_inverse=True)\n",
    "_, stim = np.unique(dat.time, return_inverse=True)\n",
    "\n",
    "print(np.min(unit), np.max(unit), len(np.unique(unit)))\n",
    "\n",
    "print(np.min(stim), np.max(stim), len(np.unique(stim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27038, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = dat['count'].values\n",
    "Xdat = dat[['isfirst', 'isrewarded']].values\n",
    "Xdat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = dat.shape[0]  # number of trials\n",
    "NU = len(np.unique(unit))  # number of units\n",
    "NS = len(np.unique(stim))  # number of stims\n",
    "P = Xdat.shape[1]  # number of specified regressors\n",
    "K = 5  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=tf.ones((NU, P)), name='B')\n",
    "    C = ed.models.Normal(mu=tf.zeros((NU, K)), sigma=tf.ones((NU, K)), name='C')\n",
    "\n",
    "    X = ed.placeholder(tf.float32, (N, P), name='X')\n",
    "\n",
    "    delta = ed.models.Beta(a=3 * tf.ones(K), b=tf.ones(K), name='delta')\n",
    "    tf.scalar_summary('mean_delta', tf.reduce_mean(delta))\n",
    "    log_delta = tf.log(delta)\n",
    "    tf.scalar_summary('min_log_delta', tf.reduce_min(log_delta))\n",
    "    tf.scalar_summary('mean_log_delta', tf.reduce_mean(log_delta))\n",
    "\n",
    "    pi = tf.exp(tf.cumsum(log_delta), name='pi')\n",
    "    tf.scalar_summary('min_pi', tf.reduce_min(pi))\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "    tf.scalar_summary('mean_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-0.1], sigma=[0.1], name='sig')\n",
    "\n",
    "    lam = ed.models.Normal(mu=(tf.gather(A, unit) + tf.reduce_sum(tf.gather(B, unit) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, unit) * tf.gather(tf.to_float(Z), stim), 1)), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "    tf.scalar_summary('mean_lam', tf.reduce_mean(lam))\n",
    "\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.nn.softplus(lam), value=tf.ones(N), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(tf.random_normal((NS, K))), name='Z')\n",
    "    tf.scalar_summary('mean_q_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    q_delta = ed.models.BetaWithSoftplusAB(a=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           b=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           name='delta')\n",
    "    tf.scalar_summary('mean_q_delta', tf.reduce_mean(q_delta))\n",
    "\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((N,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((N,))),\n",
    "                                              name='lam')\n",
    "    tf.scalar_summary('mean_q_lam', tf.reduce_mean(q_lam))\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')\n",
    "    tf.scalar_summary('mean_q_sig', tf.reduce_mean(q_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: count, X: Xdat}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1 [  0%]: Loss = 415586.750\n",
      "Iteration   100 [  1%]: Loss = 664088.250\n",
      "Iteration   200 [  2%]: Loss = 609563.062\n",
      "Iteration   300 [  3%]: Loss = 396827.719\n",
      "Iteration   400 [  4%]: Loss = 304928.750\n",
      "Iteration   500 [  5%]: Loss = 393331.438\n",
      "Iteration   600 [  6%]: Loss = 293944.188\n",
      "Iteration   700 [  7%]: Loss = 632169.188\n",
      "Iteration   800 [  8%]: Loss = 334810.625\n",
      "Iteration   900 [  9%]: Loss = 379632.156\n",
      "Iteration  1000 [ 10%]: Loss = 491948.125\n",
      "Iteration  1100 [ 11%]: Loss = 377430.438\n",
      "Iteration  1200 [ 12%]: Loss = 626805.875\n",
      "Iteration  1300 [ 13%]: Loss = 454571.719\n",
      "Iteration  1400 [ 14%]: Loss = 345653.094\n",
      "Iteration  1500 [ 15%]: Loss = 248194.922\n",
      "Iteration  1600 [ 16%]: Loss = 301853.438\n",
      "Iteration  1700 [ 17%]: Loss = 272089.375\n",
      "Iteration  1800 [ 18%]: Loss = 375434.562\n",
      "Iteration  1900 [ 19%]: Loss = 318505.281\n",
      "Iteration  2000 [ 20%]: Loss = 433494.469\n",
      "Iteration  2100 [ 21%]: Loss = 296638.406\n",
      "Iteration  2200 [ 22%]: Loss = 335983.094\n",
      "Iteration  2300 [ 23%]: Loss = 198831.562\n",
      "Iteration  2400 [ 24%]: Loss = 275133.719\n",
      "Iteration  2500 [ 25%]: Loss = 557572.875\n",
      "Iteration  2600 [ 26%]: Loss = 581637.250\n",
      "Iteration  2700 [ 27%]: Loss = 345544.688\n",
      "Iteration  2800 [ 28%]: Loss = 386473.875\n",
      "Iteration  2900 [ 28%]: Loss = 204368.344\n",
      "Iteration  3000 [ 30%]: Loss = 225859.016\n",
      "Iteration  3100 [ 31%]: Loss = 230688.562\n",
      "Iteration  3200 [ 32%]: Loss = 485453.000\n",
      "Iteration  3300 [ 33%]: Loss = 295497.625\n",
      "Iteration  3400 [ 34%]: Loss = 379858.844\n",
      "Iteration  3500 [ 35%]: Loss = 224266.906\n",
      "Iteration  3600 [ 36%]: Loss = 579242.062\n",
      "Iteration  3700 [ 37%]: Loss = 344784.031\n",
      "Iteration  3800 [ 38%]: Loss = 211477.422\n",
      "Iteration  3900 [ 39%]: Loss = 306615.938\n",
      "Iteration  4000 [ 40%]: Loss = 510827.875\n",
      "Iteration  4100 [ 41%]: Loss = 242693.625\n",
      "Iteration  4200 [ 42%]: Loss = 443362.812\n",
      "Iteration  4300 [ 43%]: Loss = 216746.625\n",
      "Iteration  4400 [ 44%]: Loss = 245334.922\n",
      "Iteration  4500 [ 45%]: Loss = 264990.031\n",
      "Iteration  4600 [ 46%]: Loss = 298552.719\n",
      "Iteration  4700 [ 47%]: Loss = 245787.641\n",
      "Iteration  4800 [ 48%]: Loss = 262766.531\n",
      "Iteration  4900 [ 49%]: Loss = 238957.594\n",
      "Iteration  5000 [ 50%]: Loss = 204759.375\n",
      "Iteration  5100 [ 51%]: Loss = 250540.281\n",
      "Iteration  5200 [ 52%]: Loss = 173823.188\n",
      "Iteration  5300 [ 53%]: Loss = 221015.938\n",
      "Iteration  5400 [ 54%]: Loss = 248210.938\n",
      "Iteration  5500 [ 55%]: Loss = 167252.844\n",
      "Iteration  5600 [ 56%]: Loss = 255133.375\n",
      "Iteration  5700 [ 56%]: Loss = 208412.641\n",
      "Iteration  5800 [ 57%]: Loss = 266377.781\n",
      "Iteration  5900 [ 59%]: Loss = 241565.719\n",
      "Iteration  6000 [ 60%]: Loss = 205464.156\n",
      "Iteration  6100 [ 61%]: Loss = 298332.094\n",
      "Iteration  6200 [ 62%]: Loss = 582133.125\n",
      "Iteration  6300 [ 63%]: Loss = 350554.594\n",
      "Iteration  6400 [ 64%]: Loss = 194068.562\n",
      "Iteration  6500 [ 65%]: Loss = 216169.016\n",
      "Iteration  6600 [ 66%]: Loss = 181245.000\n",
      "Iteration  6700 [ 67%]: Loss = 176397.875\n",
      "Iteration  6800 [ 68%]: Loss = 199577.844\n",
      "Iteration  6900 [ 69%]: Loss = 210890.844\n",
      "Iteration  7000 [ 70%]: Loss = 223362.234\n",
      "Iteration  7100 [ 71%]: Loss = 177667.062\n",
      "Iteration  7200 [ 72%]: Loss = 197355.594\n",
      "Iteration  7300 [ 73%]: Loss = 168734.219\n",
      "Iteration  7400 [ 74%]: Loss = 215458.859\n",
      "Iteration  7500 [ 75%]: Loss = 235883.016\n",
      "Iteration  7600 [ 76%]: Loss = 168185.047\n",
      "Iteration  7700 [ 77%]: Loss = 276212.406\n",
      "Iteration  7800 [ 78%]: Loss = 196023.719\n",
      "Iteration  7900 [ 79%]: Loss = 198084.000\n",
      "Iteration  8000 [ 80%]: Loss = 207897.719\n",
      "Iteration  8100 [ 81%]: Loss = 239038.859\n",
      "Iteration  8200 [ 82%]: Loss = 221899.625\n",
      "Iteration  8300 [ 83%]: Loss = 185972.125\n",
      "Iteration  8400 [ 84%]: Loss = 263246.875\n",
      "Iteration  8500 [ 85%]: Loss = 166799.281\n",
      "Iteration  8600 [ 86%]: Loss = 194676.703\n",
      "Iteration  8700 [ 87%]: Loss = 229540.062\n",
      "Iteration  8800 [ 88%]: Loss = 279975.438\n",
      "Iteration  8900 [ 89%]: Loss = 173289.359\n",
      "Iteration  9000 [ 90%]: Loss = 192057.734\n",
      "Iteration  9100 [ 91%]: Loss = 188334.219\n",
      "Iteration  9200 [ 92%]: Loss = 190061.703\n",
      "Iteration  9300 [ 93%]: Loss = 150718.078\n",
      "Iteration  9400 [ 94%]: Loss = 175400.344\n",
      "Iteration  9500 [ 95%]: Loss = 238098.625\n",
      "Iteration  9600 [ 96%]: Loss = 200400.484\n",
      "Iteration  9700 [ 97%]: Loss = 174610.719\n",
      "Iteration  9800 [ 98%]: Loss = 174372.766\n",
      "Iteration  9900 [ 99%]: Loss = 157783.766\n",
      "Iteration 10000 [100%]: Loss = 198947.906\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=10000, n_print=100, n_samples=25, logdir='data/run1',\n",
    "             optimizer=tf.train.AdamOptimizer(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
