{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}((\\lambda)_+)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B * X)_{us} + (C * Z)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-0.1, 0.1^2) \\\\\n",
    "Z_{ks} &\\sim \\mathrm{Bernoulli}(\\pi_k) \\\\\n",
    "\\pi_k &\\equiv \\prod_{i=1}^k \\delta_k \\\\\n",
    "\\delta_j &\\sim \\mathrm{Beta}(3, 1)\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "N = 5  # number of observations per unit per stim\n",
    "NB = 10  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = 5 + 2 * np.random.randn(NU)  # baseline\n",
    "dB = np.array([0, 5, 10]) + 0.25 * np.random.randn(NU, P)  # regressor effects\n",
    "dC = np.array([-10, -5, 5, 10])[np.newaxis, :] + 0.25 * np.random.randn(NU, K)  # latent effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1284cd6d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12272ca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dX = 1 + np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "ddelta = stats.beta.rvs(1.2, 1, size=(K,))\n",
    "dpi = np.cumprod(ddelta)\n",
    "dZ = stats.bernoulli.rvs(dpi[:, np.newaxis], size=(K, NS))\n",
    "\n",
    "# plot states\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1) + np.sum(dC[dU] * dZ[:, dS].T, axis=1)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOKGbmVWEE7qZWUU4oZuZVcT/A13Qe7a6GVhoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1227e2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dlam, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcount = stats.poisson.rvs(softplus(dlam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncQa9JDXOoJekxhn0ktS4/wO+YYb4hFfjxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128b24b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount\n",
    "Xdat = dX[:, dS].T\n",
    "Xdat.shape\n",
    "unit = dU\n",
    "stim = dS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.train.range_input_producer(N).dequeue_many(NB, name='batch_inds')\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB, name='batch_counts')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=tf.ones((NU, P)), name='B')\n",
    "    C = ed.models.Normal(mu=tf.zeros((NU, K)), sigma=tf.ones((NU, K)), name='C')  \n",
    "    \n",
    "    delta = ed.models.Beta(a=3 * tf.ones(K), b=tf.ones(K), name='delta')\n",
    "    tf.scalar_summary('mean_delta', tf.reduce_mean(delta))\n",
    "    log_delta = tf.log(delta)\n",
    "    tf.scalar_summary('min_log_delta', tf.reduce_min(log_delta))\n",
    "    tf.scalar_summary('mean_log_delta', tf.reduce_mean(log_delta))\n",
    "\n",
    "    pi = tf.exp(tf.cumsum(log_delta), name='pi')\n",
    "    tf.scalar_summary('min_pi', tf.reduce_min(pi))\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "    tf.scalar_summary('mean_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-0.1], sigma=[0.1], name='sig')\n",
    "\n",
    "    lam_vars = (tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, U) * tf.gather(tf.to_float(Z), S), 1))\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "    tf.scalar_summary('mean_lam', tf.reduce_mean(lam))\n",
    "\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.nn.softplus(lam), value=tf.ones(NB), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(tf.random_normal((NS, K))), name='Z')\n",
    "    tf.scalar_summary('mean_q_Z', tf.reduce_mean(tf.to_float(Z)))\n",
    "\n",
    "    q_delta = ed.models.BetaWithSoftplusAB(a=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           b=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           name='delta')\n",
    "    tf.scalar_summary('mean_q_delta', tf.reduce_mean(q_delta))\n",
    "\n",
    "    lam_mu = tf.Variable(tf.random_normal((N,)))\n",
    "    lam_sig = tf.Variable(tf.random_uniform((N,)))\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "    tf.scalar_summary('mean_q_lam', tf.reduce_mean(q_lam))\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')\n",
    "    tf.scalar_summary('mean_q_sig', tf.reduce_mean(q_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, \n",
    "                    data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1 [  0%]: Loss = 18631.346\n",
      "Iteration   100 [  0%]: Loss = 150930.438\n",
      "Iteration   200 [  0%]: Loss = 3586.414\n",
      "Iteration   300 [  1%]: Loss = 5054.804\n",
      "Iteration   400 [  1%]: Loss = 3198.501\n",
      "Iteration   500 [  1%]: Loss = 17516.754\n",
      "Iteration   600 [  2%]: Loss = 44975.336\n",
      "Iteration   700 [  2%]: Loss = 4737.905\n",
      "Iteration   800 [  2%]: Loss = 21274.695\n",
      "Iteration   900 [  3%]: Loss = 15562.155\n",
      "Iteration  1000 [  3%]: Loss = 3287.508\n",
      "Iteration  1100 [  3%]: Loss = 127264.461\n",
      "Iteration  1200 [  4%]: Loss = 49329.840\n",
      "Iteration  1300 [  4%]: Loss = 6449.157\n",
      "Iteration  1400 [  4%]: Loss = 5887.796\n",
      "Iteration  1500 [  5%]: Loss = 95612.703\n",
      "Iteration  1600 [  5%]: Loss = 2264.280\n",
      "Iteration  1700 [  5%]: Loss = 4359.103\n",
      "Iteration  1800 [  6%]: Loss = 6036.480\n",
      "Iteration  1900 [  6%]: Loss = 3713.784\n",
      "Iteration  2000 [  6%]: Loss = 48110.477\n",
      "Iteration  2100 [  7%]: Loss = 7381.340\n",
      "Iteration  2200 [  7%]: Loss = 51745.797\n",
      "Iteration  2300 [  7%]: Loss = 23350.055\n",
      "Iteration  2400 [  8%]: Loss = 18745.912\n",
      "Iteration  2500 [  8%]: Loss = 89347.922\n",
      "Iteration  2600 [  8%]: Loss = 38506.906\n",
      "Iteration  2700 [  9%]: Loss = 3163.115\n",
      "Iteration  2800 [  9%]: Loss = 93802.172\n",
      "Iteration  2900 [  9%]: Loss = 41723.926\n",
      "Iteration  3000 [ 10%]: Loss = 218214.688\n",
      "Iteration  3100 [ 10%]: Loss = 10673.730\n",
      "Iteration  3200 [ 10%]: Loss = 12741.367\n",
      "Iteration  3300 [ 11%]: Loss = 9296.285\n",
      "Iteration  3400 [ 11%]: Loss = 3775.452\n",
      "Iteration  3500 [ 11%]: Loss = 109195.195\n",
      "Iteration  3600 [ 12%]: Loss = 88214.836\n",
      "Iteration  3700 [ 12%]: Loss = 19287.152\n",
      "Iteration  3800 [ 12%]: Loss = 41465.258\n",
      "Iteration  3900 [ 13%]: Loss = 8208.194\n",
      "Iteration  4000 [ 13%]: Loss = 5101.089\n",
      "Iteration  4100 [ 13%]: Loss = 10981.713\n",
      "Iteration  4200 [ 14%]: Loss = 5476.361\n",
      "Iteration  4300 [ 14%]: Loss = 7842.043\n",
      "Iteration  4400 [ 14%]: Loss = 8342.339\n",
      "Iteration  4500 [ 15%]: Loss = 74300.734\n",
      "Iteration  4600 [ 15%]: Loss = 4050.910\n",
      "Iteration  4700 [ 15%]: Loss = 11045.614\n",
      "Iteration  4800 [ 16%]: Loss = 3760.068\n",
      "Iteration  4900 [ 16%]: Loss = 19563.863\n",
      "Iteration  5000 [ 16%]: Loss = 11755.453\n",
      "Iteration  5100 [ 17%]: Loss = 12106.130\n",
      "Iteration  5200 [ 17%]: Loss = 9476.486\n",
      "Iteration  5300 [ 17%]: Loss = 12495.516\n",
      "Iteration  5400 [ 18%]: Loss = 4341.949\n",
      "Iteration  5500 [ 18%]: Loss = 6389.604\n",
      "Iteration  5600 [ 18%]: Loss = 3791.598\n",
      "Iteration  5700 [ 19%]: Loss = 4491.209\n",
      "Iteration  5800 [ 19%]: Loss = 24995.441\n",
      "Iteration  5900 [ 19%]: Loss = 13735.432\n",
      "Iteration  6000 [ 20%]: Loss = 6545.240\n",
      "Iteration  6100 [ 20%]: Loss = 4948.156\n",
      "Iteration  6200 [ 20%]: Loss = 19999.969\n",
      "Iteration  6300 [ 21%]: Loss = 15794.459\n",
      "Iteration  6400 [ 21%]: Loss = 2443.759\n",
      "Iteration  6500 [ 21%]: Loss = 3920.166\n",
      "Iteration  6600 [ 22%]: Loss = 4885.719\n",
      "Iteration  6700 [ 22%]: Loss = 10258.736\n",
      "Iteration  6800 [ 22%]: Loss = 8304.215\n",
      "Iteration  6900 [ 23%]: Loss = 3016.185\n",
      "Iteration  7000 [ 23%]: Loss = 3332.409\n",
      "Iteration  7100 [ 23%]: Loss = 56364.242\n",
      "Iteration  7200 [ 24%]: Loss = 5468.909\n",
      "Iteration  7300 [ 24%]: Loss = 64812.602\n",
      "Iteration  7400 [ 24%]: Loss = 4493.109\n",
      "Iteration  7500 [ 25%]: Loss = 5649.001\n",
      "Iteration  7600 [ 25%]: Loss = 19214.855\n",
      "Iteration  7700 [ 25%]: Loss = 3027.172\n",
      "Iteration  7800 [ 26%]: Loss = 7833.854\n",
      "Iteration  7900 [ 26%]: Loss = 80765.148\n",
      "Iteration  8000 [ 26%]: Loss = 2960.279\n",
      "Iteration  8100 [ 27%]: Loss = 3624.000\n",
      "Iteration  8200 [ 27%]: Loss = 5989.708\n",
      "Iteration  8300 [ 27%]: Loss = 7825.483\n",
      "Iteration  8400 [ 28%]: Loss = 4563.633\n",
      "Iteration  8500 [ 28%]: Loss = 5324.263\n",
      "Iteration  8600 [ 28%]: Loss = 5521.114\n",
      "Iteration  8700 [ 28%]: Loss = 6608.435\n",
      "Iteration  8800 [ 29%]: Loss = 1968.467\n",
      "Iteration  8900 [ 29%]: Loss = 4572.443\n",
      "Iteration  9000 [ 30%]: Loss = 10165.443\n",
      "Iteration  9100 [ 30%]: Loss = 7091.217\n",
      "Iteration  9200 [ 30%]: Loss = 8428.236\n",
      "Iteration  9300 [ 31%]: Loss = 6156.204\n",
      "Iteration  9400 [ 31%]: Loss = 4057.057\n",
      "Iteration  9500 [ 31%]: Loss = 3136.630\n",
      "Iteration  9600 [ 32%]: Loss = 5433.840\n",
      "Iteration  9700 [ 32%]: Loss = 6582.553\n",
      "Iteration  9800 [ 32%]: Loss = 4390.635\n",
      "Iteration  9900 [ 33%]: Loss = 4916.431\n",
      "Iteration 10000 [ 33%]: Loss = 3482.590\n",
      "Iteration 10100 [ 33%]: Loss = 4598.748\n",
      "Iteration 10200 [ 34%]: Loss = 18183.195\n",
      "Iteration 10300 [ 34%]: Loss = 16473.750\n",
      "Iteration 10400 [ 34%]: Loss = 3449.678\n",
      "Iteration 10500 [ 35%]: Loss = 4697.371\n",
      "Iteration 10600 [ 35%]: Loss = 3867.853\n",
      "Iteration 10700 [ 35%]: Loss = 2665.766\n",
      "Iteration 10800 [ 36%]: Loss = 5177.251\n",
      "Iteration 10900 [ 36%]: Loss = 5581.273\n",
      "Iteration 11000 [ 36%]: Loss = 11455.398\n",
      "Iteration 11100 [ 37%]: Loss = 12271.179\n",
      "Iteration 11200 [ 37%]: Loss = 5782.263\n",
      "Iteration 11300 [ 37%]: Loss = 8100.298\n",
      "Iteration 11400 [ 38%]: Loss = 3875.391\n",
      "Iteration 11500 [ 38%]: Loss = 4139.535\n",
      "Iteration 11600 [ 38%]: Loss = 4143.833\n",
      "Iteration 11700 [ 39%]: Loss = 3708.895\n",
      "Iteration 11800 [ 39%]: Loss = 4476.886\n",
      "Iteration 11900 [ 39%]: Loss = 5649.443\n",
      "Iteration 12000 [ 40%]: Loss = 4974.509\n",
      "Iteration 12100 [ 40%]: Loss = 3829.823\n",
      "Iteration 12200 [ 40%]: Loss = 4618.860\n",
      "Iteration 12300 [ 41%]: Loss = 4285.698\n",
      "Iteration 12400 [ 41%]: Loss = 4926.002\n",
      "Iteration 12500 [ 41%]: Loss = 4164.395\n",
      "Iteration 12600 [ 42%]: Loss = 3508.887\n",
      "Iteration 12700 [ 42%]: Loss = 4513.248\n",
      "Iteration 12800 [ 42%]: Loss = 4534.851\n",
      "Iteration 12900 [ 43%]: Loss = 4205.002\n",
      "Iteration 13000 [ 43%]: Loss = 2839.046\n",
      "Iteration 13100 [ 43%]: Loss = 6077.846\n",
      "Iteration 13200 [ 44%]: Loss = 4128.017\n",
      "Iteration 13300 [ 44%]: Loss = 5165.150\n",
      "Iteration 13400 [ 44%]: Loss = 6428.462\n",
      "Iteration 13500 [ 45%]: Loss = 4719.403\n",
      "Iteration 13600 [ 45%]: Loss = 2979.414\n",
      "Iteration 13700 [ 45%]: Loss = 2889.111\n",
      "Iteration 13800 [ 46%]: Loss = 3449.612\n",
      "Iteration 13900 [ 46%]: Loss = 4514.360\n",
      "Iteration 14000 [ 46%]: Loss = 8530.811\n",
      "Iteration 14100 [ 47%]: Loss = 5180.754\n",
      "Iteration 14200 [ 47%]: Loss = 4164.891\n",
      "Iteration 14300 [ 47%]: Loss = 5202.994\n",
      "Iteration 14400 [ 48%]: Loss = 3225.148\n",
      "Iteration 14500 [ 48%]: Loss = 4638.394\n",
      "Iteration 14600 [ 48%]: Loss = 2824.136\n",
      "Iteration 14700 [ 49%]: Loss = 3525.605\n",
      "Iteration 14800 [ 49%]: Loss = 4546.155\n",
      "Iteration 14900 [ 49%]: Loss = 6095.115\n",
      "Iteration 15000 [ 50%]: Loss = 6275.767\n",
      "Iteration 15100 [ 50%]: Loss = 3407.667\n",
      "Iteration 15200 [ 50%]: Loss = 4802.765\n",
      "Iteration 15300 [ 51%]: Loss = 4827.789\n",
      "Iteration 15400 [ 51%]: Loss = 2804.948\n",
      "Iteration 15500 [ 51%]: Loss = 4567.067\n",
      "Iteration 15600 [ 52%]: Loss = 3703.590\n",
      "Iteration 15700 [ 52%]: Loss = 5254.021\n",
      "Iteration 15800 [ 52%]: Loss = 3920.005\n",
      "Iteration 15900 [ 53%]: Loss = 2306.363\n",
      "Iteration 16000 [ 53%]: Loss = 4438.452\n",
      "Iteration 16100 [ 53%]: Loss = 2267.787\n",
      "Iteration 16200 [ 54%]: Loss = 3201.267\n",
      "Iteration 16300 [ 54%]: Loss = 4853.904\n",
      "Iteration 16400 [ 54%]: Loss = 5913.525\n",
      "Iteration 16500 [ 55%]: Loss = 4912.902\n",
      "Iteration 16600 [ 55%]: Loss = 2385.469\n",
      "Iteration 16700 [ 55%]: Loss = 4930.234\n",
      "Iteration 16800 [ 56%]: Loss = 5463.167\n",
      "Iteration 16900 [ 56%]: Loss = 3701.578\n",
      "Iteration 17000 [ 56%]: Loss = 6031.341\n",
      "Iteration 17100 [ 56%]: Loss = 4512.433\n",
      "Iteration 17200 [ 57%]: Loss = 4283.299\n",
      "Iteration 17300 [ 57%]: Loss = 3060.203\n",
      "Iteration 17400 [ 57%]: Loss = 3976.869\n",
      "Iteration 17500 [ 58%]: Loss = 4778.056\n",
      "Iteration 17600 [ 58%]: Loss = 4365.376\n",
      "Iteration 17700 [ 59%]: Loss = 3600.133\n",
      "Iteration 17800 [ 59%]: Loss = 5252.965\n",
      "Iteration 17900 [ 59%]: Loss = 3121.890\n",
      "Iteration 18000 [ 60%]: Loss = 4145.764\n",
      "Iteration 18100 [ 60%]: Loss = 5044.076\n",
      "Iteration 18200 [ 60%]: Loss = 4202.597\n",
      "Iteration 18300 [ 61%]: Loss = 5593.113\n",
      "Iteration 18400 [ 61%]: Loss = 4230.898\n",
      "Iteration 18500 [ 61%]: Loss = 4969.818\n",
      "Iteration 18600 [ 62%]: Loss = 4712.521\n",
      "Iteration 18700 [ 62%]: Loss = 2995.800\n",
      "Iteration 18800 [ 62%]: Loss = 3676.588\n",
      "Iteration 18900 [ 63%]: Loss = 4860.695\n",
      "Iteration 19000 [ 63%]: Loss = 3700.807\n",
      "Iteration 19100 [ 63%]: Loss = 4368.331\n",
      "Iteration 19200 [ 64%]: Loss = 5588.878\n",
      "Iteration 19300 [ 64%]: Loss = 3337.805\n",
      "Iteration 19400 [ 64%]: Loss = 2874.391\n",
      "Iteration 19500 [ 65%]: Loss = 4961.075\n",
      "Iteration 19600 [ 65%]: Loss = 4813.289\n",
      "Iteration 19700 [ 65%]: Loss = 4299.697\n",
      "Iteration 19800 [ 66%]: Loss = 4117.043\n",
      "Iteration 19900 [ 66%]: Loss = 4647.964\n",
      "Iteration 20000 [ 66%]: Loss = 4050.155\n",
      "Iteration 20100 [ 67%]: Loss = 3519.647\n",
      "Iteration 20200 [ 67%]: Loss = 3630.248\n",
      "Iteration 20300 [ 67%]: Loss = 3701.647\n",
      "Iteration 20400 [ 68%]: Loss = 4484.133\n",
      "Iteration 20500 [ 68%]: Loss = 7291.434\n",
      "Iteration 20600 [ 68%]: Loss = 5055.210\n",
      "Iteration 20700 [ 69%]: Loss = 2247.662\n",
      "Iteration 20800 [ 69%]: Loss = 5889.402\n",
      "Iteration 20900 [ 69%]: Loss = 3487.235\n",
      "Iteration 21000 [ 70%]: Loss = 4028.346\n",
      "Iteration 21100 [ 70%]: Loss = 4119.759\n",
      "Iteration 21200 [ 70%]: Loss = 3430.109\n",
      "Iteration 21300 [ 71%]: Loss = 3194.707\n",
      "Iteration 21400 [ 71%]: Loss = 4699.896\n",
      "Iteration 21500 [ 71%]: Loss = 5750.501\n",
      "Iteration 21600 [ 72%]: Loss = 3438.218\n",
      "Iteration 21700 [ 72%]: Loss = 4284.997\n",
      "Iteration 21800 [ 72%]: Loss = 4911.951\n",
      "Iteration 21900 [ 73%]: Loss = 3597.302\n",
      "Iteration 22000 [ 73%]: Loss = 4772.276\n",
      "Iteration 22100 [ 73%]: Loss = 4180.023\n",
      "Iteration 22200 [ 74%]: Loss = 3447.844\n",
      "Iteration 22300 [ 74%]: Loss = 4626.312\n",
      "Iteration 22400 [ 74%]: Loss = 4110.860\n",
      "Iteration 22500 [ 75%]: Loss = 4171.206\n",
      "Iteration 22600 [ 75%]: Loss = 4367.242\n",
      "Iteration 22700 [ 75%]: Loss = 3183.215\n",
      "Iteration 22800 [ 76%]: Loss = 2992.028\n",
      "Iteration 22900 [ 76%]: Loss = 3960.908\n",
      "Iteration 23000 [ 76%]: Loss = 4866.547\n",
      "Iteration 23100 [ 77%]: Loss = 5600.317\n",
      "Iteration 23200 [ 77%]: Loss = 3925.433\n",
      "Iteration 23300 [ 77%]: Loss = 4117.906\n",
      "Iteration 23400 [ 78%]: Loss = 4425.769\n",
      "Iteration 23500 [ 78%]: Loss = 5964.337\n",
      "Iteration 23600 [ 78%]: Loss = 3542.791\n",
      "Iteration 23700 [ 79%]: Loss = 5129.121\n",
      "Iteration 23800 [ 79%]: Loss = 4377.593\n",
      "Iteration 23900 [ 79%]: Loss = 1945.636\n",
      "Iteration 24000 [ 80%]: Loss = 13769.872\n",
      "Iteration 24100 [ 80%]: Loss = 5513.840\n",
      "Iteration 24200 [ 80%]: Loss = 4371.354\n",
      "Iteration 24300 [ 81%]: Loss = 3951.023\n",
      "Iteration 24400 [ 81%]: Loss = 5462.961\n",
      "Iteration 24500 [ 81%]: Loss = 5220.798\n",
      "Iteration 24600 [ 82%]: Loss = 4075.840\n",
      "Iteration 24700 [ 82%]: Loss = 3971.000\n",
      "Iteration 24800 [ 82%]: Loss = 4151.752\n",
      "Iteration 24900 [ 83%]: Loss = 6018.559\n",
      "Iteration 25000 [ 83%]: Loss = 3981.824\n",
      "Iteration 25100 [ 83%]: Loss = 5008.068\n",
      "Iteration 25200 [ 84%]: Loss = 4780.646\n",
      "Iteration 25300 [ 84%]: Loss = 4716.665\n",
      "Iteration 25400 [ 84%]: Loss = 4067.129\n",
      "Iteration 25500 [ 85%]: Loss = 3133.428\n",
      "Iteration 25600 [ 85%]: Loss = 4553.054\n",
      "Iteration 25700 [ 85%]: Loss = 4110.359\n",
      "Iteration 25800 [ 86%]: Loss = 3231.012\n",
      "Iteration 25900 [ 86%]: Loss = 3925.868\n",
      "Iteration 26000 [ 86%]: Loss = 2550.028\n",
      "Iteration 26100 [ 87%]: Loss = 4920.784\n",
      "Iteration 26200 [ 87%]: Loss = 4295.537\n",
      "Iteration 26300 [ 87%]: Loss = 5198.505\n",
      "Iteration 26400 [ 88%]: Loss = 5581.654\n",
      "Iteration 26500 [ 88%]: Loss = 4423.607\n",
      "Iteration 26600 [ 88%]: Loss = 5853.988\n",
      "Iteration 26700 [ 89%]: Loss = 4129.604\n",
      "Iteration 26800 [ 89%]: Loss = 3769.626\n",
      "Iteration 26900 [ 89%]: Loss = 3813.853\n",
      "Iteration 27000 [ 90%]: Loss = 3773.824\n",
      "Iteration 27100 [ 90%]: Loss = 4656.359\n",
      "Iteration 27200 [ 90%]: Loss = 3948.381\n",
      "Iteration 27300 [ 91%]: Loss = 4884.259\n",
      "Iteration 27400 [ 91%]: Loss = 2159.435\n",
      "Iteration 27500 [ 91%]: Loss = 2703.239\n",
      "Iteration 27600 [ 92%]: Loss = 4889.898\n",
      "Iteration 27700 [ 92%]: Loss = 4142.872\n",
      "Iteration 27800 [ 92%]: Loss = 4104.086\n",
      "Iteration 27900 [ 93%]: Loss = 3372.501\n",
      "Iteration 28000 [ 93%]: Loss = 3771.366\n",
      "Iteration 28100 [ 93%]: Loss = 3704.218\n",
      "Iteration 28200 [ 94%]: Loss = 4751.207\n",
      "Iteration 28300 [ 94%]: Loss = 3727.455\n",
      "Iteration 28400 [ 94%]: Loss = 4881.231\n",
      "Iteration 28500 [ 95%]: Loss = 3500.113\n",
      "Iteration 28600 [ 95%]: Loss = 4050.831\n",
      "Iteration 28700 [ 95%]: Loss = 5158.880\n",
      "Iteration 28800 [ 96%]: Loss = 3847.740\n",
      "Iteration 28900 [ 96%]: Loss = 4211.383\n",
      "Iteration 29000 [ 96%]: Loss = 3012.712\n",
      "Iteration 29100 [ 97%]: Loss = 3687.435\n",
      "Iteration 29200 [ 97%]: Loss = 4127.277\n",
      "Iteration 29300 [ 97%]: Loss = 4229.458\n",
      "Iteration 29400 [ 98%]: Loss = 3179.480\n",
      "Iteration 29500 [ 98%]: Loss = 4202.378\n",
      "Iteration 29600 [ 98%]: Loss = 4707.099\n",
      "Iteration 29700 [ 99%]: Loss = 1695.838\n",
      "Iteration 29800 [ 99%]: Loss = 4209.995\n",
      "Iteration 29900 [ 99%]: Loss = 4412.414\n",
      "Iteration 30000 [100%]: Loss = 4611.991\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=30000, n_print=100, n_samples=1,\n",
    "              logdir='data/run1',\n",
    "              optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "              scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b62fe48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1223a9d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b450d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Zmat = q_Z.value().eval()\n",
    "\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')\n",
    "plt.matshow(Zmat.T, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
