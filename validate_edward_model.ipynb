{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}(e^\\lambda)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B * X)_{us} + (C * Z)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-7, 1^2) \\\\\n",
    "Z_{ks} &\\sim \\mathrm{Bernoulli}(\\pi_k) \\\\\n",
    "\\pi_k &\\equiv \\prod_{i=1}^k \\delta_k \\\\\n",
    "\\delta_j &\\sim \\mathrm{Beta}(3, 1)\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "Nrep = 50  # number of observations per unit per stim\n",
    "NB = 1000  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = np.log(softplus(25 + 5 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.1 * np.random.randn(NU, P))  # regressor effects\n",
    "dC = np.log(np.array([0.25, 0.55, 1.4, 2.2])[np.newaxis, :] + 0.1 * np.random.randn(NU, K))  # latent effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70137886  0.57732232  0.24895886  0.49916835] [ 0.70137886  0.40492167  0.10080884  0.05032058]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127bda5c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACmlJREFUeJzt3V+IbedZB+Dfe3poqBSKWtpIjjGIaDEU2puI5MKxUnJU\naLwSg4h6rSSgSLU3HS8E7yTQS9MSArXVXJgUBFuJQ6liE2xCQ5O0BSH9g+d4UyklINW+XsxuGY+T\n2TvfrDWz1j7PAwf2XmfNt7/1rXetPb/51t6rujsAAAAw4spldwAAAID1EioBAAAYJlQCAAAwTKgE\nAABgmFAJAADAMKESAACAYRcSKqvqelW9UlVfqaoPXsRrwhSq6rGqullVXzyx7Ier6tNV9eWq+vuq\nettl9hG2qaprVfVMVX2pql6sqoc3y9Uyq1JVd1TV56vq+U0tf3izXC2zOlV1paq+UFVPb56rY1Zr\n9lBZVVeSfCTJA0nuTfJQVb1r7teFiXwsx7V70h8n+Yfu/pkkzyT5kwvvFbwx/53kD7r73iQ/n+T3\nNudhtcyqdPd/JfnF7n5vkvck+eWqui9qmXV6JMlLJ56rY1brImYq70vy1e5+tbu/m+QTSR68gNeF\nc+vuzyX51i2LH0zy+Obx40l+7UI7BW9Qd9/o7hc2j7+T5OUk16KWWaHufm3z8I4kV5N01DIrU1XX\nkvxKkr88sVgds1oXESrvSvL1E8+/sVkGa/WO7r6ZHP+ynuQdl9wf2FlV3ZPjGZ5/SfJOtczabC4Z\nfD7JjSSf6e7nopZZn79I8kc5/qPI96ljVssX9cD59fZV4PJV1VuTPJnkkc2M5a21q5ZZvO7+3uby\n12tJ7quqe6OWWZGq+tUkNzdXkNQZq6pjVuMiQuU3k9x94vm1zTJYq5tV9c4kqao7k/zHJfcHtqqq\nqzkOlE9091ObxWqZ1erubyc5SnI9apl1uT/JB6rq35L8VZL3VdUTSW6oY9bqIkLlc0l+qqp+oqre\nnOQ3kjx9Aa8LU6n8378kPp3kdzaPfzvJU7f+ACzQR5O81N2PnlimllmVqnr7978Rs6rekuT9Of6M\nsFpmNbr7Q919d3f/ZI5/L36mu38ryaeijlmp6p5/Zr2qrid5NMch9rHu/vPZXxQmUFUfT3KQ5EeT\n3Ezy4SR/m+Rvkvx4kleT/Hp3/+dl9RG2qar7k3w2yYs5vpyqk3woybNJ/jpqmZWoqnfn+AtMrmz+\nfbK7/6yqfiRqmRWqql9I8ofd/QF1zJpdSKgEAABgP/miHgAAAIYJlQAAAAwTKgEAABgmVAIAADBM\nqAQAAGDY1akaqipfIwsAALCnurtOWz5ZqNy8yJn/f3h4mMPDwylfcnZVp47bpZjq9i9TbdPtejua\nXep4SXWTqB1Ot8ZzMrcf5535GeP57esY7+t27aO5fzd1+SsAAADDhEoAAACGXWioPDg4uMiXg1mo\nY/aFWgYAplATftaq9/F66CV9Ns7n4tZjSXWTqB1gvZx35meM57evY7yv27WPJtxXpzbk8lcAAACG\nCZUAAAAMEyoBAAAYtlOorKrrVfVKVX2lqj44d6cAAABYh62hsqquJPlIkgeS3Jvkoap619wdAwAA\nYPl2mam8L8lXu/vV7v5ukk8keXDebgEAALAGu4TKu5J8/cTzb2yWAQAAcJvzRT0AAAAMu7rDOt9M\ncveJ59c2y/6fw8PDHzw+ODjIwcHBOboGAADA0lV3n71C1ZuSfDnJLyX59yTPJnmou1++Zb3e1tYa\nVdVld+EHphrfqbZpH/f3VJZUN4naAdbLeWd+xnh++zrG+7pd+2jCfXVqQ1tnKrv7f6rq95N8OseX\nyz52a6AEAADg9rR1pnLnhsxUzs5s03osqW4StQOsl/PO/Izx/PZ1jPd1u/bR3DOVvqgHAACAYUIl\nAAAAw4RKAAAAhgmVAAAADBMqAQAAGCZUAgAAMEyoBAAAYJhQCQAAwDChEgAAgGFXp2ysqqZsjpl0\n92V3YdGmqOOpxniqY2pfj82ptmuK/bW0MV5aDS7tvLOk/WVsLsa+bteSGOPbj/PX69vH9+Gz+mKm\nEgAAgGFCJQAAAMOESgAAAIYJlQAAAAwTKgEAABgmVAIAADBMqAQAAGCYUAkAAMAwoRIAAIBhW0Nl\nVT1WVTer6osX0SEAAADWY5eZyo8leWDujgAAALA+W0Nld38uybcuoC8AAACsjM9UAgAAMEyoBAAA\nYNjVy+4AAAAAy3J0dJSjo6Od1q3u3r5S1T1JPtXd7z5jne0NcS677CvOr6rO3cZU+2qKvizR0sZn\niv4sbV/t4xhPaUn7y9gAu1ra+WJplnT+2sf34apKd5/aoV1uKfLxJP+c5Ker6mtV9bvn7hEAAAB7\nYaeZyp0aMlM5O3+duhhmKue3tPExU/n6ljTGU1rS/jI2wK6Wdr5YmiWdv/bxffhcM5UAAADweoRK\nAAAAhgmVAAAADBMqAQAAGCZUAgAAMEyoBAAAYJhQCQAAwDChEgAAgGFCJQAAAMOESgAAAIZVd0/T\nUNU0DQGTm/A4n6SdqezrdjG/KWpH3ZzN8Tk/Y7we9tXZ9nF89nGbkqS7T+2QmUoAAACGCZUAAAAM\nEyoBAAAYJlQCAAAwTKgEAABgmFAJAADAMKESAACAYUIlAAAAw7aGyqq6VlXPVNWXqurFqnr4IjoG\nAADA8lV3n71C1Z1J7uzuF6rqrUn+NcmD3f3KLeud3RBwabYd57uqqknamcq+bhfzm6J21M3ZHJ/z\nM8brYV+dbR/HZx+3KUm6+9QObZ2p7O4b3f3C5vF3kryc5K5puwcAAMAavaHPVFbVPUnek+Tzc3QG\nAACAddk5VG4ufX0yySObGUsAAABuczuFyqq6muNA+UR3PzVvlwAAAFiLXWcqP5rkpe5+dM7OAAAA\nsC673FLk/iS/meR9VfV8VX2hqq7P3zUAAACWbustRXZuyC1FYLH2+GutJ2lnadvF/NxSZH6Oz/kZ\n4/Wwr862j+Ozj9uUnOOWIgAAAPB6hEoAAACGCZUAAAAMEyoBAAAYJlQCAAAwTKgEAABgmFAJAADA\nMKESAACAYUIlAAAAw4RKAAAAhlV3T9NQ1SQNTdifSdoBgDXw/gnL5fhkX3T3qUVophIAAIBhQiUA\nAADDhEoAAACGCZUAAAAMEyoBAAAYJlQCAAAwTKgEAABgmFAJAADAsKvbVqiqO5J8NsmbN+s/2d1/\nOnfHAAAAWL7q7u0rVf1Qd79WVW9K8k9JHu7uZ29ZZ3tDO9ilP7uoqknaAYA18P4Jy+X4ZF9096lF\nuNPlr9392ubhHTmerZzmyAAAAGDVdgqVVXWlqp5PciPJZ7r7uXm7BQAAwBrsOlP5ve5+b5JrSX6u\nqn523m4BAACwBm/o21+7+9tJ/jHJ9Xm6AwAAwJpsDZVV9faqetvm8VuSvD/JK3N3DAAAgOXbekuR\nJD+W5PGqupLjEPrJ7v67ebsFAADAGux0S5GdGnJLEQC4NN4/Ybkcn+yLc91SBAAAAE4jVAIAADBM\nqAQAAGCYUAkAAMAwoRIAAIBhQiUAAADDhEoAALhkR0dHl90FGCZUAgDAJRMqWTOhEgAAgGFCJQAA\nAMOqu6dpqGqahgAAAFic7q7Tlk8WKgEAALj9uPwVAACAYUIlAAAAw4RKAAAAhgmVAAAADBMqAQAA\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12250a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dX = 0.1 * np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "ddelta = stats.beta.rvs(1.2, 1, size=(K,))\n",
    "dpi = np.cumprod(ddelta)\n",
    "print(ddelta, dpi)\n",
    "dZ = stats.bernoulli.rvs(dpi[:, np.newaxis], size=(K, NS))\n",
    "\n",
    "# plot states\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = np.tile(dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1) + np.sum(dC[dU] * dZ[:, dS].T, axis=1), Nrep)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcount = stats.poisson.rvs(np.exp(dlam))\n",
    "dcount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIQQ9AEgIQR9AEgIQR8AEvL//neNIhq8upEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224fc6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dlam, bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224fc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.exp(dlam), bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCS1MigkSa0MCklSq/8PyPnIWq7ZugoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12918bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount.copy()\n",
    "Xdat = np.tile(dX[:, dS], (1, Nrep)).T\n",
    "unit = np.tile(dU, Nrep)\n",
    "stim = np.tile(dS, Nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GLM to get inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "Xdf = pd.DataFrame(Xdat)\n",
    "Xdf.columns = ['X' + str(c) for c in Xdf.columns]\n",
    "dta = pd.concat([pd.DataFrame({'count': count, 'unit': unit, 'stim': stim}), Xdf], axis=1)\n",
    "formula = 'count ~ -1 + C(unit) + C(unit) * (' + '+'.join(Xdf.columns) + ')'\n",
    "mod = smf.glm(formula=formula, data=dta, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the model's fitted parameters into inits for $A$ and $B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_init = mod.params.values[:NU].astype('float32')\n",
    "B_init = mod.params.values[NU:].reshape(P, NU).T.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(125000), Dimension(3)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.train.range_input_producer(N).dequeue_many(NB, name='batch_inds')\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB, name='batch_counts')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = N\n",
    "batch_inds = np.arange(N)\n",
    "batch_counts = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=5 * tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=3 * tf.ones((NU, P)), name='B')\n",
    "    C = ed.models.Normal(mu=tf.zeros((NU, K)), sigma=3 * tf.ones((NU, K)), name='C')  \n",
    "    \n",
    "    delta = ed.models.Beta(a=3 * tf.ones(K), b=tf.ones(K), name='delta')\n",
    "    log_delta = tf.log(delta)\n",
    "\n",
    "    pi = tf.exp(tf.cumsum(log_delta), name='pi')\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-7.0], sigma=[1.], name='sig')\n",
    "\n",
    "    lam_vars = (tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, U) * tf.gather(tf.to_float(Z), S), 1))\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(NB), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "#     q_A = ed.models.NormalWithSoftplusSigma(mu=np.log(25) + tf.Variable(tf.random_normal((NU,))), \n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(A_init), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    tf.scalar_summary('q_A', tf.reduce_mean(q_A.mean()))\n",
    "\n",
    "#     q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(B_init), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    tf.scalar_summary('q_B', tf.reduce_mean(q_B.mean()))\n",
    "\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    tf.scalar_summary('q_C', tf.reduce_mean(q_C.mean()))\n",
    "    \n",
    "#     q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(-2 + tf.random_normal((NS, K))), name='Z')\n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(-1.5 + tf.zeros((NS, K))), name='Z')\n",
    "    tf.scalar_summary('q_Z', tf.reduce_mean(q_Z.mean()))\n",
    "\n",
    "    q_delta = ed.models.BetaWithSoftplusAB(a=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           b=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           name='delta')\n",
    "    tf.scalar_summary('min_q_delta', tf.reduce_min(q_delta.mean()))\n",
    "    tf.scalar_summary('max_q_delta', tf.reduce_max(q_delta.mean()))\n",
    "\n",
    "    lam_mu = tf.Variable(2 + tf.random_normal((N,)))\n",
    "    tf.scalar_summary('lam_mu_mean', tf.reduce_mean(tf.gather(lam_mu, batch_inds)))\n",
    "    lam_sig = tf.Variable(3 * tf.random_uniform((N,)) + 2)\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_ELBO(latent_vars, data, scale):\n",
    "    from edward.util import copy\n",
    "    p_log_prob = 0.0\n",
    "    q_log_prob = 0.0\n",
    "    z_sample = {}\n",
    "    scope = \"ELBO\"\n",
    "\n",
    "    for z, qz in latent_vars.items():\n",
    "        # Copy q(z) to obtain new set of posterior samples.\n",
    "        qz_copy = copy(qz, scope=scope)\n",
    "        z_sample[z] = qz_copy.value()\n",
    "        z_log_prob = tf.reduce_sum(qz.log_prob(tf.stop_gradient(z_sample[z])))\n",
    "        if z in scale:\n",
    "            z_log_prob *= scale[z]\n",
    "\n",
    "        q_log_prob += z_log_prob\n",
    "\n",
    "    dict_swap = z_sample\n",
    "    for x, qx in data.items():\n",
    "        if isinstance(x, ed.RandomVariable):\n",
    "            if isinstance(qx, ed.RandomVariable):\n",
    "                qx_copy = copy(qx, scope=scope)\n",
    "                dict_swap[x] = qx_copy.value()\n",
    "            else:\n",
    "                dict_swap[x] = qx\n",
    "\n",
    "            for z in latent_vars.keys():\n",
    "                z_copy = copy(z, dict_swap, scope=scope)\n",
    "                z_log_prob = tf.reduce_sum(z_copy.log_prob(dict_swap[z]))\n",
    "                if z in scale:\n",
    "                    z_log_prob *= scale[z]\n",
    "\n",
    "                p_log_prob += z_log_prob\n",
    "\n",
    "            for x in data.keys():\n",
    "                if isinstance(x, ed.RandomVariable):\n",
    "                    x_copy = copy(x, dict_swap, scope=scope)\n",
    "                    x_log_prob = tf.reduce_sum(x_copy.log_prob(dict_swap[x]))\n",
    "                if x in scale:\n",
    "                    x_log_prob *= scale[x]\n",
    "\n",
    "                p_log_prob += x_log_prob\n",
    "\n",
    "    return tf.reduce_mean(p_log_prob - q_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = make_ELBO({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, \n",
    "                 {cnt: tf.cast(batch_counts, 'float32')}, \n",
    "                 {lam: N/NB, cnt: N/NB})\n",
    "\n",
    "tf.scalar_summary('ELBO', elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, \n",
    "                    data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference_lam = ed.KLqp({lam: q_lam, sig: q_sig}, data={cnt: batch_counts, Z: q_Z, \n",
    "                                            delta: q_delta, A: q_A, B: q_B, \n",
    "                                            C: q_C, delta: q_delta})\n",
    "inference_coeffs = ed.KLqp({A: q_A, B: q_B, C: q_C}, \n",
    "                    data={cnt: batch_counts, Z: q_Z, delta: q_delta, lam: q_lam, sig: q_sig})\n",
    "inference_latents = ed.KLqp({Z: q_Z, delta: q_delta}, \n",
    "                    data={cnt: batch_counts, A: q_A, B: q_B, C: q_C, sig: q_sig, lam: q_lam})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 [100%]: Loss = 10241294336.000\n",
      "Iteration 1 [100%]: Loss = 22332803072.000\n",
      "Iteration 1 [100%]: Loss = 8233658368.000\n"
     ]
    }
   ],
   "source": [
    "for inf in [inference_lam, inference_coeffs, inference_latents]:\n",
    "    if inf is inference_lam:\n",
    "        logdir = 'data/run40'\n",
    "    else:\n",
    "        logdir = None\n",
    "        \n",
    "    inf.run(n_iter=1, n_print=100, n_samples=1,  # basically just for purposes of initializing\n",
    "                  logdir=logdir,\n",
    "                  optimizer=tf.train.AdamOptimizer(5e-3),\n",
    "                  scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70100 [7010000%]: Loss = 403087.188\n",
      "Iteration 70200 [7020000%]: Loss = 404660.062\n",
      "Iteration 70300 [7030000%]: Loss = 404875.969\n",
      "Iteration 70400 [7040000%]: Loss = 403750.812\n",
      "Iteration 70500 [7050000%]: Loss = 403941.656\n",
      "Iteration 70600 [7060000%]: Loss = 402609.156\n",
      "Iteration 70700 [7070000%]: Loss = 407210.969\n",
      "Iteration 70800 [7080000%]: Loss = 402472.594\n",
      "Iteration 70900 [7090000%]: Loss = 404259.750\n",
      "Iteration 71000 [7100000%]: Loss = 401575.188\n",
      "Iteration 71100 [7110000%]: Loss = 405021.812\n",
      "Iteration 71200 [7120000%]: Loss = 403078.812\n",
      "Iteration 71300 [7130000%]: Loss = 402824.875\n",
      "Iteration 71400 [7140000%]: Loss = 402879.625\n",
      "Iteration 71500 [7150000%]: Loss = 405142.344\n",
      "Iteration 71600 [7160000%]: Loss = 402073.656\n",
      "Iteration 71700 [7170000%]: Loss = 406725.531\n",
      "Iteration 71800 [7180000%]: Loss = 400001.094\n",
      "Iteration 71900 [7190000%]: Loss = 403587.969\n",
      "Iteration 72000 [7200000%]: Loss = 405002.188\n",
      "Iteration 72100 [7210000%]: Loss = 405847.000\n",
      "Iteration 72200 [7220000%]: Loss = 404170.688\n",
      "Iteration 72300 [7230000%]: Loss = 403643.406\n",
      "Iteration 72400 [7240000%]: Loss = 404763.906\n",
      "Iteration 72500 [7250000%]: Loss = 404830.562\n",
      "Iteration 72600 [7260000%]: Loss = 404557.062\n",
      "Iteration 72700 [7270000%]: Loss = 402304.188\n",
      "Iteration 72800 [7280000%]: Loss = 403409.656\n",
      "Iteration 72900 [7290000%]: Loss = 403179.500\n",
      "Iteration 73000 [7300000%]: Loss = 401185.719\n",
      "Iteration 73100 [7310000%]: Loss = 402987.062\n",
      "Iteration 73200 [7320000%]: Loss = 403249.156\n",
      "Iteration 73300 [7330000%]: Loss = 404771.062\n",
      "Iteration 73400 [7340000%]: Loss = 403264.938\n",
      "Iteration 73500 [7350000%]: Loss = 401891.125\n",
      "Iteration 73600 [7360000%]: Loss = 403627.562\n",
      "Iteration 73700 [7370000%]: Loss = 402578.031\n",
      "Iteration 73800 [7380000%]: Loss = 402175.188\n",
      "Iteration 73900 [7390000%]: Loss = 403702.219\n",
      "Iteration 74000 [7400000%]: Loss = 402150.750\n",
      "Iteration 74100 [7410000%]: Loss = 404062.250\n",
      "Iteration 74200 [7420000%]: Loss = 404346.562\n",
      "Iteration 74300 [7430000%]: Loss = 403544.188\n",
      "Iteration 74400 [7440000%]: Loss = 402121.250\n",
      "Iteration 74500 [7450000%]: Loss = 403117.156\n",
      "Iteration 74600 [7460000%]: Loss = 402594.781\n",
      "Iteration 74700 [7470000%]: Loss = 404663.344\n",
      "Iteration 74800 [7480000%]: Loss = 402801.750\n",
      "Iteration 74900 [7490000%]: Loss = 404580.312\n",
      "Iteration 75000 [7500000%]: Loss = 402871.281\n",
      "Iteration 75100 [7510000%]: Loss = 404867.156\n",
      "Iteration 75200 [7520000%]: Loss = 402444.031\n",
      "Iteration 75300 [7530000%]: Loss = 402239.062\n",
      "Iteration 75400 [7540000%]: Loss = 405132.438\n",
      "Iteration 75500 [7550000%]: Loss = 404016.906\n",
      "Iteration 75600 [7560000%]: Loss = 403501.750\n",
      "Iteration 75700 [7570000%]: Loss = 403962.969\n",
      "Iteration 75800 [7580000%]: Loss = 403029.438\n",
      "Iteration 75900 [7590000%]: Loss = 404410.125\n",
      "Iteration 76000 [7600000%]: Loss = 405192.469\n",
      "Iteration 76100 [7610000%]: Loss = 404921.781\n",
      "Iteration 76200 [7620000%]: Loss = 405053.219\n",
      "Iteration 76300 [7630000%]: Loss = 404511.031\n",
      "Iteration 76400 [7640000%]: Loss = 404294.625\n",
      "Iteration 76500 [7650000%]: Loss = 403019.875\n",
      "Iteration 76600 [7660000%]: Loss = 405014.719\n",
      "Iteration 76700 [7670000%]: Loss = 403797.875\n",
      "Iteration 76800 [7680000%]: Loss = 404085.406\n",
      "Iteration 76900 [7690000%]: Loss = 400654.875\n",
      "Iteration 77000 [7700000%]: Loss = 404122.188\n",
      "Iteration 77100 [7710000%]: Loss = 404182.812\n",
      "Iteration 77200 [7720000%]: Loss = 406864.812\n",
      "Iteration 77300 [7730000%]: Loss = 401957.375\n",
      "Iteration 77400 [7740000%]: Loss = 402378.750\n",
      "Iteration 77500 [7750000%]: Loss = 403734.531\n",
      "Iteration 77600 [7760000%]: Loss = 403794.969\n",
      "Iteration 77700 [7770000%]: Loss = 404369.656\n",
      "Iteration 77800 [7780000%]: Loss = 403619.062\n",
      "Iteration 77900 [7790000%]: Loss = 402609.438\n",
      "Iteration 78000 [7800000%]: Loss = 403965.688\n",
      "Iteration 78100 [7810000%]: Loss = 401800.250\n",
      "Iteration 78200 [7820000%]: Loss = 405444.875\n",
      "Iteration 78300 [7830000%]: Loss = 401812.438\n",
      "Iteration 78400 [7840000%]: Loss = 404652.406\n",
      "Iteration 78500 [7850000%]: Loss = 402888.406\n",
      "Iteration 78600 [7860000%]: Loss = 403433.531\n",
      "Iteration 78700 [7870000%]: Loss = 403263.406\n",
      "Iteration 78800 [7880000%]: Loss = 403015.219\n",
      "Iteration 78900 [7890000%]: Loss = 402725.875\n",
      "Iteration 79000 [7900000%]: Loss = 403821.656\n",
      "Iteration 79100 [7910000%]: Loss = 404040.969\n",
      "Iteration 79200 [7920000%]: Loss = 403125.719\n",
      "Iteration 79300 [7930000%]: Loss = 404432.688\n",
      "Iteration 79400 [7940000%]: Loss = 404761.094\n",
      "Iteration 79500 [7950000%]: Loss = 405377.750\n",
      "Iteration 79600 [7960000%]: Loss = 402501.594\n",
      "Iteration 79700 [7970000%]: Loss = 404744.250\n",
      "Iteration 79800 [7980000%]: Loss = 401662.875\n",
      "Iteration 79900 [7990000%]: Loss = 405889.375\n",
      "Iteration 80000 [8000000%]: Loss = 401657.125\n",
      "Iteration 80100 [8010000%]: Loss = 404260.125\n",
      "Iteration 80200 [8020000%]: Loss = 404277.812\n",
      "Iteration 80300 [8030000%]: Loss = 402576.312\n",
      "Iteration 80400 [8040000%]: Loss = 401230.312\n",
      "Iteration 80500 [8050000%]: Loss = 401949.750\n",
      "Iteration 80600 [8060000%]: Loss = 401984.062\n",
      "Iteration 80700 [8070000%]: Loss = 403797.719\n",
      "Iteration 80800 [8080000%]: Loss = 404197.562\n",
      "Iteration 80900 [8090000%]: Loss = 404679.062\n",
      "Iteration 81000 [8100000%]: Loss = 401761.312\n",
      "Iteration 81100 [8110000%]: Loss = 405499.562\n",
      "Iteration 81200 [8120000%]: Loss = 402929.812\n",
      "Iteration 81300 [8130000%]: Loss = 402526.156\n",
      "Iteration 81400 [8140000%]: Loss = 403432.469\n",
      "Iteration 81500 [8150000%]: Loss = 401374.062\n",
      "Iteration 81600 [8160000%]: Loss = 400839.500\n",
      "Iteration 81700 [8170000%]: Loss = 403433.188\n",
      "Iteration 81800 [8180000%]: Loss = 402240.625\n",
      "Iteration 81900 [8190000%]: Loss = 405543.688\n",
      "Iteration 82000 [8200000%]: Loss = 402271.688\n",
      "Iteration 82100 [8210000%]: Loss = 402008.750\n",
      "Iteration 82200 [8220000%]: Loss = 403568.438\n",
      "Iteration 82300 [8230000%]: Loss = 403125.000\n",
      "Iteration 82400 [8240000%]: Loss = 402826.531\n",
      "Iteration 82500 [8250000%]: Loss = 408292.969\n",
      "Iteration 82600 [8260000%]: Loss = 402506.031\n",
      "Iteration 82700 [8270000%]: Loss = 400524.375\n",
      "Iteration 82800 [8280000%]: Loss = 401640.250\n",
      "Iteration 82900 [8290000%]: Loss = 405268.312\n",
      "Iteration 83000 [8300000%]: Loss = 401207.938\n",
      "Iteration 83100 [8310000%]: Loss = 404433.875\n",
      "Iteration 83200 [8320000%]: Loss = 403165.250\n",
      "Iteration 83300 [8330000%]: Loss = 403498.375\n",
      "Iteration 83400 [8340000%]: Loss = 403128.250\n",
      "Iteration 83500 [8350000%]: Loss = 403014.812\n",
      "Iteration 83600 [8360000%]: Loss = 402432.844\n",
      "Iteration 83700 [8370000%]: Loss = 404275.875\n",
      "Iteration 83800 [8380000%]: Loss = 402975.812\n",
      "Iteration 83900 [8390000%]: Loss = 402960.875\n",
      "Iteration 84000 [8400000%]: Loss = 403825.000\n",
      "Iteration 84100 [8410000%]: Loss = 401533.906\n",
      "Iteration 84200 [8420000%]: Loss = 402534.156\n",
      "Iteration 84300 [8430000%]: Loss = 400727.531\n",
      "Iteration 84400 [8440000%]: Loss = 402702.875\n",
      "Iteration 84500 [8450000%]: Loss = 401795.188\n",
      "Iteration 84600 [8460000%]: Loss = 401989.781\n",
      "Iteration 84700 [8470000%]: Loss = 401588.625\n",
      "Iteration 84800 [8480000%]: Loss = 400803.656\n",
      "Iteration 84900 [8490000%]: Loss = 405311.625\n",
      "Iteration 85000 [8500000%]: Loss = nan\n",
      "Iteration 85100 [8510000%]: Loss = nan\n",
      "Iteration 85200 [8520000%]: Loss = nan\n",
      "Iteration 85300 [8530000%]: Loss = nan\n",
      "Iteration 85400 [8540000%]: Loss = nan\n",
      "Iteration 85500 [8550000%]: Loss = nan\n",
      "Iteration 85600 [8560000%]: Loss = nan\n",
      "Iteration 85700 [8570000%]: Loss = nan\n",
      "Iteration 85800 [8580000%]: Loss = nan\n",
      "Iteration 85900 [8590000%]: Loss = nan\n",
      "Iteration 86000 [8600000%]: Loss = nan\n",
      "Iteration 86100 [8610000%]: Loss = nan\n",
      "Iteration 86200 [8620000%]: Loss = nan\n",
      "Iteration 86300 [8630000%]: Loss = nan\n",
      "Iteration 86400 [8640000%]: Loss = nan\n",
      "Iteration 86500 [8650000%]: Loss = nan\n",
      "Iteration 86600 [8660000%]: Loss = nan\n",
      "Iteration 86700 [8670000%]: Loss = nan\n",
      "Iteration 86800 [8680000%]: Loss = nan\n",
      "Iteration 86900 [8690000%]: Loss = nan\n",
      "Iteration 87000 [8700000%]: Loss = nan\n",
      "Iteration 87100 [8710000%]: Loss = nan\n",
      "Iteration 87200 [8720000%]: Loss = nan\n",
      "Iteration 87300 [8730000%]: Loss = nan\n",
      "Iteration 87400 [8740000%]: Loss = nan\n",
      "Iteration 87500 [8750000%]: Loss = nan\n",
      "Iteration 87600 [8760000%]: Loss = nan\n",
      "Iteration 87700 [8770000%]: Loss = nan\n",
      "Iteration 87800 [8780000%]: Loss = nan\n",
      "Iteration 87900 [8790000%]: Loss = nan\n",
      "Iteration 88000 [8800000%]: Loss = nan\n",
      "Iteration 88100 [8810000%]: Loss = nan\n",
      "Iteration 88200 [8820000%]: Loss = nan\n",
      "Iteration 88300 [8830000%]: Loss = nan\n",
      "Iteration 88400 [8840000%]: Loss = nan\n",
      "Iteration 88500 [8850000%]: Loss = nan\n",
      "Iteration 88600 [8860000%]: Loss = nan\n",
      "Iteration 88700 [8870000%]: Loss = nan\n",
      "Iteration 88800 [8880000%]: Loss = nan\n",
      "Iteration 88900 [8890000%]: Loss = nan\n",
      "Iteration 89000 [8900000%]: Loss = nan\n",
      "Iteration 89100 [8910000%]: Loss = nan\n",
      "Iteration 89200 [8920000%]: Loss = nan\n",
      "Iteration 89300 [8930000%]: Loss = nan\n",
      "Iteration 89400 [8940000%]: Loss = nan\n",
      "Iteration 89500 [8950000%]: Loss = nan\n",
      "Iteration 89600 [8960000%]: Loss = nan\n",
      "Iteration 89700 [8970000%]: Loss = nan\n",
      "Iteration 89800 [8980000%]: Loss = nan\n",
      "Iteration 89900 [8990000%]: Loss = nan\n",
      "Iteration 90000 [9000000%]: Loss = nan\n",
      "Iteration 90100 [9010000%]: Loss = nan\n",
      "Iteration 90200 [9020000%]: Loss = nan\n",
      "Iteration 90300 [9030000%]: Loss = nan\n",
      "Iteration 90400 [9040000%]: Loss = nan\n",
      "Iteration 90500 [9050000%]: Loss = nan\n",
      "Iteration 90600 [9060000%]: Loss = nan\n",
      "Iteration 90700 [9070000%]: Loss = nan\n",
      "Iteration 90800 [9080000%]: Loss = nan\n",
      "Iteration 90900 [9090000%]: Loss = nan\n",
      "Iteration 91000 [9100000%]: Loss = nan\n",
      "Iteration 91100 [9110000%]: Loss = nan\n",
      "Iteration 91200 [9120000%]: Loss = nan\n",
      "Iteration 91300 [9130000%]: Loss = nan\n",
      "Iteration 91400 [9140000%]: Loss = nan\n",
      "Iteration 91500 [9150000%]: Loss = nan\n",
      "Iteration 91600 [9160000%]: Loss = nan\n",
      "Iteration 91700 [9170000%]: Loss = nan\n",
      "Iteration 91800 [9180000%]: Loss = nan\n",
      "Iteration 91900 [9190000%]: Loss = nan\n",
      "Iteration 92000 [9200000%]: Loss = nan\n",
      "Iteration 92100 [9210000%]: Loss = nan\n",
      "Iteration 92200 [9220000%]: Loss = nan\n",
      "Iteration 92300 [9230000%]: Loss = nan\n",
      "Iteration 92400 [9240000%]: Loss = nan\n",
      "Iteration 92500 [9250000%]: Loss = nan\n",
      "Iteration 92600 [9260000%]: Loss = nan\n",
      "Iteration 92700 [9270000%]: Loss = nan\n",
      "Iteration 92800 [9280000%]: Loss = nan\n",
      "Iteration 92900 [9290000%]: Loss = nan\n",
      "Iteration 93000 [9300000%]: Loss = nan\n",
      "Iteration 93100 [9310000%]: Loss = nan\n",
      "Iteration 93200 [9320000%]: Loss = nan\n",
      "Iteration 93300 [9330000%]: Loss = nan\n",
      "Iteration 93400 [9340000%]: Loss = nan\n",
      "Iteration 93500 [9350000%]: Loss = nan\n",
      "Iteration 93600 [9360000%]: Loss = nan\n",
      "Iteration 93700 [9370000%]: Loss = nan\n",
      "Iteration 93800 [9380000%]: Loss = nan\n",
      "Iteration 93900 [9390000%]: Loss = nan\n",
      "Iteration 94000 [9400000%]: Loss = nan\n",
      "Iteration 94100 [9410000%]: Loss = nan\n",
      "Iteration 94200 [9420000%]: Loss = nan\n",
      "Iteration 94300 [9430000%]: Loss = nan\n",
      "Iteration 94400 [9440000%]: Loss = nan\n",
      "Iteration 94500 [9450000%]: Loss = nan\n",
      "Iteration 94600 [9460000%]: Loss = nan\n",
      "Iteration 94700 [9470000%]: Loss = nan\n",
      "Iteration 94800 [9480000%]: Loss = nan\n",
      "Iteration 94900 [9490000%]: Loss = nan\n",
      "Iteration 95000 [9500000%]: Loss = nan\n",
      "Iteration 95100 [9510000%]: Loss = nan\n",
      "Iteration 95200 [9520000%]: Loss = nan\n",
      "Iteration 95300 [9530000%]: Loss = nan\n",
      "Iteration 95400 [9540000%]: Loss = nan\n",
      "Iteration 95500 [9550000%]: Loss = nan\n",
      "Iteration 95600 [9560000%]: Loss = nan\n",
      "Iteration 95700 [9570000%]: Loss = nan\n",
      "Iteration 95800 [9580000%]: Loss = nan\n",
      "Iteration 95900 [9590000%]: Loss = nan\n",
      "Iteration 96000 [9600000%]: Loss = nan\n",
      "Iteration 96100 [9610000%]: Loss = nan\n",
      "Iteration 96200 [9620000%]: Loss = nan\n",
      "Iteration 96300 [9630000%]: Loss = nan\n",
      "Iteration 96400 [9640000%]: Loss = nan\n",
      "Iteration 96500 [9650000%]: Loss = nan\n",
      "Iteration 96600 [9660000%]: Loss = nan\n",
      "Iteration 96700 [9670000%]: Loss = nan\n",
      "Iteration 96800 [9680000%]: Loss = nan\n",
      "Iteration 96900 [9690000%]: Loss = nan\n",
      "Iteration 97000 [9700000%]: Loss = nan\n",
      "Iteration 97100 [9710000%]: Loss = nan\n",
      "Iteration 97200 [9720000%]: Loss = nan\n",
      "Iteration 97300 [9730000%]: Loss = nan\n",
      "Iteration 97400 [9740000%]: Loss = nan\n",
      "Iteration 97500 [9750000%]: Loss = nan\n",
      "Iteration 97600 [9760000%]: Loss = nan\n",
      "Iteration 97700 [9770000%]: Loss = nan\n",
      "Iteration 97800 [9780000%]: Loss = nan\n",
      "Iteration 97900 [9790000%]: Loss = nan\n",
      "Iteration 98000 [9800000%]: Loss = nan\n",
      "Iteration 98100 [9810000%]: Loss = nan\n",
      "Iteration 98200 [9820000%]: Loss = nan\n",
      "Iteration 98300 [9830000%]: Loss = nan\n",
      "Iteration 98400 [9840000%]: Loss = nan\n",
      "Iteration 98500 [9850000%]: Loss = nan\n",
      "Iteration 98600 [9860000%]: Loss = nan\n",
      "Iteration 98700 [9870000%]: Loss = nan\n",
      "Iteration 98800 [9880000%]: Loss = nan\n",
      "Iteration 98900 [9890000%]: Loss = nan\n",
      "Iteration 99000 [9900000%]: Loss = nan\n",
      "Iteration 99100 [9910000%]: Loss = nan\n",
      "Iteration 99200 [9920000%]: Loss = nan\n",
      "Iteration 99300 [9930000%]: Loss = nan\n",
      "Iteration 99400 [9940000%]: Loss = nan\n",
      "Iteration 99500 [9950000%]: Loss = nan\n",
      "Iteration 99600 [9960000%]: Loss = nan\n",
      "Iteration 99700 [9970000%]: Loss = nan\n",
      "Iteration 99800 [9980000%]: Loss = nan\n",
      "Iteration 99900 [9990000%]: Loss = nan\n",
      "Iteration 100000 [10000000%]: Loss = nan\n",
      "Iteration 100100 [10010000%]: Loss = nan\n",
      "Iteration 100200 [10020000%]: Loss = nan\n",
      "Iteration 100300 [10030000%]: Loss = nan\n",
      "Iteration 100400 [10040000%]: Loss = nan\n",
      "Iteration 100500 [10050000%]: Loss = nan\n",
      "Iteration 100600 [10060000%]: Loss = nan\n",
      "Iteration 100700 [10070000%]: Loss = nan\n",
      "Iteration 100800 [10080000%]: Loss = nan\n",
      "Iteration 100900 [10090000%]: Loss = nan\n",
      "Iteration 101000 [10100000%]: Loss = nan\n",
      "Iteration 101100 [10110000%]: Loss = nan\n",
      "Iteration 101200 [10120000%]: Loss = nan\n",
      "Iteration 101300 [10130000%]: Loss = nan\n",
      "Iteration 101400 [10140000%]: Loss = nan\n",
      "Iteration 101500 [10150000%]: Loss = nan\n",
      "Iteration 101600 [10160000%]: Loss = nan\n",
      "Iteration 101700 [10170000%]: Loss = nan\n",
      "Iteration 101800 [10180000%]: Loss = nan\n",
      "Iteration 101900 [10190000%]: Loss = nan\n",
      "Iteration 102000 [10200000%]: Loss = nan\n",
      "Iteration 102100 [10210000%]: Loss = nan\n",
      "Iteration 102200 [10220000%]: Loss = nan\n",
      "Iteration 102300 [10230000%]: Loss = nan\n",
      "Iteration 102400 [10240000%]: Loss = nan\n",
      "Iteration 102500 [10250000%]: Loss = nan\n",
      "Iteration 102600 [10260000%]: Loss = nan\n",
      "Iteration 102700 [10270000%]: Loss = nan\n",
      "Iteration 102800 [10280000%]: Loss = nan\n",
      "Iteration 102900 [10290000%]: Loss = nan\n",
      "Iteration 103000 [10300000%]: Loss = nan\n",
      "Iteration 103100 [10310000%]: Loss = nan\n",
      "Iteration 103200 [10320000%]: Loss = nan\n",
      "Iteration 103300 [10330000%]: Loss = nan\n",
      "Iteration 103400 [10340000%]: Loss = nan\n",
      "Iteration 103500 [10350000%]: Loss = nan\n",
      "Iteration 103600 [10360000%]: Loss = nan\n",
      "Iteration 103700 [10370000%]: Loss = nan\n",
      "Iteration 103800 [10380000%]: Loss = nan\n",
      "Iteration 103900 [10390000%]: Loss = nan\n",
      "Iteration 104000 [10400000%]: Loss = nan\n",
      "Iteration 104100 [10410000%]: Loss = nan\n",
      "Iteration 104200 [10420000%]: Loss = nan\n",
      "Iteration 104300 [10430000%]: Loss = nan\n",
      "Iteration 104400 [10440000%]: Loss = nan\n",
      "Iteration 104500 [10450000%]: Loss = nan\n",
      "Iteration 104600 [10460000%]: Loss = nan\n",
      "Iteration 104700 [10470000%]: Loss = nan\n",
      "Iteration 104800 [10480000%]: Loss = nan\n",
      "Iteration 104900 [10490000%]: Loss = nan\n",
      "Iteration 105000 [10500000%]: Loss = nan\n",
      "Iteration 105100 [10510000%]: Loss = nan\n",
      "Iteration 105200 [10520000%]: Loss = nan\n",
      "Iteration 105300 [10530000%]: Loss = nan\n",
      "Iteration 105400 [10540000%]: Loss = nan\n",
      "Iteration 105500 [10550000%]: Loss = nan\n",
      "Iteration 105600 [10560000%]: Loss = nan\n",
      "Iteration 105700 [10570000%]: Loss = nan\n",
      "Iteration 105800 [10580000%]: Loss = nan\n",
      "Iteration 105900 [10590000%]: Loss = nan\n",
      "Iteration 106000 [10600000%]: Loss = nan\n",
      "Iteration 106100 [10610000%]: Loss = nan\n",
      "Iteration 106200 [10620000%]: Loss = nan\n",
      "Iteration 106300 [10630000%]: Loss = nan\n",
      "Iteration 106400 [10640000%]: Loss = nan\n",
      "Iteration 106500 [10650000%]: Loss = nan\n",
      "Iteration 106600 [10660000%]: Loss = nan\n",
      "Iteration 106700 [10670000%]: Loss = nan\n",
      "Iteration 106800 [10680000%]: Loss = nan\n",
      "Iteration 106900 [10690000%]: Loss = nan\n",
      "Iteration 107000 [10700000%]: Loss = nan\n",
      "Iteration 107100 [10710000%]: Loss = nan\n",
      "Iteration 107200 [10720000%]: Loss = nan\n",
      "Iteration 107300 [10730000%]: Loss = nan\n",
      "Iteration 107400 [10740000%]: Loss = nan\n",
      "Iteration 107500 [10750000%]: Loss = nan\n",
      "Iteration 107600 [10760000%]: Loss = nan\n",
      "Iteration 107700 [10770000%]: Loss = nan\n",
      "Iteration 107800 [10780000%]: Loss = nan\n",
      "Iteration 107900 [10790000%]: Loss = nan\n",
      "Iteration 108000 [10800000%]: Loss = nan\n",
      "Iteration 108100 [10810000%]: Loss = nan\n",
      "Iteration 108200 [10820000%]: Loss = nan\n",
      "Iteration 108300 [10830000%]: Loss = nan\n",
      "Iteration 108400 [10840000%]: Loss = nan\n",
      "Iteration 108500 [10850000%]: Loss = nan\n",
      "Iteration 108600 [10860000%]: Loss = nan\n",
      "Iteration 108700 [10870000%]: Loss = nan\n",
      "Iteration 108800 [10880000%]: Loss = nan\n",
      "Iteration 108900 [10890000%]: Loss = nan\n",
      "Iteration 109000 [10900000%]: Loss = nan\n",
      "Iteration 109100 [10910000%]: Loss = nan\n",
      "Iteration 109200 [10920000%]: Loss = nan\n",
      "Iteration 109300 [10930000%]: Loss = nan\n",
      "Iteration 109400 [10940000%]: Loss = nan\n",
      "Iteration 109500 [10950000%]: Loss = nan\n",
      "Iteration 109600 [10960000%]: Loss = nan\n",
      "Iteration 109700 [10970000%]: Loss = nan\n",
      "Iteration 109800 [10980000%]: Loss = nan\n",
      "Iteration 109900 [10990000%]: Loss = nan\n",
      "Iteration 110000 [11000000%]: Loss = nan\n",
      "Iteration 110100 [11010000%]: Loss = nan\n",
      "Iteration 110200 [11020000%]: Loss = nan\n",
      "Iteration 110300 [11030000%]: Loss = nan\n",
      "Iteration 110400 [11040000%]: Loss = nan\n",
      "Iteration 110500 [11050000%]: Loss = nan\n",
      "Iteration 110600 [11060000%]: Loss = nan\n",
      "Iteration 110700 [11070000%]: Loss = nan\n",
      "Iteration 110800 [11080000%]: Loss = nan\n",
      "Iteration 110900 [11090000%]: Loss = nan\n",
      "Iteration 111000 [11100000%]: Loss = nan\n",
      "Iteration 111100 [11110000%]: Loss = nan\n",
      "Iteration 111200 [11120000%]: Loss = nan\n",
      "Iteration 111300 [11130000%]: Loss = nan\n",
      "Iteration 111400 [11140000%]: Loss = nan\n",
      "Iteration 111500 [11150000%]: Loss = nan\n",
      "Iteration 111600 [11160000%]: Loss = nan\n",
      "Iteration 111700 [11170000%]: Loss = nan\n",
      "Iteration 111800 [11180000%]: Loss = nan\n",
      "Iteration 111900 [11190000%]: Loss = nan\n",
      "Iteration 112000 [11200000%]: Loss = nan\n",
      "Iteration 112100 [11210000%]: Loss = nan\n",
      "Iteration 112200 [11220000%]: Loss = nan\n",
      "Iteration 112300 [11230000%]: Loss = nan\n",
      "Iteration 112400 [11240000%]: Loss = nan\n",
      "Iteration 112500 [11250000%]: Loss = nan\n",
      "Iteration 112600 [11260000%]: Loss = nan\n",
      "Iteration 112700 [11270000%]: Loss = nan\n",
      "Iteration 112800 [11280000%]: Loss = nan\n",
      "Iteration 112900 [11290000%]: Loss = nan\n",
      "Iteration 113000 [11300000%]: Loss = nan\n",
      "Iteration 113100 [11310000%]: Loss = nan\n",
      "Iteration 113200 [11320000%]: Loss = nan\n",
      "Iteration 113300 [11330000%]: Loss = nan\n",
      "Iteration 113400 [11340000%]: Loss = nan\n",
      "Iteration 113500 [11350000%]: Loss = nan\n",
      "Iteration 113600 [11360000%]: Loss = nan\n",
      "Iteration 113700 [11370000%]: Loss = nan\n",
      "Iteration 113800 [11380000%]: Loss = nan\n",
      "Iteration 113900 [11390000%]: Loss = nan\n",
      "Iteration 114000 [11400000%]: Loss = nan\n",
      "Iteration 114100 [11410000%]: Loss = nan\n",
      "Iteration 114200 [11420000%]: Loss = nan\n",
      "Iteration 114300 [11430000%]: Loss = nan\n",
      "Iteration 114400 [11440000%]: Loss = nan\n",
      "Iteration 114500 [11450000%]: Loss = nan\n",
      "Iteration 114600 [11460000%]: Loss = nan\n",
      "Iteration 114700 [11470000%]: Loss = nan\n",
      "Iteration 114800 [11480000%]: Loss = nan\n",
      "Iteration 114900 [11490000%]: Loss = nan\n",
      "Iteration 115000 [11500000%]: Loss = nan\n",
      "Iteration 115100 [11510000%]: Loss = nan\n",
      "Iteration 115200 [11520000%]: Loss = nan\n",
      "Iteration 115300 [11530000%]: Loss = nan\n",
      "Iteration 115400 [11540000%]: Loss = nan\n",
      "Iteration 115500 [11550000%]: Loss = nan\n",
      "Iteration 115600 [11560000%]: Loss = nan\n",
      "Iteration 115700 [11570000%]: Loss = nan\n",
      "Iteration 115800 [11580000%]: Loss = nan\n",
      "Iteration 115900 [11590000%]: Loss = nan\n",
      "Iteration 116000 [11600000%]: Loss = nan\n",
      "Iteration 116100 [11610000%]: Loss = nan\n",
      "Iteration 116200 [11620000%]: Loss = nan\n",
      "Iteration 116300 [11630000%]: Loss = nan\n",
      "Iteration 116400 [11640000%]: Loss = nan\n",
      "Iteration 116500 [11650000%]: Loss = nan\n",
      "Iteration 116600 [11660000%]: Loss = nan\n",
      "Iteration 116700 [11670000%]: Loss = nan\n",
      "Iteration 116800 [11680000%]: Loss = nan\n",
      "Iteration 116900 [11690000%]: Loss = nan\n",
      "Iteration 117000 [11700000%]: Loss = nan\n",
      "Iteration 117100 [11710000%]: Loss = nan\n",
      "Iteration 117200 [11720000%]: Loss = nan\n",
      "Iteration 117300 [11730000%]: Loss = nan\n",
      "Iteration 117400 [11740000%]: Loss = nan\n",
      "Iteration 117500 [11750000%]: Loss = nan\n",
      "Iteration 117600 [11760000%]: Loss = nan\n",
      "Iteration 117700 [11770000%]: Loss = nan\n",
      "Iteration 117800 [11780000%]: Loss = nan\n",
      "Iteration 117900 [11790000%]: Loss = nan\n",
      "Iteration 118000 [11800000%]: Loss = nan\n",
      "Iteration 118100 [11810000%]: Loss = nan\n",
      "Iteration 118200 [11820000%]: Loss = nan\n",
      "Iteration 118300 [11830000%]: Loss = nan\n",
      "Iteration 118400 [11840000%]: Loss = nan\n",
      "Iteration 118500 [11850000%]: Loss = nan\n",
      "Iteration 118600 [11860000%]: Loss = nan\n",
      "Iteration 118700 [11870000%]: Loss = nan\n",
      "Iteration 118800 [11880000%]: Loss = nan\n",
      "Iteration 118900 [11890000%]: Loss = nan\n",
      "Iteration 119000 [11900000%]: Loss = nan\n",
      "Iteration 119100 [11910000%]: Loss = nan\n",
      "Iteration 119200 [11920000%]: Loss = nan\n",
      "Iteration 119300 [11930000%]: Loss = nan\n",
      "Iteration 119400 [11940000%]: Loss = nan\n",
      "Iteration 119500 [11950000%]: Loss = nan\n",
      "Iteration 119600 [11960000%]: Loss = nan\n",
      "Iteration 119700 [11970000%]: Loss = nan\n",
      "Iteration 119800 [11980000%]: Loss = nan\n",
      "Iteration 119900 [11990000%]: Loss = nan\n",
      "Iteration 120000 [12000000%]: Loss = nan\n",
      "Iteration 120100 [12010000%]: Loss = nan\n",
      "Iteration 120200 [12020000%]: Loss = nan\n",
      "Iteration 120300 [12030000%]: Loss = nan\n",
      "Iteration 120400 [12040000%]: Loss = nan\n",
      "Iteration 120500 [12050000%]: Loss = nan\n",
      "Iteration 120600 [12060000%]: Loss = nan\n",
      "Iteration 120700 [12070000%]: Loss = nan\n",
      "Iteration 120800 [12080000%]: Loss = nan\n",
      "Iteration 120900 [12090000%]: Loss = nan\n",
      "Iteration 121000 [12100000%]: Loss = nan\n",
      "Iteration 121100 [12110000%]: Loss = nan\n",
      "Iteration 121200 [12120000%]: Loss = nan\n",
      "Iteration 121300 [12130000%]: Loss = nan\n",
      "Iteration 121400 [12140000%]: Loss = nan\n",
      "Iteration 121500 [12150000%]: Loss = nan\n",
      "Iteration 121600 [12160000%]: Loss = nan\n",
      "Iteration 121700 [12170000%]: Loss = nan\n",
      "Iteration 121800 [12180000%]: Loss = nan\n",
      "Iteration 121900 [12190000%]: Loss = nan\n",
      "Iteration 122000 [12200000%]: Loss = nan\n",
      "Iteration 122100 [12210000%]: Loss = nan\n",
      "Iteration 122200 [12220000%]: Loss = nan\n",
      "Iteration 122300 [12230000%]: Loss = nan\n",
      "Iteration 122400 [12240000%]: Loss = nan\n",
      "Iteration 122500 [12250000%]: Loss = nan\n",
      "Iteration 122600 [12260000%]: Loss = nan\n",
      "Iteration 122700 [12270000%]: Loss = nan\n",
      "Iteration 122800 [12280000%]: Loss = nan\n",
      "Iteration 122900 [12290000%]: Loss = nan\n",
      "Iteration 123000 [12300000%]: Loss = nan\n",
      "Iteration 123100 [12310000%]: Loss = nan\n",
      "Iteration 123200 [12320000%]: Loss = nan\n",
      "Iteration 123300 [12330000%]: Loss = nan\n",
      "Iteration 123400 [12340000%]: Loss = nan\n",
      "Iteration 123500 [12350000%]: Loss = nan\n",
      "Iteration 123600 [12360000%]: Loss = nan\n",
      "Iteration 123700 [12370000%]: Loss = nan\n",
      "Iteration 123800 [12380000%]: Loss = nan\n",
      "Iteration 123900 [12390000%]: Loss = nan\n",
      "Iteration 124000 [12400000%]: Loss = nan\n",
      "Iteration 124100 [12410000%]: Loss = nan\n",
      "Iteration 124200 [12420000%]: Loss = nan\n",
      "Iteration 124300 [12430000%]: Loss = nan\n",
      "Iteration 124400 [12440000%]: Loss = nan\n",
      "Iteration 124500 [12450000%]: Loss = nan\n",
      "Iteration 124600 [12460000%]: Loss = nan\n",
      "Iteration 124700 [12470000%]: Loss = nan\n",
      "Iteration 124800 [12480000%]: Loss = nan\n",
      "Iteration 124900 [12490000%]: Loss = nan\n",
      "Iteration 125000 [12500000%]: Loss = nan\n",
      "Iteration 125100 [12510000%]: Loss = nan\n",
      "Iteration 125200 [12520000%]: Loss = nan\n",
      "Iteration 125300 [12530000%]: Loss = nan\n",
      "Iteration 125400 [12540000%]: Loss = nan\n",
      "Iteration 125500 [12550000%]: Loss = nan\n",
      "Iteration 125600 [12560000%]: Loss = nan\n",
      "Iteration 125700 [12570000%]: Loss = nan\n",
      "Iteration 125800 [12580000%]: Loss = nan\n",
      "Iteration 125900 [12590000%]: Loss = nan\n",
      "Iteration 126000 [12600000%]: Loss = nan\n",
      "Iteration 126100 [12610000%]: Loss = nan\n",
      "Iteration 126200 [12620000%]: Loss = nan\n",
      "Iteration 126300 [12630000%]: Loss = nan\n",
      "Iteration 126400 [12640000%]: Loss = nan\n",
      "Iteration 126500 [12650000%]: Loss = nan\n",
      "Iteration 126600 [12660000%]: Loss = nan\n",
      "Iteration 126700 [12670000%]: Loss = nan\n",
      "Iteration 126800 [12680000%]: Loss = nan\n",
      "Iteration 126900 [12690000%]: Loss = nan\n",
      "Iteration 127000 [12700000%]: Loss = nan\n",
      "Iteration 127100 [12710000%]: Loss = nan\n",
      "Iteration 127200 [12720000%]: Loss = nan\n",
      "Iteration 127300 [12730000%]: Loss = nan\n",
      "Iteration 127400 [12740000%]: Loss = nan\n",
      "Iteration 127500 [12750000%]: Loss = nan\n",
      "Iteration 127600 [12760000%]: Loss = nan\n",
      "Iteration 127700 [12770000%]: Loss = nan\n",
      "Iteration 127800 [12780000%]: Loss = nan\n",
      "Iteration 127900 [12790000%]: Loss = nan\n",
      "Iteration 128000 [12800000%]: Loss = nan\n",
      "Iteration 128100 [12810000%]: Loss = nan\n",
      "Iteration 128200 [12820000%]: Loss = nan\n",
      "Iteration 128300 [12830000%]: Loss = nan\n",
      "Iteration 128400 [12840000%]: Loss = nan\n",
      "Iteration 128500 [12850000%]: Loss = nan\n",
      "Iteration 128600 [12860000%]: Loss = nan\n",
      "Iteration 128700 [12870000%]: Loss = nan\n",
      "Iteration 128800 [12880000%]: Loss = nan\n",
      "Iteration 128900 [12890000%]: Loss = nan\n",
      "Iteration 129000 [12900000%]: Loss = nan\n",
      "Iteration 129100 [12910000%]: Loss = nan\n",
      "Iteration 129200 [12920000%]: Loss = nan\n",
      "Iteration 129300 [12930000%]: Loss = nan\n",
      "Iteration 129400 [12940000%]: Loss = nan\n",
      "Iteration 129500 [12950000%]: Loss = nan\n",
      "Iteration 129600 [12960000%]: Loss = nan\n",
      "Iteration 129700 [12970000%]: Loss = nan\n",
      "Iteration 129800 [12980000%]: Loss = nan\n",
      "Iteration 129900 [12990000%]: Loss = nan\n",
      "Iteration 130000 [13000000%]: Loss = nan\n",
      "Iteration 130100 [13010000%]: Loss = nan\n",
      "Iteration 130200 [13020000%]: Loss = nan\n",
      "Iteration 130300 [13030000%]: Loss = nan\n",
      "Iteration 130400 [13040000%]: Loss = nan\n",
      "Iteration 130500 [13050000%]: Loss = nan\n",
      "Iteration 130600 [13060000%]: Loss = nan\n",
      "Iteration 130700 [13070000%]: Loss = nan\n",
      "Iteration 130800 [13080000%]: Loss = nan\n",
      "Iteration 130900 [13090000%]: Loss = nan\n",
      "Iteration 131000 [13100000%]: Loss = nan\n",
      "Iteration 131100 [13110000%]: Loss = nan\n",
      "Iteration 131200 [13120000%]: Loss = nan\n",
      "Iteration 131300 [13130000%]: Loss = nan\n",
      "Iteration 131400 [13140000%]: Loss = nan\n",
      "Iteration 131500 [13150000%]: Loss = nan\n",
      "Iteration 131600 [13160000%]: Loss = nan\n",
      "Iteration 131700 [13170000%]: Loss = nan\n",
      "Iteration 131800 [13180000%]: Loss = nan\n",
      "Iteration 131900 [13190000%]: Loss = nan\n",
      "Iteration 132000 [13200000%]: Loss = nan\n",
      "Iteration 132100 [13210000%]: Loss = nan\n",
      "Iteration 132200 [13220000%]: Loss = nan\n",
      "Iteration 132300 [13230000%]: Loss = nan\n",
      "Iteration 132400 [13240000%]: Loss = nan\n",
      "Iteration 132500 [13250000%]: Loss = nan\n",
      "Iteration 132600 [13260000%]: Loss = nan\n",
      "Iteration 132700 [13270000%]: Loss = nan\n",
      "Iteration 132800 [13280000%]: Loss = nan\n",
      "Iteration 132900 [13290000%]: Loss = nan\n",
      "Iteration 133000 [13300000%]: Loss = nan\n",
      "Iteration 133100 [13310000%]: Loss = nan\n",
      "Iteration 133200 [13320000%]: Loss = nan\n",
      "Iteration 133300 [13330000%]: Loss = nan\n",
      "Iteration 133400 [13340000%]: Loss = nan\n",
      "Iteration 133500 [13350000%]: Loss = nan\n",
      "Iteration 133600 [13360000%]: Loss = nan\n",
      "Iteration 133700 [13370000%]: Loss = nan\n",
      "Iteration 133800 [13380000%]: Loss = nan\n",
      "Iteration 133900 [13390000%]: Loss = nan\n",
      "Iteration 134000 [13400000%]: Loss = nan\n",
      "Iteration 134100 [13410000%]: Loss = nan\n",
      "Iteration 134200 [13420000%]: Loss = nan\n",
      "Iteration 134300 [13430000%]: Loss = nan\n",
      "Iteration 134400 [13440000%]: Loss = nan\n",
      "Iteration 134500 [13450000%]: Loss = nan\n",
      "Iteration 134600 [13460000%]: Loss = nan\n",
      "Iteration 134700 [13470000%]: Loss = nan\n",
      "Iteration 134800 [13480000%]: Loss = nan\n",
      "Iteration 134900 [13490000%]: Loss = nan\n",
      "Iteration 135000 [13500000%]: Loss = nan\n",
      "Iteration 135100 [13510000%]: Loss = nan\n",
      "Iteration 135200 [13520000%]: Loss = nan\n",
      "Iteration 135300 [13530000%]: Loss = nan\n",
      "Iteration 135400 [13540000%]: Loss = nan\n",
      "Iteration 135500 [13550000%]: Loss = nan\n",
      "Iteration 135600 [13560000%]: Loss = nan\n",
      "Iteration 135700 [13570000%]: Loss = nan\n",
      "Iteration 135800 [13580000%]: Loss = nan\n",
      "Iteration 135900 [13590000%]: Loss = nan\n",
      "Iteration 136000 [13600000%]: Loss = nan\n",
      "Iteration 136100 [13610000%]: Loss = nan\n",
      "Iteration 136200 [13620000%]: Loss = nan\n",
      "Iteration 136300 [13630000%]: Loss = nan\n",
      "Iteration 136400 [13640000%]: Loss = nan\n",
      "Iteration 136500 [13650000%]: Loss = nan\n",
      "Iteration 136600 [13660000%]: Loss = nan\n",
      "Iteration 136700 [13670000%]: Loss = nan\n",
      "Iteration 136800 [13680000%]: Loss = nan\n",
      "Iteration 136900 [13690000%]: Loss = nan\n",
      "Iteration 137000 [13700000%]: Loss = nan\n",
      "Iteration 137100 [13710000%]: Loss = nan\n",
      "Iteration 137200 [13720000%]: Loss = nan\n",
      "Iteration 137300 [13730000%]: Loss = nan\n",
      "Iteration 137400 [13740000%]: Loss = nan\n",
      "Iteration 137500 [13750000%]: Loss = nan\n",
      "Iteration 137600 [13760000%]: Loss = nan\n",
      "Iteration 137700 [13770000%]: Loss = nan\n",
      "Iteration 137800 [13780000%]: Loss = nan\n",
      "Iteration 137900 [13790000%]: Loss = nan\n",
      "Iteration 138000 [13800000%]: Loss = nan\n",
      "Iteration 138100 [13810000%]: Loss = nan\n",
      "Iteration 138200 [13820000%]: Loss = nan\n",
      "Iteration 138300 [13830000%]: Loss = nan\n",
      "Iteration 138400 [13840000%]: Loss = nan\n",
      "Iteration 138500 [13850000%]: Loss = nan\n",
      "Iteration 138600 [13860000%]: Loss = nan\n",
      "Iteration 138700 [13870000%]: Loss = nan\n",
      "Iteration 138800 [13880000%]: Loss = nan\n",
      "Iteration 138900 [13890000%]: Loss = nan\n",
      "Iteration 139000 [13900000%]: Loss = nan\n",
      "Iteration 139100 [13910000%]: Loss = nan\n",
      "Iteration 139200 [13920000%]: Loss = nan\n",
      "Iteration 139300 [13930000%]: Loss = nan\n",
      "Iteration 139400 [13940000%]: Loss = nan\n",
      "Iteration 139500 [13950000%]: Loss = nan\n",
      "Iteration 139600 [13960000%]: Loss = nan\n",
      "Iteration 139700 [13970000%]: Loss = nan\n",
      "Iteration 139800 [13980000%]: Loss = nan\n",
      "Iteration 139900 [13990000%]: Loss = nan\n",
      "Iteration 140000 [14000000%]: Loss = nan\n",
      "Iteration 140100 [14010000%]: Loss = nan\n",
      "Iteration 140200 [14020000%]: Loss = nan\n",
      "Iteration 140300 [14030000%]: Loss = nan\n",
      "Iteration 140400 [14040000%]: Loss = nan\n",
      "Iteration 140500 [14050000%]: Loss = nan\n",
      "Iteration 140600 [14060000%]: Loss = nan\n",
      "Iteration 140700 [14070000%]: Loss = nan\n",
      "Iteration 140800 [14080000%]: Loss = nan\n",
      "Iteration 140900 [14090000%]: Loss = nan\n",
      "Iteration 141000 [14100000%]: Loss = nan\n",
      "Iteration 141100 [14110000%]: Loss = nan\n",
      "Iteration 141200 [14120000%]: Loss = nan\n",
      "Iteration 141300 [14130000%]: Loss = nan\n",
      "Iteration 141400 [14140000%]: Loss = nan\n",
      "Iteration 141500 [14150000%]: Loss = nan\n",
      "Iteration 141600 [14160000%]: Loss = nan\n",
      "Iteration 141700 [14170000%]: Loss = nan\n",
      "Iteration 141800 [14180000%]: Loss = nan\n",
      "Iteration 141900 [14190000%]: Loss = nan\n",
      "Iteration 142000 [14200000%]: Loss = nan\n",
      "Iteration 142100 [14210000%]: Loss = nan\n",
      "Iteration 142200 [14220000%]: Loss = nan\n",
      "Iteration 142300 [14230000%]: Loss = nan\n",
      "Iteration 142400 [14240000%]: Loss = nan\n",
      "Iteration 142500 [14250000%]: Loss = nan\n",
      "Iteration 142600 [14260000%]: Loss = nan\n",
      "Iteration 142700 [14270000%]: Loss = nan\n",
      "Iteration 142800 [14280000%]: Loss = nan\n",
      "Iteration 142900 [14290000%]: Loss = nan\n",
      "Iteration 143000 [14300000%]: Loss = nan\n",
      "Iteration 143100 [14310000%]: Loss = nan\n",
      "Iteration 143200 [14320000%]: Loss = nan\n",
      "Iteration 143300 [14330000%]: Loss = nan\n",
      "Iteration 143400 [14340000%]: Loss = nan\n",
      "Iteration 143500 [14350000%]: Loss = nan\n",
      "Iteration 143600 [14360000%]: Loss = nan\n",
      "Iteration 143700 [14370000%]: Loss = nan\n",
      "Iteration 143800 [14380000%]: Loss = nan\n",
      "Iteration 143900 [14390000%]: Loss = nan\n",
      "Iteration 144000 [14400000%]: Loss = nan\n",
      "Iteration 144100 [14410000%]: Loss = nan\n",
      "Iteration 144200 [14420000%]: Loss = nan\n",
      "Iteration 144300 [14430000%]: Loss = nan\n",
      "Iteration 144400 [14440000%]: Loss = nan\n",
      "Iteration 144500 [14450000%]: Loss = nan\n",
      "Iteration 144600 [14460000%]: Loss = nan\n",
      "Iteration 144700 [14470000%]: Loss = nan\n",
      "Iteration 144800 [14480000%]: Loss = nan\n",
      "Iteration 144900 [14490000%]: Loss = nan\n",
      "Iteration 145000 [14500000%]: Loss = nan\n",
      "Iteration 145100 [14510000%]: Loss = nan\n",
      "Iteration 145200 [14520000%]: Loss = nan\n",
      "Iteration 145300 [14530000%]: Loss = nan\n",
      "Iteration 145400 [14540000%]: Loss = nan\n",
      "Iteration 145500 [14550000%]: Loss = nan\n",
      "Iteration 145600 [14560000%]: Loss = nan\n",
      "Iteration 145700 [14570000%]: Loss = nan\n",
      "Iteration 145800 [14580000%]: Loss = nan\n",
      "Iteration 145900 [14590000%]: Loss = nan\n",
      "Iteration 146000 [14600000%]: Loss = nan\n",
      "Iteration 146100 [14610000%]: Loss = nan\n",
      "Iteration 146200 [14620000%]: Loss = nan\n",
      "Iteration 146300 [14630000%]: Loss = nan\n",
      "Iteration 146400 [14640000%]: Loss = nan\n",
      "Iteration 146500 [14650000%]: Loss = nan\n",
      "Iteration 146600 [14660000%]: Loss = nan\n",
      "Iteration 146700 [14670000%]: Loss = nan\n",
      "Iteration 146800 [14680000%]: Loss = nan\n",
      "Iteration 146900 [14690000%]: Loss = nan\n",
      "Iteration 147000 [14700000%]: Loss = nan\n",
      "Iteration 147100 [14710000%]: Loss = nan\n",
      "Iteration 147200 [14720000%]: Loss = nan\n",
      "Iteration 147300 [14730000%]: Loss = nan\n",
      "Iteration 147400 [14740000%]: Loss = nan\n",
      "Iteration 147500 [14750000%]: Loss = nan\n",
      "Iteration 147600 [14760000%]: Loss = nan\n",
      "Iteration 147700 [14770000%]: Loss = nan\n",
      "Iteration 147800 [14780000%]: Loss = nan\n",
      "Iteration 147900 [14790000%]: Loss = nan\n",
      "Iteration 148000 [14800000%]: Loss = nan\n",
      "Iteration 148100 [14810000%]: Loss = nan\n",
      "Iteration 148200 [14820000%]: Loss = nan\n",
      "Iteration 148300 [14830000%]: Loss = nan\n",
      "Iteration 148400 [14840000%]: Loss = nan\n",
      "Iteration 148500 [14850000%]: Loss = nan\n",
      "Iteration 148600 [14860000%]: Loss = nan\n",
      "Iteration 148700 [14870000%]: Loss = nan\n",
      "Iteration 148800 [14880000%]: Loss = nan\n",
      "Iteration 148900 [14890000%]: Loss = nan\n",
      "Iteration 149000 [14900000%]: Loss = nan\n",
      "Iteration 149100 [14910000%]: Loss = nan\n",
      "Iteration 149200 [14920000%]: Loss = nan\n",
      "Iteration 149300 [14930000%]: Loss = nan\n",
      "Iteration 149400 [14940000%]: Loss = nan\n",
      "Iteration 149500 [14950000%]: Loss = nan\n",
      "Iteration 149600 [14960000%]: Loss = nan\n",
      "Iteration 149700 [14970000%]: Loss = nan\n",
      "Iteration 149800 [14980000%]: Loss = nan\n",
      "Iteration 149900 [14990000%]: Loss = nan\n",
      "Iteration 150000 [15000000%]: Loss = nan\n",
      "Iteration 150100 [15010000%]: Loss = nan\n",
      "Iteration 150200 [15020000%]: Loss = nan\n",
      "Iteration 150300 [15030000%]: Loss = nan\n",
      "Iteration 150400 [15040000%]: Loss = nan\n",
      "Iteration 150500 [15050000%]: Loss = nan\n",
      "Iteration 150600 [15060000%]: Loss = nan\n",
      "Iteration 150700 [15070000%]: Loss = nan\n",
      "Iteration 150800 [15080000%]: Loss = nan\n",
      "Iteration 150900 [15090000%]: Loss = nan\n",
      "Iteration 151000 [15100000%]: Loss = nan\n",
      "Iteration 151100 [15110000%]: Loss = nan\n",
      "Iteration 151200 [15120000%]: Loss = nan\n",
      "Iteration 151300 [15130000%]: Loss = nan\n",
      "Iteration 151400 [15140000%]: Loss = nan\n",
      "Iteration 151500 [15150000%]: Loss = nan\n",
      "Iteration 151600 [15160000%]: Loss = nan\n",
      "Iteration 151700 [15170000%]: Loss = nan\n",
      "Iteration 151800 [15180000%]: Loss = nan\n",
      "Iteration 151900 [15190000%]: Loss = nan\n",
      "Iteration 152000 [15200000%]: Loss = nan\n",
      "Iteration 152100 [15210000%]: Loss = nan\n",
      "Iteration 152200 [15220000%]: Loss = nan\n",
      "Iteration 152300 [15230000%]: Loss = nan\n",
      "Iteration 152400 [15240000%]: Loss = nan\n",
      "Iteration 152500 [15250000%]: Loss = nan\n",
      "Iteration 152600 [15260000%]: Loss = nan\n",
      "Iteration 152700 [15270000%]: Loss = nan\n",
      "Iteration 152800 [15280000%]: Loss = nan\n",
      "Iteration 152900 [15290000%]: Loss = nan\n",
      "Iteration 153000 [15300000%]: Loss = nan\n",
      "Iteration 153100 [15310000%]: Loss = nan\n",
      "Iteration 153200 [15320000%]: Loss = nan\n",
      "Iteration 153300 [15330000%]: Loss = nan\n",
      "Iteration 153400 [15340000%]: Loss = nan\n",
      "Iteration 153500 [15350000%]: Loss = nan\n",
      "Iteration 153600 [15360000%]: Loss = nan\n",
      "Iteration 153700 [15370000%]: Loss = nan\n",
      "Iteration 153800 [15380000%]: Loss = nan\n",
      "Iteration 153900 [15390000%]: Loss = nan\n",
      "Iteration 154000 [15400000%]: Loss = nan\n",
      "Iteration 154100 [15410000%]: Loss = nan\n",
      "Iteration 154200 [15420000%]: Loss = nan\n",
      "Iteration 154300 [15430000%]: Loss = nan\n",
      "Iteration 154400 [15440000%]: Loss = nan\n",
      "Iteration 154500 [15450000%]: Loss = nan\n",
      "Iteration 154600 [15460000%]: Loss = nan\n",
      "Iteration 154700 [15470000%]: Loss = nan\n",
      "Iteration 154800 [15480000%]: Loss = nan\n",
      "Iteration 154900 [15490000%]: Loss = nan\n",
      "Iteration 155000 [15500000%]: Loss = nan\n",
      "Iteration 155100 [15510000%]: Loss = nan\n",
      "Iteration 155200 [15520000%]: Loss = nan\n",
      "Iteration 155300 [15530000%]: Loss = nan\n",
      "Iteration 155400 [15540000%]: Loss = nan\n",
      "Iteration 155500 [15550000%]: Loss = nan\n",
      "Iteration 155600 [15560000%]: Loss = nan\n",
      "Iteration 155700 [15570000%]: Loss = nan\n",
      "Iteration 155800 [15580000%]: Loss = nan\n",
      "Iteration 155900 [15590000%]: Loss = nan\n",
      "Iteration 156000 [15600000%]: Loss = nan\n",
      "Iteration 156100 [15610000%]: Loss = nan\n",
      "Iteration 156200 [15620000%]: Loss = nan\n",
      "Iteration 156300 [15630000%]: Loss = nan\n",
      "Iteration 156400 [15640000%]: Loss = nan\n",
      "Iteration 156500 [15650000%]: Loss = nan\n",
      "Iteration 156600 [15660000%]: Loss = nan\n",
      "Iteration 156700 [15670000%]: Loss = nan\n",
      "Iteration 156800 [15680000%]: Loss = nan\n",
      "Iteration 156900 [15690000%]: Loss = nan\n",
      "Iteration 157000 [15700000%]: Loss = nan\n",
      "Iteration 157100 [15710000%]: Loss = nan\n",
      "Iteration 157200 [15720000%]: Loss = nan\n",
      "Iteration 157300 [15730000%]: Loss = nan\n",
      "Iteration 157400 [15740000%]: Loss = nan\n",
      "Iteration 157500 [15750000%]: Loss = nan\n",
      "Iteration 157600 [15760000%]: Loss = nan\n",
      "Iteration 157700 [15770000%]: Loss = nan\n",
      "Iteration 157800 [15780000%]: Loss = nan\n",
      "Iteration 157900 [15790000%]: Loss = nan\n",
      "Iteration 158000 [15800000%]: Loss = nan\n",
      "Iteration 158100 [15810000%]: Loss = nan\n",
      "Iteration 158200 [15820000%]: Loss = nan\n",
      "Iteration 158300 [15830000%]: Loss = nan\n",
      "Iteration 158400 [15840000%]: Loss = nan\n",
      "Iteration 158500 [15850000%]: Loss = nan\n",
      "Iteration 158600 [15860000%]: Loss = nan\n",
      "Iteration 158700 [15870000%]: Loss = nan\n",
      "Iteration 158800 [15880000%]: Loss = nan\n",
      "Iteration 158900 [15890000%]: Loss = nan\n",
      "Iteration 159000 [15900000%]: Loss = nan\n",
      "Iteration 159100 [15910000%]: Loss = nan\n",
      "Iteration 159200 [15920000%]: Loss = nan\n",
      "Iteration 159300 [15930000%]: Loss = nan\n",
      "Iteration 159400 [15940000%]: Loss = nan\n",
      "Iteration 159500 [15950000%]: Loss = nan\n",
      "Iteration 159600 [15960000%]: Loss = nan\n",
      "Iteration 159700 [15970000%]: Loss = nan\n",
      "Iteration 159800 [15980000%]: Loss = nan\n",
      "Iteration 159900 [15990000%]: Loss = nan\n",
      "Iteration 160000 [16000000%]: Loss = nan\n",
      "Iteration 160100 [16010000%]: Loss = nan\n",
      "Iteration 160200 [16020000%]: Loss = nan\n",
      "Iteration 160300 [16030000%]: Loss = nan\n",
      "Iteration 160400 [16040000%]: Loss = nan\n",
      "Iteration 160500 [16050000%]: Loss = nan\n",
      "Iteration 160600 [16060000%]: Loss = nan\n",
      "Iteration 160700 [16070000%]: Loss = nan\n",
      "Iteration 160800 [16080000%]: Loss = nan\n",
      "Iteration 160900 [16090000%]: Loss = nan\n",
      "Iteration 161000 [16100000%]: Loss = nan\n",
      "Iteration 161100 [16110000%]: Loss = nan\n",
      "Iteration 161200 [16120000%]: Loss = nan\n",
      "Iteration 161300 [16130000%]: Loss = nan\n",
      "Iteration 161400 [16140000%]: Loss = nan\n",
      "Iteration 161500 [16150000%]: Loss = nan\n",
      "Iteration 161600 [16160000%]: Loss = nan\n",
      "Iteration 161700 [16170000%]: Loss = nan\n",
      "Iteration 161800 [16180000%]: Loss = nan\n",
      "Iteration 161900 [16190000%]: Loss = nan\n",
      "Iteration 162000 [16200000%]: Loss = nan\n",
      "Iteration 162100 [16210000%]: Loss = nan\n",
      "Iteration 162200 [16220000%]: Loss = nan\n",
      "Iteration 162300 [16230000%]: Loss = nan\n",
      "Iteration 162400 [16240000%]: Loss = nan\n",
      "Iteration 162500 [16250000%]: Loss = nan\n",
      "Iteration 162600 [16260000%]: Loss = nan\n",
      "Iteration 162700 [16270000%]: Loss = nan\n",
      "Iteration 162800 [16280000%]: Loss = nan\n",
      "Iteration 162900 [16290000%]: Loss = nan\n",
      "Iteration 163000 [16300000%]: Loss = nan\n",
      "Iteration 163100 [16310000%]: Loss = nan\n",
      "Iteration 163200 [16320000%]: Loss = nan\n",
      "Iteration 163300 [16330000%]: Loss = nan\n",
      "Iteration 163400 [16340000%]: Loss = nan\n",
      "Iteration 163500 [16350000%]: Loss = nan\n",
      "Iteration 163600 [16360000%]: Loss = nan\n",
      "Iteration 163700 [16370000%]: Loss = nan\n",
      "Iteration 163800 [16380000%]: Loss = nan\n",
      "Iteration 163900 [16390000%]: Loss = nan\n",
      "Iteration 164000 [16400000%]: Loss = nan\n",
      "Iteration 164100 [16410000%]: Loss = nan\n",
      "Iteration 164200 [16420000%]: Loss = nan\n",
      "Iteration 164300 [16430000%]: Loss = nan\n",
      "Iteration 164400 [16440000%]: Loss = nan\n",
      "Iteration 164500 [16450000%]: Loss = nan\n",
      "Iteration 164600 [16460000%]: Loss = nan\n",
      "Iteration 164700 [16470000%]: Loss = nan\n",
      "Iteration 164800 [16480000%]: Loss = nan\n",
      "Iteration 164900 [16490000%]: Loss = nan\n",
      "Iteration 165000 [16500000%]: Loss = nan\n",
      "Iteration 165100 [16510000%]: Loss = nan\n",
      "Iteration 165200 [16520000%]: Loss = nan\n",
      "Iteration 165300 [16530000%]: Loss = nan\n",
      "Iteration 165400 [16540000%]: Loss = nan\n",
      "Iteration 165500 [16550000%]: Loss = nan\n",
      "Iteration 165600 [16560000%]: Loss = nan\n",
      "Iteration 165700 [16570000%]: Loss = nan\n",
      "Iteration 165800 [16580000%]: Loss = nan\n",
      "Iteration 165900 [16590000%]: Loss = nan\n",
      "Iteration 166000 [16600000%]: Loss = nan\n",
      "Iteration 166100 [16610000%]: Loss = nan\n",
      "Iteration 166200 [16620000%]: Loss = nan\n",
      "Iteration 166300 [16630000%]: Loss = nan\n",
      "Iteration 166400 [16640000%]: Loss = nan\n",
      "Iteration 166500 [16650000%]: Loss = nan\n",
      "Iteration 166600 [16660000%]: Loss = nan\n",
      "Iteration 166700 [16670000%]: Loss = nan\n",
      "Iteration 166800 [16680000%]: Loss = nan\n",
      "Iteration 166900 [16690000%]: Loss = nan\n",
      "Iteration 167000 [16700000%]: Loss = nan\n",
      "Iteration 167100 [16710000%]: Loss = nan\n",
      "Iteration 167200 [16720000%]: Loss = nan\n",
      "Iteration 167300 [16730000%]: Loss = nan\n",
      "Iteration 167400 [16740000%]: Loss = nan\n",
      "Iteration 167500 [16750000%]: Loss = nan\n",
      "Iteration 167600 [16760000%]: Loss = nan\n",
      "Iteration 167700 [16770000%]: Loss = nan\n",
      "Iteration 167800 [16780000%]: Loss = nan\n",
      "Iteration 167900 [16790000%]: Loss = nan\n",
      "Iteration 168000 [16800000%]: Loss = nan\n",
      "Iteration 168100 [16810000%]: Loss = nan\n",
      "Iteration 168200 [16820000%]: Loss = nan\n",
      "Iteration 168300 [16830000%]: Loss = nan\n",
      "Iteration 168400 [16840000%]: Loss = nan\n",
      "Iteration 168500 [16850000%]: Loss = nan\n",
      "Iteration 168600 [16860000%]: Loss = nan\n",
      "Iteration 168700 [16870000%]: Loss = nan\n",
      "Iteration 168800 [16880000%]: Loss = nan\n",
      "Iteration 168900 [16890000%]: Loss = nan\n",
      "Iteration 169000 [16900000%]: Loss = nan\n",
      "Iteration 169100 [16910000%]: Loss = nan\n",
      "Iteration 169200 [16920000%]: Loss = nan\n",
      "Iteration 169300 [16930000%]: Loss = nan\n",
      "Iteration 169400 [16940000%]: Loss = nan\n",
      "Iteration 169500 [16950000%]: Loss = nan\n",
      "Iteration 169600 [16960000%]: Loss = nan\n",
      "Iteration 169700 [16970000%]: Loss = nan\n",
      "Iteration 169800 [16980000%]: Loss = nan\n",
      "Iteration 169900 [16990000%]: Loss = nan\n",
      "Iteration 170000 [17000000%]: Loss = nan\n",
      "Iteration 170100 [17010000%]: Loss = nan\n",
      "Iteration 170200 [17020000%]: Loss = nan\n",
      "Iteration 170300 [17030000%]: Loss = nan\n",
      "Iteration 170400 [17040000%]: Loss = nan\n",
      "Iteration 170500 [17050000%]: Loss = nan\n",
      "Iteration 170600 [17060000%]: Loss = nan\n",
      "Iteration 170700 [17070000%]: Loss = nan\n",
      "Iteration 170800 [17080000%]: Loss = nan\n",
      "Iteration 170900 [17090000%]: Loss = nan\n",
      "Iteration 171000 [17100000%]: Loss = nan\n",
      "Iteration 171100 [17110000%]: Loss = nan\n",
      "Iteration 171200 [17120000%]: Loss = nan\n",
      "Iteration 171300 [17130000%]: Loss = nan\n",
      "Iteration 171400 [17140000%]: Loss = nan\n",
      "Iteration 171500 [17150000%]: Loss = nan\n",
      "Iteration 171600 [17160000%]: Loss = nan\n",
      "Iteration 171700 [17170000%]: Loss = nan\n",
      "Iteration 171800 [17180000%]: Loss = nan\n",
      "Iteration 171900 [17190000%]: Loss = nan\n",
      "Iteration 172000 [17200000%]: Loss = nan\n",
      "Iteration 172100 [17210000%]: Loss = nan\n",
      "Iteration 172200 [17220000%]: Loss = nan\n",
      "Iteration 172300 [17230000%]: Loss = nan\n",
      "Iteration 172400 [17240000%]: Loss = nan\n",
      "Iteration 172500 [17250000%]: Loss = nan\n",
      "Iteration 172600 [17260000%]: Loss = nan\n",
      "Iteration 172700 [17270000%]: Loss = nan\n",
      "Iteration 172800 [17280000%]: Loss = nan\n",
      "Iteration 172900 [17290000%]: Loss = nan\n",
      "Iteration 173000 [17300000%]: Loss = nan\n",
      "Iteration 173100 [17310000%]: Loss = nan\n",
      "Iteration 173200 [17320000%]: Loss = nan\n",
      "Iteration 173300 [17330000%]: Loss = nan\n",
      "Iteration 173400 [17340000%]: Loss = nan\n",
      "Iteration 173500 [17350000%]: Loss = nan\n",
      "Iteration 173600 [17360000%]: Loss = nan\n",
      "Iteration 173700 [17370000%]: Loss = nan\n",
      "Iteration 173800 [17380000%]: Loss = nan\n",
      "Iteration 173900 [17390000%]: Loss = nan\n",
      "Iteration 174000 [17400000%]: Loss = nan\n",
      "Iteration 174100 [17410000%]: Loss = nan\n",
      "Iteration 174200 [17420000%]: Loss = nan\n",
      "Iteration 174300 [17430000%]: Loss = nan\n",
      "Iteration 174400 [17440000%]: Loss = nan\n",
      "Iteration 174500 [17450000%]: Loss = nan\n",
      "Iteration 174600 [17460000%]: Loss = nan\n",
      "Iteration 174700 [17470000%]: Loss = nan\n",
      "Iteration 174800 [17480000%]: Loss = nan\n",
      "Iteration 174900 [17490000%]: Loss = nan\n",
      "Iteration 175000 [17500000%]: Loss = nan\n",
      "Iteration 175100 [17510000%]: Loss = nan\n",
      "Iteration 175200 [17520000%]: Loss = nan\n",
      "Iteration 175300 [17530000%]: Loss = nan\n",
      "Iteration 175400 [17540000%]: Loss = nan\n",
      "Iteration 175500 [17550000%]: Loss = nan\n",
      "Iteration 175600 [17560000%]: Loss = nan\n",
      "Iteration 175700 [17570000%]: Loss = nan\n",
      "Iteration 175800 [17580000%]: Loss = nan\n",
      "Iteration 175900 [17590000%]: Loss = nan\n",
      "Iteration 176000 [17600000%]: Loss = nan\n",
      "Iteration 176100 [17610000%]: Loss = nan\n",
      "Iteration 176200 [17620000%]: Loss = nan\n",
      "Iteration 176300 [17630000%]: Loss = nan\n",
      "Iteration 176400 [17640000%]: Loss = nan\n",
      "Iteration 176500 [17650000%]: Loss = nan\n",
      "Iteration 176600 [17660000%]: Loss = nan\n",
      "Iteration 176700 [17670000%]: Loss = nan\n",
      "Iteration 176800 [17680000%]: Loss = nan\n",
      "Iteration 176900 [17690000%]: Loss = nan\n",
      "Iteration 177000 [17700000%]: Loss = nan\n",
      "Iteration 177100 [17710000%]: Loss = nan\n",
      "Iteration 177200 [17720000%]: Loss = nan\n",
      "Iteration 177300 [17730000%]: Loss = nan\n",
      "Iteration 177400 [17740000%]: Loss = nan\n",
      "Iteration 177500 [17750000%]: Loss = nan\n",
      "Iteration 177600 [17760000%]: Loss = nan\n",
      "Iteration 177700 [17770000%]: Loss = nan\n",
      "Iteration 177800 [17780000%]: Loss = nan\n",
      "Iteration 177900 [17790000%]: Loss = nan\n",
      "Iteration 178000 [17800000%]: Loss = nan\n",
      "Iteration 178100 [17810000%]: Loss = nan\n",
      "Iteration 178200 [17820000%]: Loss = nan\n",
      "Iteration 178300 [17830000%]: Loss = nan\n",
      "Iteration 178400 [17840000%]: Loss = nan\n",
      "Iteration 178500 [17850000%]: Loss = nan\n",
      "Iteration 178600 [17860000%]: Loss = nan\n",
      "Iteration 178700 [17870000%]: Loss = nan\n",
      "Iteration 178800 [17880000%]: Loss = nan\n",
      "Iteration 178900 [17890000%]: Loss = nan\n",
      "Iteration 179000 [17900000%]: Loss = nan\n",
      "Iteration 179100 [17910000%]: Loss = nan\n",
      "Iteration 179200 [17920000%]: Loss = nan\n",
      "Iteration 179300 [17930000%]: Loss = nan\n",
      "Iteration 179400 [17940000%]: Loss = nan\n",
      "Iteration 179500 [17950000%]: Loss = nan\n",
      "Iteration 179600 [17960000%]: Loss = nan\n",
      "Iteration 179700 [17970000%]: Loss = nan\n",
      "Iteration 179800 [17980000%]: Loss = nan\n",
      "Iteration 179900 [17990000%]: Loss = nan\n",
      "Iteration 180000 [18000000%]: Loss = nan\n",
      "Iteration 180100 [18010000%]: Loss = nan\n",
      "Iteration 180200 [18020000%]: Loss = nan\n",
      "Iteration 180300 [18030000%]: Loss = nan\n",
      "Iteration 180400 [18040000%]: Loss = nan\n",
      "Iteration 180500 [18050000%]: Loss = nan\n",
      "Iteration 180600 [18060000%]: Loss = nan\n",
      "Iteration 180700 [18070000%]: Loss = nan\n",
      "Iteration 180800 [18080000%]: Loss = nan\n",
      "Iteration 180900 [18090000%]: Loss = nan\n",
      "Iteration 181000 [18100000%]: Loss = nan\n",
      "Iteration 181100 [18110000%]: Loss = nan\n",
      "Iteration 181200 [18120000%]: Loss = nan\n",
      "Iteration 181300 [18130000%]: Loss = nan\n",
      "Iteration 181400 [18140000%]: Loss = nan\n",
      "Iteration 181500 [18150000%]: Loss = nan\n",
      "Iteration 181600 [18160000%]: Loss = nan\n",
      "Iteration 181700 [18170000%]: Loss = nan\n",
      "Iteration 181800 [18180000%]: Loss = nan\n",
      "Iteration 181900 [18190000%]: Loss = nan\n",
      "Iteration 182000 [18200000%]: Loss = nan\n",
      "Iteration 182100 [18210000%]: Loss = nan\n",
      "Iteration 182200 [18220000%]: Loss = nan\n",
      "Iteration 182300 [18230000%]: Loss = nan\n",
      "Iteration 182400 [18240000%]: Loss = nan\n",
      "Iteration 182500 [18250000%]: Loss = nan\n",
      "Iteration 182600 [18260000%]: Loss = nan\n",
      "Iteration 182700 [18270000%]: Loss = nan\n",
      "Iteration 182800 [18280000%]: Loss = nan\n",
      "Iteration 182900 [18290000%]: Loss = nan\n",
      "Iteration 183000 [18300000%]: Loss = nan\n",
      "Iteration 183100 [18310000%]: Loss = nan\n",
      "Iteration 183200 [18320000%]: Loss = nan\n",
      "Iteration 183300 [18330000%]: Loss = nan\n",
      "Iteration 183400 [18340000%]: Loss = nan\n",
      "Iteration 183500 [18350000%]: Loss = nan\n",
      "Iteration 183600 [18360000%]: Loss = nan\n",
      "Iteration 183700 [18370000%]: Loss = nan\n",
      "Iteration 183800 [18380000%]: Loss = nan\n",
      "Iteration 183900 [18390000%]: Loss = nan\n",
      "Iteration 184000 [18400000%]: Loss = nan\n",
      "Iteration 184100 [18410000%]: Loss = nan\n",
      "Iteration 184200 [18420000%]: Loss = nan\n",
      "Iteration 184300 [18430000%]: Loss = nan\n",
      "Iteration 184400 [18440000%]: Loss = nan\n",
      "Iteration 184500 [18450000%]: Loss = nan\n",
      "Iteration 184600 [18460000%]: Loss = nan\n",
      "Iteration 184700 [18470000%]: Loss = nan\n",
      "Iteration 184800 [18480000%]: Loss = nan\n",
      "Iteration 184900 [18490000%]: Loss = nan\n",
      "Iteration 185000 [18500000%]: Loss = nan\n",
      "Iteration 185100 [18510000%]: Loss = nan\n",
      "Iteration 185200 [18520000%]: Loss = nan\n",
      "Iteration 185300 [18530000%]: Loss = nan\n",
      "Iteration 185400 [18540000%]: Loss = nan\n",
      "Iteration 185500 [18550000%]: Loss = nan\n",
      "Iteration 185600 [18560000%]: Loss = nan\n",
      "Iteration 185700 [18570000%]: Loss = nan\n",
      "Iteration 185800 [18580000%]: Loss = nan\n",
      "Iteration 185900 [18590000%]: Loss = nan\n",
      "Iteration 186000 [18600000%]: Loss = nan\n",
      "Iteration 186100 [18610000%]: Loss = nan\n",
      "Iteration 186200 [18620000%]: Loss = nan\n",
      "Iteration 186300 [18630000%]: Loss = nan\n",
      "Iteration 186400 [18640000%]: Loss = nan\n",
      "Iteration 186500 [18650000%]: Loss = nan\n",
      "Iteration 186600 [18660000%]: Loss = nan\n",
      "Iteration 186700 [18670000%]: Loss = nan\n",
      "Iteration 186800 [18680000%]: Loss = nan\n",
      "Iteration 186900 [18690000%]: Loss = nan\n",
      "Iteration 187000 [18700000%]: Loss = nan\n",
      "Iteration 187100 [18710000%]: Loss = nan\n",
      "Iteration 187200 [18720000%]: Loss = nan\n",
      "Iteration 187300 [18730000%]: Loss = nan\n",
      "Iteration 187400 [18740000%]: Loss = nan\n",
      "Iteration 187500 [18750000%]: Loss = nan\n",
      "Iteration 187600 [18760000%]: Loss = nan\n",
      "Iteration 187700 [18770000%]: Loss = nan\n",
      "Iteration 187800 [18780000%]: Loss = nan\n",
      "Iteration 187900 [18790000%]: Loss = nan\n",
      "Iteration 188000 [18800000%]: Loss = nan\n",
      "Iteration 188100 [18810000%]: Loss = nan\n",
      "Iteration 188200 [18820000%]: Loss = nan\n",
      "Iteration 188300 [18830000%]: Loss = nan\n",
      "Iteration 188400 [18840000%]: Loss = nan\n",
      "Iteration 188500 [18850000%]: Loss = nan\n",
      "Iteration 188600 [18860000%]: Loss = nan\n",
      "Iteration 188700 [18870000%]: Loss = nan\n",
      "Iteration 188800 [18880000%]: Loss = nan\n",
      "Iteration 188900 [18890000%]: Loss = nan\n",
      "Iteration 189000 [18900000%]: Loss = nan\n",
      "Iteration 189100 [18910000%]: Loss = nan\n",
      "Iteration 189200 [18920000%]: Loss = nan\n",
      "Iteration 189300 [18930000%]: Loss = nan\n",
      "Iteration 189400 [18940000%]: Loss = nan\n",
      "Iteration 189500 [18950000%]: Loss = nan\n",
      "Iteration 189600 [18960000%]: Loss = nan\n",
      "Iteration 189700 [18970000%]: Loss = nan\n",
      "Iteration 189800 [18980000%]: Loss = nan\n",
      "Iteration 189900 [18990000%]: Loss = nan\n",
      "Iteration 190000 [19000000%]: Loss = nan\n",
      "Iteration 190100 [19010000%]: Loss = nan\n",
      "Iteration 190200 [19020000%]: Loss = nan\n",
      "Iteration 190300 [19030000%]: Loss = nan\n",
      "Iteration 190400 [19040000%]: Loss = nan\n",
      "Iteration 190500 [19050000%]: Loss = nan\n",
      "Iteration 190600 [19060000%]: Loss = nan\n",
      "Iteration 190700 [19070000%]: Loss = nan\n",
      "Iteration 190800 [19080000%]: Loss = nan\n",
      "Iteration 190900 [19090000%]: Loss = nan\n",
      "Iteration 191000 [19100000%]: Loss = nan\n",
      "Iteration 191100 [19110000%]: Loss = nan\n",
      "Iteration 191200 [19120000%]: Loss = nan\n",
      "Iteration 191300 [19130000%]: Loss = nan\n",
      "Iteration 191400 [19140000%]: Loss = nan\n",
      "Iteration 191500 [19150000%]: Loss = nan\n",
      "Iteration 191600 [19160000%]: Loss = nan\n",
      "Iteration 191700 [19170000%]: Loss = nan\n",
      "Iteration 191800 [19180000%]: Loss = nan\n",
      "Iteration 191900 [19190000%]: Loss = nan\n",
      "Iteration 192000 [19200000%]: Loss = nan\n",
      "Iteration 192100 [19210000%]: Loss = nan\n",
      "Iteration 192200 [19220000%]: Loss = nan\n",
      "Iteration 192300 [19230000%]: Loss = nan\n",
      "Iteration 192400 [19240000%]: Loss = nan\n",
      "Iteration 192500 [19250000%]: Loss = nan\n",
      "Iteration 192600 [19260000%]: Loss = nan\n",
      "Iteration 192700 [19270000%]: Loss = nan\n",
      "Iteration 192800 [19280000%]: Loss = nan\n",
      "Iteration 192900 [19290000%]: Loss = nan\n",
      "Iteration 193000 [19300000%]: Loss = nan\n",
      "Iteration 193100 [19310000%]: Loss = nan\n",
      "Iteration 193200 [19320000%]: Loss = nan\n",
      "Iteration 193300 [19330000%]: Loss = nan\n",
      "Iteration 193400 [19340000%]: Loss = nan\n",
      "Iteration 193500 [19350000%]: Loss = nan\n",
      "Iteration 193600 [19360000%]: Loss = nan\n",
      "Iteration 193700 [19370000%]: Loss = nan\n",
      "Iteration 193800 [19380000%]: Loss = nan\n",
      "Iteration 193900 [19390000%]: Loss = nan\n",
      "Iteration 194000 [19400000%]: Loss = nan\n",
      "Iteration 194100 [19410000%]: Loss = nan\n",
      "Iteration 194200 [19420000%]: Loss = nan\n",
      "Iteration 194300 [19430000%]: Loss = nan\n",
      "Iteration 194400 [19440000%]: Loss = nan\n",
      "Iteration 194500 [19450000%]: Loss = nan\n",
      "Iteration 194600 [19460000%]: Loss = nan\n",
      "Iteration 194700 [19470000%]: Loss = nan\n",
      "Iteration 194800 [19480000%]: Loss = nan\n",
      "Iteration 194900 [19490000%]: Loss = nan\n",
      "Iteration 195000 [19500000%]: Loss = nan\n",
      "Iteration 195100 [19510000%]: Loss = nan\n",
      "Iteration 195200 [19520000%]: Loss = nan\n",
      "Iteration 195300 [19530000%]: Loss = nan\n",
      "Iteration 195400 [19540000%]: Loss = nan\n",
      "Iteration 195500 [19550000%]: Loss = nan\n",
      "Iteration 195600 [19560000%]: Loss = nan\n",
      "Iteration 195700 [19570000%]: Loss = nan\n",
      "Iteration 195800 [19580000%]: Loss = nan\n",
      "Iteration 195900 [19590000%]: Loss = nan\n",
      "Iteration 196000 [19600000%]: Loss = nan\n",
      "Iteration 196100 [19610000%]: Loss = nan\n",
      "Iteration 196200 [19620000%]: Loss = nan\n",
      "Iteration 196300 [19630000%]: Loss = nan\n",
      "Iteration 196400 [19640000%]: Loss = nan\n",
      "Iteration 196500 [19650000%]: Loss = nan\n",
      "Iteration 196600 [19660000%]: Loss = nan\n",
      "Iteration 196700 [19670000%]: Loss = nan\n",
      "Iteration 196800 [19680000%]: Loss = nan\n",
      "Iteration 196900 [19690000%]: Loss = nan\n",
      "Iteration 197000 [19700000%]: Loss = nan\n",
      "Iteration 197100 [19710000%]: Loss = nan\n",
      "Iteration 197200 [19720000%]: Loss = nan\n",
      "Iteration 197300 [19730000%]: Loss = nan\n",
      "Iteration 197400 [19740000%]: Loss = nan\n",
      "Iteration 197500 [19750000%]: Loss = nan\n",
      "Iteration 197600 [19760000%]: Loss = nan\n",
      "Iteration 197700 [19770000%]: Loss = nan\n",
      "Iteration 197800 [19780000%]: Loss = nan\n",
      "Iteration 197900 [19790000%]: Loss = nan\n",
      "Iteration 198000 [19800000%]: Loss = nan\n",
      "Iteration 198100 [19810000%]: Loss = nan\n",
      "Iteration 198200 [19820000%]: Loss = nan\n",
      "Iteration 198300 [19830000%]: Loss = nan\n",
      "Iteration 198400 [19840000%]: Loss = nan\n",
      "Iteration 198500 [19850000%]: Loss = nan\n",
      "Iteration 198600 [19860000%]: Loss = nan\n",
      "Iteration 198700 [19870000%]: Loss = nan\n",
      "Iteration 198800 [19880000%]: Loss = nan\n",
      "Iteration 198900 [19890000%]: Loss = nan\n",
      "Iteration 199000 [19900000%]: Loss = nan\n",
      "Iteration 199100 [19910000%]: Loss = nan\n",
      "Iteration 199200 [19920000%]: Loss = nan\n",
      "Iteration 199300 [19930000%]: Loss = nan\n",
      "Iteration 199400 [19940000%]: Loss = nan\n",
      "Iteration 199500 [19950000%]: Loss = nan\n",
      "Iteration 199600 [19960000%]: Loss = nan\n",
      "Iteration 199700 [19970000%]: Loss = nan\n",
      "Iteration 199800 [19980000%]: Loss = nan\n",
      "Iteration 199900 [19990000%]: Loss = nan\n",
      "Iteration 200000 [20000000%]: Loss = nan\n",
      "Iteration 200100 [20010000%]: Loss = nan\n",
      "Iteration 200200 [20020000%]: Loss = nan\n",
      "Iteration 200300 [20030000%]: Loss = nan\n",
      "Iteration 200400 [20040000%]: Loss = nan\n",
      "Iteration 200500 [20050000%]: Loss = nan\n",
      "Iteration 200600 [20060000%]: Loss = nan\n",
      "Iteration 200700 [20070000%]: Loss = nan\n",
      "Iteration 200800 [20080000%]: Loss = nan\n",
      "Iteration 200900 [20090000%]: Loss = nan\n",
      "Iteration 201000 [20100000%]: Loss = nan\n",
      "Iteration 201100 [20110000%]: Loss = nan\n",
      "Iteration 201200 [20120000%]: Loss = nan\n",
      "Iteration 201300 [20130000%]: Loss = nan\n",
      "Iteration 201400 [20140000%]: Loss = nan\n",
      "Iteration 201500 [20150000%]: Loss = nan\n",
      "Iteration 201600 [20160000%]: Loss = nan\n",
      "Iteration 201700 [20170000%]: Loss = nan\n",
      "Iteration 201800 [20180000%]: Loss = nan\n",
      "Iteration 201900 [20190000%]: Loss = nan\n",
      "Iteration 202000 [20200000%]: Loss = nan\n",
      "Iteration 202100 [20210000%]: Loss = nan\n",
      "Iteration 202200 [20220000%]: Loss = nan\n",
      "Iteration 202300 [20230000%]: Loss = nan\n",
      "Iteration 202400 [20240000%]: Loss = nan\n",
      "Iteration 202500 [20250000%]: Loss = nan\n",
      "Iteration 202600 [20260000%]: Loss = nan\n",
      "Iteration 202700 [20270000%]: Loss = nan\n",
      "Iteration 202800 [20280000%]: Loss = nan\n",
      "Iteration 202900 [20290000%]: Loss = nan\n",
      "Iteration 203000 [20300000%]: Loss = nan\n",
      "Iteration 203100 [20310000%]: Loss = nan\n",
      "Iteration 203200 [20320000%]: Loss = nan\n",
      "Iteration 203300 [20330000%]: Loss = nan\n",
      "Iteration 203400 [20340000%]: Loss = nan\n",
      "Iteration 203500 [20350000%]: Loss = nan\n",
      "Iteration 203600 [20360000%]: Loss = nan\n",
      "Iteration 203700 [20370000%]: Loss = nan\n",
      "Iteration 203800 [20380000%]: Loss = nan\n",
      "Iteration 203900 [20390000%]: Loss = nan\n",
      "Iteration 204000 [20400000%]: Loss = nan\n",
      "Iteration 204100 [20410000%]: Loss = nan\n",
      "Iteration 204200 [20420000%]: Loss = nan\n",
      "Iteration 204300 [20430000%]: Loss = nan\n",
      "Iteration 204400 [20440000%]: Loss = nan\n",
      "Iteration 204500 [20450000%]: Loss = nan\n",
      "Iteration 204600 [20460000%]: Loss = nan\n",
      "Iteration 204700 [20470000%]: Loss = nan\n",
      "Iteration 204800 [20480000%]: Loss = nan\n",
      "Iteration 204900 [20490000%]: Loss = nan\n",
      "Iteration 205000 [20500000%]: Loss = nan\n",
      "Iteration 205100 [20510000%]: Loss = nan\n",
      "Iteration 205200 [20520000%]: Loss = nan\n",
      "Iteration 205300 [20530000%]: Loss = nan\n",
      "Iteration 205400 [20540000%]: Loss = nan\n",
      "Iteration 205500 [20550000%]: Loss = nan\n",
      "Iteration 205600 [20560000%]: Loss = nan\n",
      "Iteration 205700 [20570000%]: Loss = nan\n",
      "Iteration 205800 [20580000%]: Loss = nan\n",
      "Iteration 205900 [20590000%]: Loss = nan\n",
      "Iteration 206000 [20600000%]: Loss = nan\n",
      "Iteration 206100 [20610000%]: Loss = nan\n",
      "Iteration 206200 [20620000%]: Loss = nan\n",
      "Iteration 206300 [20630000%]: Loss = nan\n",
      "Iteration 206400 [20640000%]: Loss = nan\n",
      "Iteration 206500 [20650000%]: Loss = nan\n",
      "Iteration 206600 [20660000%]: Loss = nan\n",
      "Iteration 206700 [20670000%]: Loss = nan\n",
      "Iteration 206800 [20680000%]: Loss = nan\n",
      "Iteration 206900 [20690000%]: Loss = nan\n",
      "Iteration 207000 [20700000%]: Loss = nan\n",
      "Iteration 207100 [20710000%]: Loss = nan\n",
      "Iteration 207200 [20720000%]: Loss = nan\n",
      "Iteration 207300 [20730000%]: Loss = nan\n",
      "Iteration 207400 [20740000%]: Loss = nan\n",
      "Iteration 207500 [20750000%]: Loss = nan\n",
      "Iteration 207600 [20760000%]: Loss = nan\n",
      "Iteration 207700 [20770000%]: Loss = nan\n",
      "Iteration 207800 [20780000%]: Loss = nan\n",
      "Iteration 207900 [20790000%]: Loss = nan\n",
      "Iteration 208000 [20800000%]: Loss = nan\n",
      "Iteration 208100 [20810000%]: Loss = nan\n",
      "Iteration 208200 [20820000%]: Loss = nan\n",
      "Iteration 208300 [20830000%]: Loss = nan\n",
      "Iteration 208400 [20840000%]: Loss = nan\n",
      "Iteration 208500 [20850000%]: Loss = nan\n",
      "Iteration 208600 [20860000%]: Loss = nan\n",
      "Iteration 208700 [20870000%]: Loss = nan\n",
      "Iteration 208800 [20880000%]: Loss = nan\n",
      "Iteration 208900 [20890000%]: Loss = nan\n",
      "Iteration 209000 [20900000%]: Loss = nan\n",
      "Iteration 209100 [20910000%]: Loss = nan\n",
      "Iteration 209200 [20920000%]: Loss = nan\n",
      "Iteration 209300 [20930000%]: Loss = nan\n",
      "Iteration 209400 [20940000%]: Loss = nan\n",
      "Iteration 209500 [20950000%]: Loss = nan\n",
      "Iteration 209600 [20960000%]: Loss = nan\n",
      "Iteration 209700 [20970000%]: Loss = nan\n",
      "Iteration 209800 [20980000%]: Loss = nan\n",
      "Iteration 209900 [20990000%]: Loss = nan\n",
      "Iteration 210000 [21000000%]: Loss = nan\n",
      "Iteration 210100 [21010000%]: Loss = nan\n",
      "Iteration 210200 [21020000%]: Loss = nan\n",
      "Iteration 210300 [21030000%]: Loss = nan\n",
      "Iteration 210400 [21040000%]: Loss = nan\n",
      "Iteration 210500 [21050000%]: Loss = nan\n",
      "Iteration 210600 [21060000%]: Loss = nan\n",
      "Iteration 210700 [21070000%]: Loss = nan\n",
      "Iteration 210800 [21080000%]: Loss = nan\n",
      "Iteration 210900 [21090000%]: Loss = nan\n",
      "Iteration 211000 [21100000%]: Loss = nan\n",
      "Iteration 211100 [21110000%]: Loss = nan\n",
      "Iteration 211200 [21120000%]: Loss = nan\n",
      "Iteration 211300 [21130000%]: Loss = nan\n",
      "Iteration 211400 [21140000%]: Loss = nan\n",
      "Iteration 211500 [21150000%]: Loss = nan\n",
      "Iteration 211600 [21160000%]: Loss = nan\n",
      "Iteration 211700 [21170000%]: Loss = nan\n",
      "Iteration 211800 [21180000%]: Loss = nan\n",
      "Iteration 211900 [21190000%]: Loss = nan\n",
      "Iteration 212000 [21200000%]: Loss = nan\n",
      "Iteration 212100 [21210000%]: Loss = nan\n",
      "Iteration 212200 [21220000%]: Loss = nan\n",
      "Iteration 212300 [21230000%]: Loss = nan\n",
      "Iteration 212400 [21240000%]: Loss = nan\n",
      "Iteration 212500 [21250000%]: Loss = nan\n",
      "Iteration 212600 [21260000%]: Loss = nan\n",
      "Iteration 212700 [21270000%]: Loss = nan\n",
      "Iteration 212800 [21280000%]: Loss = nan\n",
      "Iteration 212900 [21290000%]: Loss = nan\n",
      "Iteration 213000 [21300000%]: Loss = nan\n",
      "Iteration 213100 [21310000%]: Loss = nan\n",
      "Iteration 213200 [21320000%]: Loss = nan\n",
      "Iteration 213300 [21330000%]: Loss = nan\n",
      "Iteration 213400 [21340000%]: Loss = nan\n",
      "Iteration 213500 [21350000%]: Loss = nan\n",
      "Iteration 213600 [21360000%]: Loss = nan\n",
      "Iteration 213700 [21370000%]: Loss = nan\n",
      "Iteration 213800 [21380000%]: Loss = nan\n",
      "Iteration 213900 [21390000%]: Loss = nan\n",
      "Iteration 214000 [21400000%]: Loss = nan\n",
      "Iteration 214100 [21410000%]: Loss = nan\n",
      "Iteration 214200 [21420000%]: Loss = nan\n",
      "Iteration 214300 [21430000%]: Loss = nan\n",
      "Iteration 214400 [21440000%]: Loss = nan\n",
      "Iteration 214500 [21450000%]: Loss = nan\n",
      "Iteration 214600 [21460000%]: Loss = nan\n",
      "Iteration 214700 [21470000%]: Loss = nan\n",
      "Iteration 214800 [21480000%]: Loss = nan\n",
      "Iteration 214900 [21490000%]: Loss = nan\n",
      "Iteration 215000 [21500000%]: Loss = nan\n",
      "Iteration 215100 [21510000%]: Loss = nan\n",
      "Iteration 215200 [21520000%]: Loss = nan\n",
      "Iteration 215300 [21530000%]: Loss = nan\n",
      "Iteration 215400 [21540000%]: Loss = nan\n",
      "Iteration 215500 [21550000%]: Loss = nan\n",
      "Iteration 215600 [21560000%]: Loss = nan\n",
      "Iteration 215700 [21570000%]: Loss = nan\n",
      "Iteration 215800 [21580000%]: Loss = nan\n",
      "Iteration 215900 [21590000%]: Loss = nan\n",
      "Iteration 216000 [21600000%]: Loss = nan\n",
      "Iteration 216100 [21610000%]: Loss = nan\n",
      "Iteration 216200 [21620000%]: Loss = nan\n",
      "Iteration 216300 [21630000%]: Loss = nan\n",
      "Iteration 216400 [21640000%]: Loss = nan\n",
      "Iteration 216500 [21650000%]: Loss = nan\n",
      "Iteration 216600 [21660000%]: Loss = nan\n",
      "Iteration 216700 [21670000%]: Loss = nan\n",
      "Iteration 216800 [21680000%]: Loss = nan\n",
      "Iteration 216900 [21690000%]: Loss = nan\n",
      "Iteration 217000 [21700000%]: Loss = nan\n",
      "Iteration 217100 [21710000%]: Loss = nan\n",
      "Iteration 217200 [21720000%]: Loss = nan\n",
      "Iteration 217300 [21730000%]: Loss = nan\n",
      "Iteration 217400 [21740000%]: Loss = nan\n",
      "Iteration 217500 [21750000%]: Loss = nan\n",
      "Iteration 217600 [21760000%]: Loss = nan\n",
      "Iteration 217700 [21770000%]: Loss = nan\n",
      "Iteration 217800 [21780000%]: Loss = nan\n",
      "Iteration 217900 [21790000%]: Loss = nan\n",
      "Iteration 218000 [21800000%]: Loss = nan\n",
      "Iteration 218100 [21810000%]: Loss = nan\n",
      "Iteration 218200 [21820000%]: Loss = nan\n",
      "Iteration 218300 [21830000%]: Loss = nan\n",
      "Iteration 218400 [21840000%]: Loss = nan\n",
      "Iteration 218500 [21850000%]: Loss = nan\n",
      "Iteration 218600 [21860000%]: Loss = nan\n",
      "Iteration 218700 [21870000%]: Loss = nan\n",
      "Iteration 218800 [21880000%]: Loss = nan\n",
      "Iteration 218900 [21890000%]: Loss = nan\n",
      "Iteration 219000 [21900000%]: Loss = nan\n",
      "Iteration 219100 [21910000%]: Loss = nan\n",
      "Iteration 219200 [21920000%]: Loss = nan\n",
      "Iteration 219300 [21930000%]: Loss = nan\n",
      "Iteration 219400 [21940000%]: Loss = nan\n",
      "Iteration 219500 [21950000%]: Loss = nan\n",
      "Iteration 219600 [21960000%]: Loss = nan\n",
      "Iteration 219700 [21970000%]: Loss = nan\n",
      "Iteration 219800 [21980000%]: Loss = nan\n",
      "Iteration 219900 [21990000%]: Loss = nan\n",
      "Iteration 220000 [22000000%]: Loss = nan\n",
      "Iteration 220100 [22010000%]: Loss = nan\n",
      "Iteration 220200 [22020000%]: Loss = nan\n",
      "Iteration 220300 [22030000%]: Loss = nan\n",
      "Iteration 220400 [22040000%]: Loss = nan\n",
      "Iteration 220500 [22050000%]: Loss = nan\n",
      "Iteration 220600 [22060000%]: Loss = nan\n",
      "Iteration 220700 [22070000%]: Loss = nan\n",
      "Iteration 220800 [22080000%]: Loss = nan\n",
      "Iteration 220900 [22090000%]: Loss = nan\n",
      "Iteration 221000 [22100000%]: Loss = nan\n",
      "Iteration 221100 [22110000%]: Loss = nan\n",
      "Iteration 221200 [22120000%]: Loss = nan\n",
      "Iteration 221300 [22130000%]: Loss = nan\n",
      "Iteration 221400 [22140000%]: Loss = nan\n",
      "Iteration 221500 [22150000%]: Loss = nan\n",
      "Iteration 221600 [22160000%]: Loss = nan\n",
      "Iteration 221700 [22170000%]: Loss = nan\n",
      "Iteration 221800 [22180000%]: Loss = nan\n",
      "Iteration 221900 [22190000%]: Loss = nan\n",
      "Iteration 222000 [22200000%]: Loss = nan\n",
      "Iteration 222100 [22210000%]: Loss = nan\n",
      "Iteration 222200 [22220000%]: Loss = nan\n",
      "Iteration 222300 [22230000%]: Loss = nan\n",
      "Iteration 222400 [22240000%]: Loss = nan\n",
      "Iteration 222500 [22250000%]: Loss = nan\n",
      "Iteration 222600 [22260000%]: Loss = nan\n",
      "Iteration 222700 [22270000%]: Loss = nan\n",
      "Iteration 222800 [22280000%]: Loss = nan\n",
      "Iteration 222900 [22290000%]: Loss = nan\n",
      "Iteration 223000 [22300000%]: Loss = nan\n",
      "Iteration 223100 [22310000%]: Loss = nan\n",
      "Iteration 223200 [22320000%]: Loss = nan\n",
      "Iteration 223300 [22330000%]: Loss = nan\n",
      "Iteration 223400 [22340000%]: Loss = nan\n",
      "Iteration 223500 [22350000%]: Loss = nan\n",
      "Iteration 223600 [22360000%]: Loss = nan\n",
      "Iteration 223700 [22370000%]: Loss = nan\n",
      "Iteration 223800 [22380000%]: Loss = nan\n",
      "Iteration 223900 [22390000%]: Loss = nan\n",
      "Iteration 224000 [22400000%]: Loss = nan\n",
      "Iteration 224100 [22410000%]: Loss = nan\n",
      "Iteration 224200 [22420000%]: Loss = nan\n",
      "Iteration 224300 [22430000%]: Loss = nan\n",
      "Iteration 224400 [22440000%]: Loss = nan\n",
      "Iteration 224500 [22450000%]: Loss = nan\n",
      "Iteration 224600 [22460000%]: Loss = nan\n",
      "Iteration 224700 [22470000%]: Loss = nan\n",
      "Iteration 224800 [22480000%]: Loss = nan\n",
      "Iteration 224900 [22490000%]: Loss = nan\n",
      "Iteration 225000 [22500000%]: Loss = nan\n",
      "Iteration 225100 [22510000%]: Loss = nan\n",
      "Iteration 225200 [22520000%]: Loss = nan\n",
      "Iteration 225300 [22530000%]: Loss = nan\n",
      "Iteration 225400 [22540000%]: Loss = nan\n",
      "Iteration 225500 [22550000%]: Loss = nan\n",
      "Iteration 225600 [22560000%]: Loss = nan\n",
      "Iteration 225700 [22570000%]: Loss = nan\n",
      "Iteration 225800 [22580000%]: Loss = nan\n",
      "Iteration 225900 [22590000%]: Loss = nan\n",
      "Iteration 226000 [22600000%]: Loss = nan\n",
      "Iteration 226100 [22610000%]: Loss = nan\n",
      "Iteration 226200 [22620000%]: Loss = nan\n",
      "Iteration 226300 [22630000%]: Loss = nan\n",
      "Iteration 226400 [22640000%]: Loss = nan\n",
      "Iteration 226500 [22650000%]: Loss = nan\n",
      "Iteration 226600 [22660000%]: Loss = nan\n",
      "Iteration 226700 [22670000%]: Loss = nan\n",
      "Iteration 226800 [22680000%]: Loss = nan\n",
      "Iteration 226900 [22690000%]: Loss = nan\n",
      "Iteration 227000 [22700000%]: Loss = nan\n",
      "Iteration 227100 [22710000%]: Loss = nan\n",
      "Iteration 227200 [22720000%]: Loss = nan\n",
      "Iteration 227300 [22730000%]: Loss = nan\n",
      "Iteration 227400 [22740000%]: Loss = nan\n",
      "Iteration 227500 [22750000%]: Loss = nan\n",
      "Iteration 227600 [22760000%]: Loss = nan\n",
      "Iteration 227700 [22770000%]: Loss = nan\n",
      "Iteration 227800 [22780000%]: Loss = nan\n",
      "Iteration 227900 [22790000%]: Loss = nan\n",
      "Iteration 228000 [22800000%]: Loss = nan\n",
      "Iteration 228100 [22810000%]: Loss = nan\n",
      "Iteration 228200 [22820000%]: Loss = nan\n",
      "Iteration 228300 [22830000%]: Loss = nan\n",
      "Iteration 228400 [22840000%]: Loss = nan\n",
      "Iteration 228500 [22850000%]: Loss = nan\n",
      "Iteration 228600 [22860000%]: Loss = nan\n",
      "Iteration 228700 [22870000%]: Loss = nan\n",
      "Iteration 228800 [22880000%]: Loss = nan\n",
      "Iteration 228900 [22890000%]: Loss = nan\n",
      "Iteration 229000 [22900000%]: Loss = nan\n",
      "Iteration 229100 [22910000%]: Loss = nan\n",
      "Iteration 229200 [22920000%]: Loss = nan\n",
      "Iteration 229300 [22930000%]: Loss = nan\n",
      "Iteration 229400 [22940000%]: Loss = nan\n",
      "Iteration 229500 [22950000%]: Loss = nan\n",
      "Iteration 229600 [22960000%]: Loss = nan\n",
      "Iteration 229700 [22970000%]: Loss = nan\n",
      "Iteration 229800 [22980000%]: Loss = nan\n",
      "Iteration 229900 [22990000%]: Loss = nan\n",
      "Iteration 230000 [23000000%]: Loss = nan\n",
      "Iteration 230100 [23010000%]: Loss = nan\n",
      "Iteration 230200 [23020000%]: Loss = nan\n",
      "Iteration 230300 [23030000%]: Loss = nan\n",
      "Iteration 230400 [23040000%]: Loss = nan\n",
      "Iteration 230500 [23050000%]: Loss = nan\n",
      "Iteration 230600 [23060000%]: Loss = nan\n",
      "Iteration 230700 [23070000%]: Loss = nan\n",
      "Iteration 230800 [23080000%]: Loss = nan\n",
      "Iteration 230900 [23090000%]: Loss = nan\n",
      "Iteration 231000 [23100000%]: Loss = nan\n",
      "Iteration 231100 [23110000%]: Loss = nan\n",
      "Iteration 231200 [23120000%]: Loss = nan\n",
      "Iteration 231300 [23130000%]: Loss = nan\n",
      "Iteration 231400 [23140000%]: Loss = nan\n",
      "Iteration 231500 [23150000%]: Loss = nan\n",
      "Iteration 231600 [23160000%]: Loss = nan\n",
      "Iteration 231700 [23170000%]: Loss = nan\n",
      "Iteration 231800 [23180000%]: Loss = nan\n",
      "Iteration 231900 [23190000%]: Loss = nan\n",
      "Iteration 232000 [23200000%]: Loss = nan\n",
      "Iteration 232100 [23210000%]: Loss = nan\n",
      "Iteration 232200 [23220000%]: Loss = nan\n",
      "Iteration 232300 [23230000%]: Loss = nan\n",
      "Iteration 232400 [23240000%]: Loss = nan\n",
      "Iteration 232500 [23250000%]: Loss = nan\n",
      "Iteration 232600 [23260000%]: Loss = nan\n",
      "Iteration 232700 [23270000%]: Loss = nan\n",
      "Iteration 232800 [23280000%]: Loss = nan\n",
      "Iteration 232900 [23290000%]: Loss = nan\n",
      "Iteration 233000 [23300000%]: Loss = nan\n",
      "Iteration 233100 [23310000%]: Loss = nan\n",
      "Iteration 233200 [23320000%]: Loss = nan\n",
      "Iteration 233300 [23330000%]: Loss = nan\n",
      "Iteration 233400 [23340000%]: Loss = nan\n",
      "Iteration 233500 [23350000%]: Loss = nan\n",
      "Iteration 233600 [23360000%]: Loss = nan\n",
      "Iteration 233700 [23370000%]: Loss = nan\n",
      "Iteration 233800 [23380000%]: Loss = nan\n",
      "Iteration 233900 [23390000%]: Loss = nan\n",
      "Iteration 234000 [23400000%]: Loss = nan\n",
      "Iteration 234100 [23410000%]: Loss = nan\n",
      "Iteration 234200 [23420000%]: Loss = nan\n",
      "Iteration 234300 [23430000%]: Loss = nan\n",
      "Iteration 234400 [23440000%]: Loss = nan\n",
      "Iteration 234500 [23450000%]: Loss = nan\n",
      "Iteration 234600 [23460000%]: Loss = nan\n",
      "Iteration 234700 [23470000%]: Loss = nan\n",
      "Iteration 234800 [23480000%]: Loss = nan\n",
      "Iteration 234900 [23490000%]: Loss = nan\n",
      "Iteration 235000 [23500000%]: Loss = nan\n",
      "Iteration 235100 [23510000%]: Loss = nan\n",
      "Iteration 235200 [23520000%]: Loss = nan\n",
      "Iteration 235300 [23530000%]: Loss = nan\n",
      "Iteration 235400 [23540000%]: Loss = nan\n",
      "Iteration 235500 [23550000%]: Loss = nan\n",
      "Iteration 235600 [23560000%]: Loss = nan\n",
      "Iteration 235700 [23570000%]: Loss = nan\n",
      "Iteration 235800 [23580000%]: Loss = nan\n",
      "Iteration 235900 [23590000%]: Loss = nan\n",
      "Iteration 236000 [23600000%]: Loss = nan\n",
      "Iteration 236100 [23610000%]: Loss = nan\n",
      "Iteration 236200 [23620000%]: Loss = nan\n",
      "Iteration 236300 [23630000%]: Loss = nan\n",
      "Iteration 236400 [23640000%]: Loss = nan\n",
      "Iteration 236500 [23650000%]: Loss = nan\n",
      "Iteration 236600 [23660000%]: Loss = nan\n",
      "Iteration 236700 [23670000%]: Loss = nan\n",
      "Iteration 236800 [23680000%]: Loss = nan\n",
      "Iteration 236900 [23690000%]: Loss = nan\n",
      "Iteration 237000 [23700000%]: Loss = nan\n",
      "Iteration 237100 [23710000%]: Loss = nan\n",
      "Iteration 237200 [23720000%]: Loss = nan\n",
      "Iteration 237300 [23730000%]: Loss = nan\n",
      "Iteration 237400 [23740000%]: Loss = nan\n",
      "Iteration 237500 [23750000%]: Loss = nan\n",
      "Iteration 237600 [23760000%]: Loss = nan\n",
      "Iteration 237700 [23770000%]: Loss = nan\n",
      "Iteration 237800 [23780000%]: Loss = nan\n",
      "Iteration 237900 [23790000%]: Loss = nan\n",
      "Iteration 238000 [23800000%]: Loss = nan\n",
      "Iteration 238100 [23810000%]: Loss = nan\n",
      "Iteration 238200 [23820000%]: Loss = nan\n",
      "Iteration 238300 [23830000%]: Loss = nan\n",
      "Iteration 238400 [23840000%]: Loss = nan\n",
      "Iteration 238500 [23850000%]: Loss = nan\n",
      "Iteration 238600 [23860000%]: Loss = nan\n",
      "Iteration 238700 [23870000%]: Loss = nan\n",
      "Iteration 238800 [23880000%]: Loss = nan\n",
      "Iteration 238900 [23890000%]: Loss = nan\n",
      "Iteration 239000 [23900000%]: Loss = nan\n",
      "Iteration 239100 [23910000%]: Loss = nan\n",
      "Iteration 239200 [23920000%]: Loss = nan\n",
      "Iteration 239300 [23930000%]: Loss = nan\n",
      "Iteration 239400 [23940000%]: Loss = nan\n",
      "Iteration 239500 [23950000%]: Loss = nan\n",
      "Iteration 239600 [23960000%]: Loss = nan\n",
      "Iteration 239700 [23970000%]: Loss = nan\n",
      "Iteration 239800 [23980000%]: Loss = nan\n",
      "Iteration 239900 [23990000%]: Loss = nan\n",
      "Iteration 240000 [24000000%]: Loss = nan\n",
      "Iteration 240100 [24010000%]: Loss = nan\n",
      "Iteration 240200 [24020000%]: Loss = nan\n",
      "Iteration 240300 [24030000%]: Loss = nan\n",
      "Iteration 240400 [24040000%]: Loss = nan\n",
      "Iteration 240500 [24050000%]: Loss = nan\n",
      "Iteration 240600 [24060000%]: Loss = nan\n",
      "Iteration 240700 [24070000%]: Loss = nan\n",
      "Iteration 240800 [24080000%]: Loss = nan\n",
      "Iteration 240900 [24090000%]: Loss = nan\n",
      "Iteration 241000 [24100000%]: Loss = nan\n",
      "Iteration 241100 [24110000%]: Loss = nan\n",
      "Iteration 241200 [24120000%]: Loss = nan\n",
      "Iteration 241300 [24130000%]: Loss = nan\n",
      "Iteration 241400 [24140000%]: Loss = nan\n",
      "Iteration 241500 [24150000%]: Loss = nan\n",
      "Iteration 241600 [24160000%]: Loss = nan\n",
      "Iteration 241700 [24170000%]: Loss = nan\n",
      "Iteration 241800 [24180000%]: Loss = nan\n",
      "Iteration 241900 [24190000%]: Loss = nan\n",
      "Iteration 242000 [24200000%]: Loss = nan\n",
      "Iteration 242100 [24210000%]: Loss = nan\n",
      "Iteration 242200 [24220000%]: Loss = nan\n",
      "Iteration 242300 [24230000%]: Loss = nan\n",
      "Iteration 242400 [24240000%]: Loss = nan\n",
      "Iteration 242500 [24250000%]: Loss = nan\n",
      "Iteration 242600 [24260000%]: Loss = nan\n",
      "Iteration 242700 [24270000%]: Loss = nan\n",
      "Iteration 242800 [24280000%]: Loss = nan\n",
      "Iteration 242900 [24290000%]: Loss = nan\n",
      "Iteration 243000 [24300000%]: Loss = nan\n",
      "Iteration 243100 [24310000%]: Loss = nan\n",
      "Iteration 243200 [24320000%]: Loss = nan\n",
      "Iteration 243300 [24330000%]: Loss = nan\n",
      "Iteration 243400 [24340000%]: Loss = nan\n",
      "Iteration 243500 [24350000%]: Loss = nan\n",
      "Iteration 243600 [24360000%]: Loss = nan\n",
      "Iteration 243700 [24370000%]: Loss = nan\n",
      "Iteration 243800 [24380000%]: Loss = nan\n",
      "Iteration 243900 [24390000%]: Loss = nan\n",
      "Iteration 244000 [24400000%]: Loss = nan\n",
      "Iteration 244100 [24410000%]: Loss = nan\n",
      "Iteration 244200 [24420000%]: Loss = nan\n",
      "Iteration 244300 [24430000%]: Loss = nan\n",
      "Iteration 244400 [24440000%]: Loss = nan\n",
      "Iteration 244500 [24450000%]: Loss = nan\n",
      "Iteration 244600 [24460000%]: Loss = nan\n",
      "Iteration 244700 [24470000%]: Loss = nan\n",
      "Iteration 244800 [24480000%]: Loss = nan\n",
      "Iteration 244900 [24490000%]: Loss = nan\n",
      "Iteration 245000 [24500000%]: Loss = nan\n",
      "Iteration 245100 [24510000%]: Loss = nan\n",
      "Iteration 245200 [24520000%]: Loss = nan\n",
      "Iteration 245300 [24530000%]: Loss = nan\n",
      "Iteration 245400 [24540000%]: Loss = nan\n",
      "Iteration 245500 [24550000%]: Loss = nan\n",
      "Iteration 245600 [24560000%]: Loss = nan\n",
      "Iteration 245700 [24570000%]: Loss = nan\n",
      "Iteration 245800 [24580000%]: Loss = nan\n",
      "Iteration 245900 [24590000%]: Loss = nan\n",
      "Iteration 246000 [24600000%]: Loss = nan\n",
      "Iteration 246100 [24610000%]: Loss = nan\n",
      "Iteration 246200 [24620000%]: Loss = nan\n",
      "Iteration 246300 [24630000%]: Loss = nan\n",
      "Iteration 246400 [24640000%]: Loss = nan\n",
      "Iteration 246500 [24650000%]: Loss = nan\n",
      "Iteration 246600 [24660000%]: Loss = nan\n",
      "Iteration 246700 [24670000%]: Loss = nan\n",
      "Iteration 246800 [24680000%]: Loss = nan\n",
      "Iteration 246900 [24690000%]: Loss = nan\n",
      "Iteration 247000 [24700000%]: Loss = nan\n",
      "Iteration 247100 [24710000%]: Loss = nan\n",
      "Iteration 247200 [24720000%]: Loss = nan\n",
      "Iteration 247300 [24730000%]: Loss = nan\n",
      "Iteration 247400 [24740000%]: Loss = nan\n",
      "Iteration 247500 [24750000%]: Loss = nan\n",
      "Iteration 247600 [24760000%]: Loss = nan\n",
      "Iteration 247700 [24770000%]: Loss = nan\n",
      "Iteration 247800 [24780000%]: Loss = nan\n",
      "Iteration 247900 [24790000%]: Loss = nan\n",
      "Iteration 248000 [24800000%]: Loss = nan\n",
      "Iteration 248100 [24810000%]: Loss = nan\n",
      "Iteration 248200 [24820000%]: Loss = nan\n",
      "Iteration 248300 [24830000%]: Loss = nan\n",
      "Iteration 248400 [24840000%]: Loss = nan\n",
      "Iteration 248500 [24850000%]: Loss = nan\n",
      "Iteration 248600 [24860000%]: Loss = nan\n",
      "Iteration 248700 [24870000%]: Loss = nan\n",
      "Iteration 248800 [24880000%]: Loss = nan\n",
      "Iteration 248900 [24890000%]: Loss = nan\n",
      "Iteration 249000 [24900000%]: Loss = nan\n",
      "Iteration 249100 [24910000%]: Loss = nan\n",
      "Iteration 249200 [24920000%]: Loss = nan\n",
      "Iteration 249300 [24930000%]: Loss = nan\n",
      "Iteration 249400 [24940000%]: Loss = nan\n",
      "Iteration 249500 [24950000%]: Loss = nan\n",
      "Iteration 249600 [24960000%]: Loss = nan\n",
      "Iteration 249700 [24970000%]: Loss = nan\n",
      "Iteration 249800 [24980000%]: Loss = nan\n",
      "Iteration 249900 [24990000%]: Loss = nan\n",
      "Iteration 250000 [25000000%]: Loss = nan\n",
      "Iteration 250100 [25010000%]: Loss = nan\n",
      "Iteration 250200 [25020000%]: Loss = nan\n",
      "Iteration 250300 [25030000%]: Loss = nan\n",
      "Iteration 250400 [25040000%]: Loss = nan\n",
      "Iteration 250500 [25050000%]: Loss = nan\n",
      "Iteration 250600 [25060000%]: Loss = nan\n",
      "Iteration 250700 [25070000%]: Loss = nan\n",
      "Iteration 250800 [25080000%]: Loss = nan\n",
      "Iteration 250900 [25090000%]: Loss = nan\n",
      "Iteration 251000 [25100000%]: Loss = nan\n",
      "Iteration 251100 [25110000%]: Loss = nan\n",
      "Iteration 251200 [25120000%]: Loss = nan\n",
      "Iteration 251300 [25130000%]: Loss = nan\n",
      "Iteration 251400 [25140000%]: Loss = nan\n",
      "Iteration 251500 [25150000%]: Loss = nan\n",
      "Iteration 251600 [25160000%]: Loss = nan\n",
      "Iteration 251700 [25170000%]: Loss = nan\n",
      "Iteration 251800 [25180000%]: Loss = nan\n",
      "Iteration 251900 [25190000%]: Loss = nan\n",
      "Iteration 252000 [25200000%]: Loss = nan\n",
      "Iteration 252100 [25210000%]: Loss = nan\n",
      "Iteration 252200 [25220000%]: Loss = nan\n",
      "Iteration 252300 [25230000%]: Loss = nan\n",
      "Iteration 252400 [25240000%]: Loss = nan\n",
      "Iteration 252500 [25250000%]: Loss = nan\n",
      "Iteration 252600 [25260000%]: Loss = nan\n",
      "Iteration 252700 [25270000%]: Loss = nan\n",
      "Iteration 252800 [25280000%]: Loss = nan\n",
      "Iteration 252900 [25290000%]: Loss = nan\n",
      "Iteration 253000 [25300000%]: Loss = nan\n",
      "Iteration 253100 [25310000%]: Loss = nan\n",
      "Iteration 253200 [25320000%]: Loss = nan\n",
      "Iteration 253300 [25330000%]: Loss = nan\n",
      "Iteration 253400 [25340000%]: Loss = nan\n",
      "Iteration 253500 [25350000%]: Loss = nan\n",
      "Iteration 253600 [25360000%]: Loss = nan\n",
      "Iteration 253700 [25370000%]: Loss = nan\n",
      "Iteration 253800 [25380000%]: Loss = nan\n",
      "Iteration 253900 [25390000%]: Loss = nan\n",
      "Iteration 254000 [25400000%]: Loss = nan\n",
      "Iteration 254100 [25410000%]: Loss = nan\n",
      "Iteration 254200 [25420000%]: Loss = nan\n",
      "Iteration 254300 [25430000%]: Loss = nan\n",
      "Iteration 254400 [25440000%]: Loss = nan\n",
      "Iteration 254500 [25450000%]: Loss = nan\n",
      "Iteration 254600 [25460000%]: Loss = nan\n",
      "Iteration 254700 [25470000%]: Loss = nan\n",
      "Iteration 254800 [25480000%]: Loss = nan\n",
      "Iteration 254900 [25490000%]: Loss = nan\n",
      "Iteration 255000 [25500000%]: Loss = nan\n",
      "Iteration 255100 [25510000%]: Loss = nan\n",
      "Iteration 255200 [25520000%]: Loss = nan\n",
      "Iteration 255300 [25530000%]: Loss = nan\n",
      "Iteration 255400 [25540000%]: Loss = nan\n",
      "Iteration 255500 [25550000%]: Loss = nan\n",
      "Iteration 255600 [25560000%]: Loss = nan\n",
      "Iteration 255700 [25570000%]: Loss = nan\n",
      "Iteration 255800 [25580000%]: Loss = nan\n",
      "Iteration 255900 [25590000%]: Loss = nan\n",
      "Iteration 256000 [25600000%]: Loss = nan\n",
      "Iteration 256100 [25610000%]: Loss = nan\n",
      "Iteration 256200 [25620000%]: Loss = nan\n",
      "Iteration 256300 [25630000%]: Loss = nan\n",
      "Iteration 256400 [25640000%]: Loss = nan\n",
      "Iteration 256500 [25650000%]: Loss = nan\n",
      "Iteration 256600 [25660000%]: Loss = nan\n",
      "Iteration 256700 [25670000%]: Loss = nan\n",
      "Iteration 256800 [25680000%]: Loss = nan\n",
      "Iteration 256900 [25690000%]: Loss = nan\n",
      "Iteration 257000 [25700000%]: Loss = nan\n",
      "Iteration 257100 [25710000%]: Loss = nan\n",
      "Iteration 257200 [25720000%]: Loss = nan\n",
      "Iteration 257300 [25730000%]: Loss = nan\n",
      "Iteration 257400 [25740000%]: Loss = nan\n",
      "Iteration 257500 [25750000%]: Loss = nan\n",
      "Iteration 257600 [25760000%]: Loss = nan\n",
      "Iteration 257700 [25770000%]: Loss = nan\n",
      "Iteration 257800 [25780000%]: Loss = nan\n",
      "Iteration 257900 [25790000%]: Loss = nan\n",
      "Iteration 258000 [25800000%]: Loss = nan\n",
      "Iteration 258100 [25810000%]: Loss = nan\n",
      "Iteration 258200 [25820000%]: Loss = nan\n",
      "Iteration 258300 [25830000%]: Loss = nan\n",
      "Iteration 258400 [25840000%]: Loss = nan\n",
      "Iteration 258500 [25850000%]: Loss = nan\n",
      "Iteration 258600 [25860000%]: Loss = nan\n",
      "Iteration 258700 [25870000%]: Loss = nan\n",
      "Iteration 258800 [25880000%]: Loss = nan\n",
      "Iteration 258900 [25890000%]: Loss = nan\n",
      "Iteration 259000 [25900000%]: Loss = nan\n",
      "Iteration 259100 [25910000%]: Loss = nan\n",
      "Iteration 259200 [25920000%]: Loss = nan\n",
      "Iteration 259300 [25930000%]: Loss = nan\n",
      "Iteration 259400 [25940000%]: Loss = nan\n",
      "Iteration 259500 [25950000%]: Loss = nan\n",
      "Iteration 259600 [25960000%]: Loss = nan\n",
      "Iteration 259700 [25970000%]: Loss = nan\n",
      "Iteration 259800 [25980000%]: Loss = nan\n",
      "Iteration 259900 [25990000%]: Loss = nan\n",
      "Iteration 260000 [26000000%]: Loss = nan\n",
      "Iteration 260100 [26010000%]: Loss = nan\n",
      "Iteration 260200 [26020000%]: Loss = nan\n",
      "Iteration 260300 [26030000%]: Loss = nan\n",
      "Iteration 260400 [26040000%]: Loss = nan\n",
      "Iteration 260500 [26050000%]: Loss = nan\n",
      "Iteration 260600 [26060000%]: Loss = nan\n",
      "Iteration 260700 [26070000%]: Loss = nan\n",
      "Iteration 260800 [26080000%]: Loss = nan\n",
      "Iteration 260900 [26090000%]: Loss = nan\n",
      "Iteration 261000 [26100000%]: Loss = nan\n",
      "Iteration 261100 [26110000%]: Loss = nan\n",
      "Iteration 261200 [26120000%]: Loss = nan\n",
      "Iteration 261300 [26130000%]: Loss = nan\n",
      "Iteration 261400 [26140000%]: Loss = nan\n",
      "Iteration 261500 [26150000%]: Loss = nan\n",
      "Iteration 261600 [26160000%]: Loss = nan\n",
      "Iteration 261700 [26170000%]: Loss = nan\n",
      "Iteration 261800 [26180000%]: Loss = nan\n",
      "Iteration 261900 [26190000%]: Loss = nan\n",
      "Iteration 262000 [26200000%]: Loss = nan\n",
      "Iteration 262100 [26210000%]: Loss = nan\n",
      "Iteration 262200 [26220000%]: Loss = nan\n",
      "Iteration 262300 [26230000%]: Loss = nan\n",
      "Iteration 262400 [26240000%]: Loss = nan\n",
      "Iteration 262500 [26250000%]: Loss = nan\n",
      "Iteration 262600 [26260000%]: Loss = nan\n",
      "Iteration 262700 [26270000%]: Loss = nan\n",
      "Iteration 262800 [26280000%]: Loss = nan\n",
      "Iteration 262900 [26290000%]: Loss = nan\n",
      "Iteration 263000 [26300000%]: Loss = nan\n",
      "Iteration 263100 [26310000%]: Loss = nan\n",
      "Iteration 263200 [26320000%]: Loss = nan\n",
      "Iteration 263300 [26330000%]: Loss = nan\n",
      "Iteration 263400 [26340000%]: Loss = nan\n",
      "Iteration 263500 [26350000%]: Loss = nan\n",
      "Iteration 263600 [26360000%]: Loss = nan\n",
      "Iteration 263700 [26370000%]: Loss = nan\n",
      "Iteration 263800 [26380000%]: Loss = nan\n",
      "Iteration 263900 [26390000%]: Loss = nan\n",
      "Iteration 264000 [26400000%]: Loss = nan\n",
      "Iteration 264100 [26410000%]: Loss = nan\n",
      "Iteration 264200 [26420000%]: Loss = nan\n",
      "Iteration 264300 [26430000%]: Loss = nan\n",
      "Iteration 264400 [26440000%]: Loss = nan\n",
      "Iteration 264500 [26450000%]: Loss = nan\n",
      "Iteration 264600 [26460000%]: Loss = nan\n",
      "Iteration 264700 [26470000%]: Loss = nan\n",
      "Iteration 264800 [26480000%]: Loss = nan\n",
      "Iteration 264900 [26490000%]: Loss = nan\n",
      "Iteration 265000 [26500000%]: Loss = nan\n",
      "Iteration 265100 [26510000%]: Loss = nan\n",
      "Iteration 265200 [26520000%]: Loss = nan\n",
      "Iteration 265300 [26530000%]: Loss = nan\n",
      "Iteration 265400 [26540000%]: Loss = nan\n",
      "Iteration 265500 [26550000%]: Loss = nan\n",
      "Iteration 265600 [26560000%]: Loss = nan\n",
      "Iteration 265700 [26570000%]: Loss = nan\n",
      "Iteration 265800 [26580000%]: Loss = nan\n",
      "Iteration 265900 [26590000%]: Loss = nan\n",
      "Iteration 266000 [26600000%]: Loss = nan\n",
      "Iteration 266100 [26610000%]: Loss = nan\n",
      "Iteration 266200 [26620000%]: Loss = nan\n",
      "Iteration 266300 [26630000%]: Loss = nan\n",
      "Iteration 266400 [26640000%]: Loss = nan\n",
      "Iteration 266500 [26650000%]: Loss = nan\n",
      "Iteration 266600 [26660000%]: Loss = nan\n",
      "Iteration 266700 [26670000%]: Loss = nan\n",
      "Iteration 266800 [26680000%]: Loss = nan\n",
      "Iteration 266900 [26690000%]: Loss = nan\n",
      "Iteration 267000 [26700000%]: Loss = nan\n",
      "Iteration 267100 [26710000%]: Loss = nan\n",
      "Iteration 267200 [26720000%]: Loss = nan\n",
      "Iteration 267300 [26730000%]: Loss = nan\n",
      "Iteration 267400 [26740000%]: Loss = nan\n",
      "Iteration 267500 [26750000%]: Loss = nan\n",
      "Iteration 267600 [26760000%]: Loss = nan\n",
      "Iteration 267700 [26770000%]: Loss = nan\n",
      "Iteration 267800 [26780000%]: Loss = nan\n",
      "Iteration 267900 [26790000%]: Loss = nan\n",
      "Iteration 268000 [26800000%]: Loss = nan\n",
      "Iteration 268100 [26810000%]: Loss = nan\n",
      "Iteration 268200 [26820000%]: Loss = nan\n",
      "Iteration 268300 [26830000%]: Loss = nan\n",
      "Iteration 268400 [26840000%]: Loss = nan\n",
      "Iteration 268500 [26850000%]: Loss = nan\n",
      "Iteration 268600 [26860000%]: Loss = nan\n",
      "Iteration 268700 [26870000%]: Loss = nan\n",
      "Iteration 268800 [26880000%]: Loss = nan\n",
      "Iteration 268900 [26890000%]: Loss = nan\n",
      "Iteration 269000 [26900000%]: Loss = nan\n",
      "Iteration 269100 [26910000%]: Loss = nan\n",
      "Iteration 269200 [26920000%]: Loss = nan\n",
      "Iteration 269300 [26930000%]: Loss = nan\n",
      "Iteration 269400 [26940000%]: Loss = nan\n",
      "Iteration 269500 [26950000%]: Loss = nan\n",
      "Iteration 269600 [26960000%]: Loss = nan\n",
      "Iteration 269700 [26970000%]: Loss = nan\n",
      "Iteration 269800 [26980000%]: Loss = nan\n",
      "Iteration 269900 [26990000%]: Loss = nan\n",
      "Iteration 270000 [27000000%]: Loss = nan\n"
     ]
    }
   ],
   "source": [
    "n_iter = 200000\n",
    "for _ in range(n_iter):\n",
    "    for inf in [inference_lam, inference_coeffs, inference_latents]:\n",
    "        for _ in range(1):  # make multiple steps along each set of coords\n",
    "            info_dict = inf.update()\n",
    "        if inf is inference_lam:\n",
    "            inf.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "inference.run(n_iter=200000, n_print=100, n_samples=1,\n",
    "              logdir='data/run29',\n",
    "              optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "              scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13ab36e80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a5499e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxT7o0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a5499b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Zmat = q_Z.mean().eval()\n",
    "\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')\n",
    "plt.matshow(Zmat.T, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEUIdABIh1AEgkf8HQkvhiVONCgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e702a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_A.mean().eval().ravel()); plt.hist(dA.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a53c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_B.mean().eval().ravel(), 200), plt.hist(dB.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e6f3278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_C.mean().eval().ravel(), 200), plt.hist(dC.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n38ysQRz6ZmYN4tA3M2sQh76ZWYP8PwsWeHq2qYtIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a9465f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lam_mu.eval(), 200), plt.hist(dlam, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.88979483,  2.45703912,  0.32039168, ...,  2.60091019,\n",
       "        2.78731441,  2.51170897], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_mu.value().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.88979483,  2.45703912,  0.32039168, ...,  2.60091019,\n",
       "        2.78731441,  2.51170897], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lam.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25033447], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sig.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
