{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}(e^\\lambda)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B * X)_{us} + (C * Z)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-7, 1^2) \\\\\n",
    "Z_{ks} &\\sim \\mathrm{Bernoulli}(\\pi_k) \\\\\n",
    "\\pi_k &\\equiv \\prod_{i=1}^k \\delta_k \\\\\n",
    "\\delta_j &\\sim \\mathrm{Beta}(3, 1)\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "Nrep = 50  # number of observations per unit per stim\n",
    "NB = 1000  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = np.log(softplus(25 + 5 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.1 * np.random.randn(NU, P))  # regressor effects\n",
    "dC = np.log(np.array([0.25, 0.55, 1.4, 2.2])[np.newaxis, :] + 0.1 * np.random.randn(NU, K))  # latent effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70137886  0.57732232  0.24895886  0.49916835] [ 0.70137886  0.40492167  0.10080884  0.05032058]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127bda5c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12250a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dX = 0.1 * np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "ddelta = stats.beta.rvs(1.2, 1, size=(K,))\n",
    "dpi = np.cumprod(ddelta)\n",
    "print(ddelta, dpi)\n",
    "dZ = stats.bernoulli.rvs(dpi[:, np.newaxis], size=(K, NS))\n",
    "\n",
    "# plot states\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = np.tile(dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1) + np.sum(dC[dU] * dZ[:, dS].T, axis=1), Nrep)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcount = stats.poisson.rvs(np.exp(dlam))\n",
    "dcount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIQQ9AEgIQR9AEgIQR8AEvL//neNIhq8upEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224fc6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dlam, bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224fc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.exp(dlam), bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCS1MigkSa0MCklSq/8PyPnIWq7ZugoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12918bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount.copy()\n",
    "Xdat = np.tile(dX[:, dS], (1, Nrep)).T\n",
    "unit = np.tile(dU, Nrep)\n",
    "stim = np.tile(dS, Nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GLM to get inits\n",
    "\n",
    "Here, we run a GLM on the observed data with $A$ and $B$ included (but not $C \\cdot Z$) to get a rough starting point. This takes time, but is a net win in terms of saving SGD iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "Xdf = pd.DataFrame(Xdat)\n",
    "Xdf.columns = ['X' + str(c) for c in Xdf.columns]\n",
    "dta = pd.concat([pd.DataFrame({'count': count, 'unit': unit, 'stim': stim}), Xdf], axis=1)\n",
    "formula = 'count ~ -1 + C(unit) + C(unit) * (' + '+'.join(Xdf.columns) + ')'\n",
    "mod = smf.glm(formula=formula, data=dta, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the model's fitted parameters into inits for $A$ and $B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_init = mod.params.values[:NU].astype('float32')\n",
    "B_init = mod.params.values[NU:].reshape(P, NU).T.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(125000), Dimension(3)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.train.range_input_producer(N).dequeue_many(NB, name='batch_inds')\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB, name='batch_counts')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = N\n",
    "batch_inds = np.arange(N)\n",
    "batch_counts = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=5 * tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=3 * tf.ones((NU, P)), name='B')\n",
    "    C = ed.models.Normal(mu=tf.zeros((NU, K)), sigma=3 * tf.ones((NU, K)), name='C')  \n",
    "    \n",
    "    delta = ed.models.Beta(a=3 * tf.ones(K), b=tf.ones(K), name='delta')\n",
    "    log_delta = tf.log(delta)\n",
    "\n",
    "    pi = tf.exp(tf.cumsum(log_delta), name='pi')\n",
    "\n",
    "    Z = ed.models.Bernoulli(p=tf.tile(tf.expand_dims(pi, 0), [NS, 1]), name='Z')\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-7.0], sigma=[1.], name='sig')\n",
    "\n",
    "    lam_vars = (tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1) + \n",
    "           tf.reduce_sum(tf.gather(C, U) * tf.gather(tf.to_float(Z), S), 1))\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(NB), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(A_init), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    tf.scalar_summary('q_A', tf.reduce_mean(q_A.mean()))\n",
    "\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(B_init), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    tf.scalar_summary('q_B', tf.reduce_mean(q_B.mean()))\n",
    "\n",
    "    q_C = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, K))), \n",
    "                                            sigma=tf.Variable(0.5 + tf.random_uniform((NU, K))),\n",
    "                                            name='C')\n",
    "    tf.scalar_summary('q_C', tf.reduce_mean(q_C.mean()))\n",
    "    \n",
    "    q_Z = ed.models.BernoulliWithSigmoidP(p=tf.Variable(-1.5 + tf.zeros((NS, K))), name='Z')\n",
    "    tf.scalar_summary('q_Z', tf.reduce_mean(q_Z.mean()))\n",
    "\n",
    "    q_delta = ed.models.BetaWithSoftplusAB(a=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           b=tf.Variable(1 + tf.random_uniform((K,))),\n",
    "                                           name='delta')\n",
    "    tf.scalar_summary('min_q_delta', tf.reduce_min(q_delta.mean()))\n",
    "    tf.scalar_summary('max_q_delta', tf.reduce_max(q_delta.mean()))\n",
    "\n",
    "    lam_mu = tf.Variable(2 + tf.random_normal((N,)))\n",
    "    tf.scalar_summary('lam_mu_mean', tf.reduce_mean(tf.gather(lam_mu, batch_inds)))\n",
    "    lam_sig = tf.Variable(3 * tf.random_uniform((N,)) + 2)\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-0.1 * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_ELBO(latent_vars, data, scale):\n",
    "    from edward.util import copy\n",
    "    p_log_prob = 0.0\n",
    "    q_log_prob = 0.0\n",
    "    z_sample = {}\n",
    "    scope = \"ELBO\"\n",
    "\n",
    "    for z, qz in latent_vars.items():\n",
    "        # Copy q(z) to obtain new set of posterior samples.\n",
    "        qz_copy = copy(qz, scope=scope)\n",
    "        z_sample[z] = qz_copy.value()\n",
    "        z_log_prob = tf.reduce_sum(qz.log_prob(tf.stop_gradient(z_sample[z])))\n",
    "        if z in scale:\n",
    "            z_log_prob *= scale[z]\n",
    "\n",
    "        q_log_prob += z_log_prob\n",
    "\n",
    "    dict_swap = z_sample\n",
    "    for x, qx in data.items():\n",
    "        if isinstance(x, ed.RandomVariable):\n",
    "            if isinstance(qx, ed.RandomVariable):\n",
    "                qx_copy = copy(qx, scope=scope)\n",
    "                dict_swap[x] = qx_copy.value()\n",
    "            else:\n",
    "                dict_swap[x] = qx\n",
    "\n",
    "            for z in latent_vars.keys():\n",
    "                z_copy = copy(z, dict_swap, scope=scope)\n",
    "                z_log_prob = tf.reduce_sum(z_copy.log_prob(dict_swap[z]))\n",
    "                if z in scale:\n",
    "                    z_log_prob *= scale[z]\n",
    "\n",
    "                p_log_prob += z_log_prob\n",
    "\n",
    "            for x in data.keys():\n",
    "                if isinstance(x, ed.RandomVariable):\n",
    "                    x_copy = copy(x, dict_swap, scope=scope)\n",
    "                    x_log_prob = tf.reduce_sum(x_copy.log_prob(dict_swap[x]))\n",
    "                if x in scale:\n",
    "                    x_log_prob *= scale[x]\n",
    "\n",
    "                p_log_prob += x_log_prob\n",
    "\n",
    "    return tf.reduce_mean(p_log_prob - q_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = make_ELBO({A: q_A, B: q_B, C: q_C, Z: q_Z, sig: q_sig, delta: q_delta, lam: q_lam}, \n",
    "                 {cnt: tf.cast(batch_counts, 'float32')}, \n",
    "                 {lam: N/NB, cnt: N/NB})\n",
    "\n",
    "tf.scalar_summary('ELBO', elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference_lam = ed.KLqp({lam: q_lam, sig: q_sig}, data={cnt: batch_counts, Z: q_Z, \n",
    "                                            delta: q_delta, A: q_A, B: q_B, \n",
    "                                            C: q_C, delta: q_delta})\n",
    "inference_coeffs = ed.KLqp({A: q_A, B: q_B, C: q_C}, \n",
    "                    data={cnt: batch_counts, Z: q_Z, delta: q_delta, lam: q_lam, sig: q_sig})\n",
    "inference_latents = ed.KLqp({Z: q_Z, delta: q_delta}, \n",
    "                    data={cnt: batch_counts, A: q_A, B: q_B, C: q_C, sig: q_sig, lam: q_lam})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- The learning rate is a difficult tradeoff: 1e-2 drastically speeds convergence but can run into NaNs; 1e-3 (the default) is much slower.\n",
    "    - **TO DO**: Does regularizing emergence of `NaN`s help with this?\n",
    "- I'm currently using \"all\" the data, which appears to be faster (run-time, wise) than using minibatches. (Not entirely sure why this is, except perhaps that switching data into and out of the graph has a cost.) I've also found that minibatches need to be fairly substantial to be effective, since most variables ($\\lambda$, $A$, $B$, $C$) are unit-specific (i.e., local), so unless you have several observations from that unit, convergence can be slow.\n",
    "    - Ultimately, it might speed things to have a smarter minibatch selection (i.e., all observations for a single unit) when updating the local variables.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. \n",
    "- I've noticed no difference below in how many steps one takes along each coordinate before switching (number of inner loop iterations), either in runtime or convergence. Perhaps this matters in the final stages, but I would suspect that then it favors tighter inner loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 [100%]: Loss = 10241294336.000\n",
      "Iteration 1 [100%]: Loss = 22332803072.000\n",
      "Iteration 1 [100%]: Loss = 8233658368.000\n"
     ]
    }
   ],
   "source": [
    "# Here, we're running each inference for 1 iteration to handle boilerplate setup\n",
    "# one **TO DO**: is to fix logging so that we don't need to do this.\n",
    "for inf in [inference_lam, inference_coeffs, inference_latents]:\n",
    "    if inf is inference_lam:\n",
    "        logdir = 'data/run40'\n",
    "    else:\n",
    "        logdir = None\n",
    "        \n",
    "    inf.run(n_iter=1, n_print=100, n_samples=1,  \n",
    "                  logdir=logdir,\n",
    "                  optimizer=tf.train.AdamOptimizer(5e-3),\n",
    "                  scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70100 [7010000%]: Loss = 403087.188\n",
      "Iteration 70200 [7020000%]: Loss = 404660.062\n",
      "Iteration 70300 [7030000%]: Loss = 404875.969\n",
      "Iteration 70400 [7040000%]: Loss = 403750.812\n",
      "Iteration 70500 [7050000%]: Loss = 403941.656\n",
      "Iteration 70600 [7060000%]: Loss = 402609.156\n",
      "Iteration 70700 [7070000%]: Loss = 407210.969\n",
      "Iteration 70800 [7080000%]: Loss = 402472.594\n",
      "Iteration 70900 [7090000%]: Loss = 404259.750\n",
      "Iteration 71000 [7100000%]: Loss = 401575.188\n",
      "Iteration 71100 [7110000%]: Loss = 405021.812\n",
      "Iteration 71200 [7120000%]: Loss = 403078.812\n",
      "Iteration 71300 [7130000%]: Loss = 402824.875\n",
      "Iteration 71400 [7140000%]: Loss = 402879.625\n",
      "Iteration 71500 [7150000%]: Loss = 405142.344\n",
      "Iteration 71600 [7160000%]: Loss = 402073.656\n",
      "Iteration 71700 [7170000%]: Loss = 406725.531\n",
      "Iteration 71800 [7180000%]: Loss = 400001.094\n",
      "Iteration 71900 [7190000%]: Loss = 403587.969\n",
      "Iteration 72000 [7200000%]: Loss = 405002.188\n",
      "Iteration 72100 [7210000%]: Loss = 405847.000\n",
      "Iteration 72200 [7220000%]: Loss = 404170.688\n",
      "Iteration 72300 [7230000%]: Loss = 403643.406\n",
      "Iteration 72400 [7240000%]: Loss = 404763.906\n",
      "Iteration 72500 [7250000%]: Loss = 404830.562\n",
      "Iteration 72600 [7260000%]: Loss = 404557.062\n",
      "Iteration 72700 [7270000%]: Loss = 402304.188\n",
      "Iteration 72800 [7280000%]: Loss = 403409.656\n",
      "Iteration 72900 [7290000%]: Loss = 403179.500\n",
      "Iteration 73000 [7300000%]: Loss = 401185.719\n",
      "Iteration 73100 [7310000%]: Loss = 402987.062\n",
      "Iteration 73200 [7320000%]: Loss = 403249.156\n",
      "Iteration 73300 [7330000%]: Loss = 404771.062\n",
      "Iteration 73400 [7340000%]: Loss = 403264.938\n",
      "Iteration 73500 [7350000%]: Loss = 401891.125\n",
      "Iteration 73600 [7360000%]: Loss = 403627.562\n",
      "Iteration 73700 [7370000%]: Loss = 402578.031\n",
      "Iteration 73800 [7380000%]: Loss = 402175.188\n",
      "Iteration 73900 [7390000%]: Loss = 403702.219\n",
      "Iteration 74000 [7400000%]: Loss = 402150.750\n",
      "Iteration 74100 [7410000%]: Loss = 404062.250\n",
      "Iteration 74200 [7420000%]: Loss = 404346.562\n",
      "Iteration 74300 [7430000%]: Loss = 403544.188\n",
      "Iteration 74400 [7440000%]: Loss = 402121.250\n",
      "Iteration 74500 [7450000%]: Loss = 403117.156\n",
      "Iteration 74600 [7460000%]: Loss = 402594.781\n",
      "Iteration 74700 [7470000%]: Loss = 404663.344\n",
      "Iteration 74800 [7480000%]: Loss = 402801.750\n",
      "Iteration 74900 [7490000%]: Loss = 404580.312\n",
      "Iteration 75000 [7500000%]: Loss = 402871.281\n",
      "Iteration 75100 [7510000%]: Loss = 404867.156\n",
      "Iteration 75200 [7520000%]: Loss = 402444.031\n",
      "Iteration 75300 [7530000%]: Loss = 402239.062\n",
      "Iteration 75400 [7540000%]: Loss = 405132.438\n",
      "Iteration 75500 [7550000%]: Loss = 404016.906\n",
      "Iteration 75600 [7560000%]: Loss = 403501.750\n",
      "Iteration 75700 [7570000%]: Loss = 403962.969\n",
      "Iteration 75800 [7580000%]: Loss = 403029.438\n",
      "Iteration 75900 [7590000%]: Loss = 404410.125\n",
      "Iteration 76000 [7600000%]: Loss = 405192.469\n",
      "Iteration 76100 [7610000%]: Loss = 404921.781\n",
      "Iteration 76200 [7620000%]: Loss = 405053.219\n",
      "Iteration 76300 [7630000%]: Loss = 404511.031\n",
      "Iteration 76400 [7640000%]: Loss = 404294.625\n",
      "Iteration 76500 [7650000%]: Loss = 403019.875\n",
      "Iteration 76600 [7660000%]: Loss = 405014.719\n",
      "Iteration 76700 [7670000%]: Loss = 403797.875\n",
      "Iteration 76800 [7680000%]: Loss = 404085.406\n",
      "Iteration 76900 [7690000%]: Loss = 400654.875\n",
      "Iteration 77000 [7700000%]: Loss = 404122.188\n",
      "Iteration 77100 [7710000%]: Loss = 404182.812\n",
      "Iteration 77200 [7720000%]: Loss = 406864.812\n",
      "Iteration 77300 [7730000%]: Loss = 401957.375\n",
      "Iteration 77400 [7740000%]: Loss = 402378.750\n",
      "Iteration 77500 [7750000%]: Loss = 403734.531\n",
      "Iteration 77600 [7760000%]: Loss = 403794.969\n",
      "Iteration 77700 [7770000%]: Loss = 404369.656\n",
      "Iteration 77800 [7780000%]: Loss = 403619.062\n",
      "Iteration 77900 [7790000%]: Loss = 402609.438\n",
      "Iteration 78000 [7800000%]: Loss = 403965.688\n",
      "Iteration 78100 [7810000%]: Loss = 401800.250\n",
      "Iteration 78200 [7820000%]: Loss = 405444.875\n",
      "Iteration 78300 [7830000%]: Loss = 401812.438\n",
      "Iteration 78400 [7840000%]: Loss = 404652.406\n",
      "Iteration 78500 [7850000%]: Loss = 402888.406\n",
      "Iteration 78600 [7860000%]: Loss = 403433.531\n",
      "Iteration 78700 [7870000%]: Loss = 403263.406\n",
      "Iteration 78800 [7880000%]: Loss = 403015.219\n",
      "Iteration 78900 [7890000%]: Loss = 402725.875\n",
      "Iteration 79000 [7900000%]: Loss = 403821.656\n",
      "Iteration 79100 [7910000%]: Loss = 404040.969\n",
      "Iteration 79200 [7920000%]: Loss = 403125.719\n",
      "Iteration 79300 [7930000%]: Loss = 404432.688\n",
      "Iteration 79400 [7940000%]: Loss = 404761.094\n",
      "Iteration 79500 [7950000%]: Loss = 405377.750\n",
      "Iteration 79600 [7960000%]: Loss = 402501.594\n",
      "Iteration 79700 [7970000%]: Loss = 404744.250\n",
      "Iteration 79800 [7980000%]: Loss = 401662.875\n",
      "Iteration 79900 [7990000%]: Loss = 405889.375\n",
      "Iteration 80000 [8000000%]: Loss = 401657.125\n",
      "Iteration 80100 [8010000%]: Loss = 404260.125\n",
      "Iteration 80200 [8020000%]: Loss = 404277.812\n",
      "Iteration 80300 [8030000%]: Loss = 402576.312\n",
      "Iteration 80400 [8040000%]: Loss = 401230.312\n",
      "Iteration 80500 [8050000%]: Loss = 401949.750\n",
      "Iteration 80600 [8060000%]: Loss = 401984.062\n",
      "Iteration 80700 [8070000%]: Loss = 403797.719\n",
      "Iteration 80800 [8080000%]: Loss = 404197.562\n",
      "Iteration 80900 [8090000%]: Loss = 404679.062\n",
      "Iteration 81000 [8100000%]: Loss = 401761.312\n",
      "Iteration 81100 [8110000%]: Loss = 405499.562\n",
      "Iteration 81200 [8120000%]: Loss = 402929.812\n",
      "Iteration 81300 [8130000%]: Loss = 402526.156\n",
      "Iteration 81400 [8140000%]: Loss = 403432.469\n",
      "Iteration 81500 [8150000%]: Loss = 401374.062\n",
      "Iteration 81600 [8160000%]: Loss = 400839.500\n",
      "Iteration 81700 [8170000%]: Loss = 403433.188\n",
      "Iteration 81800 [8180000%]: Loss = 402240.625\n",
      "Iteration 81900 [8190000%]: Loss = 405543.688\n",
      "Iteration 82000 [8200000%]: Loss = 402271.688\n",
      "Iteration 82100 [8210000%]: Loss = 402008.750\n",
      "Iteration 82200 [8220000%]: Loss = 403568.438\n",
      "Iteration 82300 [8230000%]: Loss = 403125.000\n",
      "Iteration 82400 [8240000%]: Loss = 402826.531\n",
      "Iteration 82500 [8250000%]: Loss = 408292.969\n",
      "Iteration 82600 [8260000%]: Loss = 402506.031\n",
      "Iteration 82700 [8270000%]: Loss = 400524.375\n",
      "Iteration 82800 [8280000%]: Loss = 401640.250\n",
      "Iteration 82900 [8290000%]: Loss = 405268.312\n",
      "Iteration 83000 [8300000%]: Loss = 401207.938\n",
      "Iteration 83100 [8310000%]: Loss = 404433.875\n",
      "Iteration 83200 [8320000%]: Loss = 403165.250\n",
      "Iteration 83300 [8330000%]: Loss = 403498.375\n",
      "Iteration 83400 [8340000%]: Loss = 403128.250\n",
      "Iteration 83500 [8350000%]: Loss = 403014.812\n",
      "Iteration 83600 [8360000%]: Loss = 402432.844\n",
      "Iteration 83700 [8370000%]: Loss = 404275.875\n",
      "Iteration 83800 [8380000%]: Loss = 402975.812\n",
      "Iteration 83900 [8390000%]: Loss = 402960.875\n",
      "Iteration 84000 [8400000%]: Loss = 403825.000\n",
      "Iteration 84100 [8410000%]: Loss = 401533.906\n",
      "Iteration 84200 [8420000%]: Loss = 402534.156\n",
      "Iteration 84300 [8430000%]: Loss = 400727.531\n",
      "Iteration 84400 [8440000%]: Loss = 402702.875\n",
      "Iteration 84500 [8450000%]: Loss = 401795.188\n",
      "Iteration 84600 [8460000%]: Loss = 401989.781\n",
      "Iteration 84700 [8470000%]: Loss = 401588.625\n",
      "Iteration 84800 [8480000%]: Loss = 400803.656\n",
      "Iteration 84900 [8490000%]: Loss = 405311.625\n",
      "Iteration 85000 [8500000%]: Loss = nan\n",
      "Iteration 85100 [8510000%]: Loss = nan\n",
      "Iteration 85200 [8520000%]: Loss = nan\n",
      "Iteration 85300 [8530000%]: Loss = nan\n",
      "Iteration 85400 [8540000%]: Loss = nan\n",
      "Iteration 85500 [8550000%]: Loss = nan\n",
      "Iteration 85600 [8560000%]: Loss = nan\n",
      "Iteration 85700 [8570000%]: Loss = nan\n",
      "Iteration 85800 [8580000%]: Loss = nan\n",
      "Iteration 85900 [8590000%]: Loss = nan\n",
      "Iteration 86000 [8600000%]: Loss = nan\n",
      "Iteration 86100 [8610000%]: Loss = nan\n",
      "Iteration 86200 [8620000%]: Loss = nan\n",
      "Iteration 86300 [8630000%]: Loss = nan\n",
      "Iteration 86400 [8640000%]: Loss = nan\n",
      "Iteration 86500 [8650000%]: Loss = nan\n",
      "Iteration 86600 [8660000%]: Loss = nan\n",
      "Iteration 86700 [8670000%]: Loss = nan\n",
      "Iteration 86800 [8680000%]: Loss = nan\n",
      "Iteration 86900 [8690000%]: Loss = nan\n",
      "Iteration 87000 [8700000%]: Loss = nan\n",
      "Iteration 87100 [8710000%]: Loss = nan\n",
      "Iteration 87200 [8720000%]: Loss = nan\n",
      "Iteration 87300 [8730000%]: Loss = nan\n",
      "Iteration 87400 [8740000%]: Loss = nan\n",
      "Iteration 87500 [8750000%]: Loss = nan\n",
      "Iteration 87600 [8760000%]: Loss = nan\n",
      "Iteration 87700 [8770000%]: Loss = nan\n",
      "Iteration 87800 [8780000%]: Loss = nan\n",
      "Iteration 87900 [8790000%]: Loss = nan\n",
      "Iteration 88000 [8800000%]: Loss = nan\n",
      "Iteration 88100 [8810000%]: Loss = nan\n",
      "Iteration 88200 [8820000%]: Loss = nan\n",
      "Iteration 88300 [8830000%]: Loss = nan\n",
      "Iteration 88400 [8840000%]: Loss = nan\n",
      "Iteration 88500 [8850000%]: Loss = nan\n",
      "Iteration 88600 [8860000%]: Loss = nan\n",
      "Iteration 88700 [8870000%]: Loss = nan\n",
      "Iteration 88800 [8880000%]: Loss = nan\n",
      "Iteration 88900 [8890000%]: Loss = nan\n",
      "Iteration 89000 [8900000%]: Loss = nan\n",
      "Iteration 89100 [8910000%]: Loss = nan\n",
      "Iteration 89200 [8920000%]: Loss = nan\n",
      "Iteration 89300 [8930000%]: Loss = nan\n",
      "Iteration 89400 [8940000%]: Loss = nan\n",
      "Iteration 89500 [8950000%]: Loss = nan\n",
      "Iteration 89600 [8960000%]: Loss = nan\n",
      "Iteration 89700 [8970000%]: Loss = nan\n",
      "Iteration 89800 [8980000%]: Loss = nan\n",
      "Iteration 89900 [8990000%]: Loss = nan\n",
      "Iteration 90000 [9000000%]: Loss = nan\n",
      "Iteration 90100 [9010000%]: Loss = nan\n",
      "Iteration 90200 [9020000%]: Loss = nan\n",
      "Iteration 90300 [9030000%]: Loss = nan\n",
      "Iteration 90400 [9040000%]: Loss = nan\n",
      "Iteration 90500 [9050000%]: Loss = nan\n",
      "Iteration 90600 [9060000%]: Loss = nan\n",
      "Iteration 90700 [9070000%]: Loss = nan\n",
      "Iteration 90800 [9080000%]: Loss = nan\n",
      "Iteration 90900 [9090000%]: Loss = nan\n",
      "Iteration 91000 [9100000%]: Loss = nan\n",
      "Iteration 91100 [9110000%]: Loss = nan\n",
      "Iteration 91200 [9120000%]: Loss = nan\n",
      "Iteration 91300 [9130000%]: Loss = nan\n",
      "Iteration 91400 [9140000%]: Loss = nan\n",
      "Iteration 91500 [9150000%]: Loss = nan\n",
      "Iteration 91600 [9160000%]: Loss = nan\n",
      "Iteration 91700 [9170000%]: Loss = nan\n",
      "Iteration 91800 [9180000%]: Loss = nan\n",
      "Iteration 91900 [9190000%]: Loss = nan\n",
      "Iteration 92000 [9200000%]: Loss = nan\n",
      "Iteration 92100 [9210000%]: Loss = nan\n",
      "Iteration 92200 [9220000%]: Loss = nan\n",
      "Iteration 92300 [9230000%]: Loss = nan\n",
      "Iteration 92400 [9240000%]: Loss = nan\n",
      "Iteration 92500 [9250000%]: Loss = nan\n",
      "Iteration 92600 [9260000%]: Loss = nan\n",
      "Iteration 92700 [9270000%]: Loss = nan\n",
      "Iteration 92800 [9280000%]: Loss = nan\n",
      "Iteration 92900 [9290000%]: Loss = nan\n",
      "Iteration 93000 [9300000%]: Loss = nan\n",
      "Iteration 93100 [9310000%]: Loss = nan\n",
      "Iteration 93200 [9320000%]: Loss = nan\n",
      "Iteration 93300 [9330000%]: Loss = nan\n",
      "Iteration 93400 [9340000%]: Loss = nan\n",
      "Iteration 93500 [9350000%]: Loss = nan\n",
      "Iteration 93600 [9360000%]: Loss = nan\n",
      "Iteration 93700 [9370000%]: Loss = nan\n",
      "Iteration 93800 [9380000%]: Loss = nan\n",
      "Iteration 93900 [9390000%]: Loss = nan\n",
      "Iteration 94000 [9400000%]: Loss = nan\n",
      "Iteration 94100 [9410000%]: Loss = nan\n",
      "Iteration 94200 [9420000%]: Loss = nan\n",
      "Iteration 94300 [9430000%]: Loss = nan\n",
      "Iteration 94400 [9440000%]: Loss = nan\n",
      "Iteration 94500 [9450000%]: Loss = nan\n",
      "Iteration 94600 [9460000%]: Loss = nan\n",
      "Iteration 94700 [9470000%]: Loss = nan\n",
      "Iteration 94800 [9480000%]: Loss = nan\n",
      "Iteration 94900 [9490000%]: Loss = nan\n",
      "Iteration 95000 [9500000%]: Loss = nan\n",
      "Iteration 95100 [9510000%]: Loss = nan\n",
      "Iteration 95200 [9520000%]: Loss = nan\n",
      "Iteration 95300 [9530000%]: Loss = nan\n",
      "Iteration 95400 [9540000%]: Loss = nan\n",
      "Iteration 95500 [9550000%]: Loss = nan\n",
      "Iteration 95600 [9560000%]: Loss = nan\n",
      "Iteration 95700 [9570000%]: Loss = nan\n",
      "Iteration 95800 [9580000%]: Loss = nan\n",
      "Iteration 95900 [9590000%]: Loss = nan\n",
      "Iteration 96000 [9600000%]: Loss = nan\n",
      "Iteration 96100 [9610000%]: Loss = nan\n",
      "Iteration 96200 [9620000%]: Loss = nan\n",
      "Iteration 96300 [9630000%]: Loss = nan\n",
      "Iteration 96400 [9640000%]: Loss = nan\n",
      "Iteration 96500 [9650000%]: Loss = nan\n",
      "Iteration 96600 [9660000%]: Loss = nan\n",
      "Iteration 96700 [9670000%]: Loss = nan\n",
      "Iteration 96800 [9680000%]: Loss = nan\n",
      "Iteration 96900 [9690000%]: Loss = nan\n",
      "Iteration 97000 [9700000%]: Loss = nan\n",
      "Iteration 97100 [9710000%]: Loss = nan\n",
      "Iteration 97200 [9720000%]: Loss = nan\n",
      "Iteration 97300 [9730000%]: Loss = nan\n",
      "Iteration 97400 [9740000%]: Loss = nan\n",
      "Iteration 97500 [9750000%]: Loss = nan\n",
      "Iteration 97600 [9760000%]: Loss = nan\n",
      "Iteration 97700 [9770000%]: Loss = nan\n",
      "Iteration 97800 [9780000%]: Loss = nan\n",
      "Iteration 97900 [9790000%]: Loss = nan\n",
      "Iteration 98000 [9800000%]: Loss = nan\n",
      "Iteration 98100 [9810000%]: Loss = nan\n",
      "Iteration 98200 [9820000%]: Loss = nan\n",
      "Iteration 98300 [9830000%]: Loss = nan\n",
      "Iteration 98400 [9840000%]: Loss = nan\n",
      "Iteration 98500 [9850000%]: Loss = nan\n",
      "Iteration 98600 [9860000%]: Loss = nan\n",
      "Iteration 98700 [9870000%]: Loss = nan\n",
      "Iteration 98800 [9880000%]: Loss = nan\n",
      "Iteration 98900 [9890000%]: Loss = nan\n",
      "Iteration 99000 [9900000%]: Loss = nan\n",
      "Iteration 99100 [9910000%]: Loss = nan\n",
      "Iteration 99200 [9920000%]: Loss = nan\n",
      "Iteration 99300 [9930000%]: Loss = nan\n",
      "Iteration 99400 [9940000%]: Loss = nan\n",
      "Iteration 99500 [9950000%]: Loss = nan\n",
      "Iteration 99600 [9960000%]: Loss = nan\n",
      "Iteration 99700 [9970000%]: Loss = nan\n",
      "Iteration 99800 [9980000%]: Loss = nan\n",
      "Iteration 99900 [9990000%]: Loss = nan\n",
      "Iteration 100000 [10000000%]: Loss = nan\n",
      "Iteration 100100 [10010000%]: Loss = nan\n",
      "Iteration 100200 [10020000%]: Loss = nan\n",
      "Iteration 100300 [10030000%]: Loss = nan\n",
      "Iteration 100400 [10040000%]: Loss = nan\n",
      "Iteration 100500 [10050000%]: Loss = nan\n",
      "Iteration 100600 [10060000%]: Loss = nan\n",
      "Iteration 100700 [10070000%]: Loss = nan\n",
      "Iteration 100800 [10080000%]: Loss = nan\n",
      "Iteration 100900 [10090000%]: Loss = nan\n",
      "Iteration 101000 [10100000%]: Loss = nan\n",
      "Iteration 101100 [10110000%]: Loss = nan\n",
      "Iteration 101200 [10120000%]: Loss = nan\n",
      "Iteration 101300 [10130000%]: Loss = nan\n",
      "Iteration 101400 [10140000%]: Loss = nan\n",
      "Iteration 101500 [10150000%]: Loss = nan\n",
      "Iteration 101600 [10160000%]: Loss = nan\n",
      "Iteration 101700 [10170000%]: Loss = nan\n",
      "Iteration 101800 [10180000%]: Loss = nan\n",
      "Iteration 101900 [10190000%]: Loss = nan\n",
      "Iteration 102000 [10200000%]: Loss = nan\n",
      "Iteration 102100 [10210000%]: Loss = nan\n",
      "Iteration 102200 [10220000%]: Loss = nan\n",
      "Iteration 102300 [10230000%]: Loss = nan\n",
      "Iteration 102400 [10240000%]: Loss = nan\n",
      "Iteration 102500 [10250000%]: Loss = nan\n",
      "Iteration 102600 [10260000%]: Loss = nan\n",
      "Iteration 102700 [10270000%]: Loss = nan\n",
      "Iteration 102800 [10280000%]: Loss = nan\n",
      "Iteration 102900 [10290000%]: Loss = nan\n",
      "Iteration 103000 [10300000%]: Loss = nan\n",
      "Iteration 103100 [10310000%]: Loss = nan\n",
      "Iteration 103200 [10320000%]: Loss = nan\n",
      "Iteration 103300 [10330000%]: Loss = nan\n",
      "Iteration 103400 [10340000%]: Loss = nan\n",
      "Iteration 103500 [10350000%]: Loss = nan\n",
      "Iteration 103600 [10360000%]: Loss = nan\n",
      "Iteration 103700 [10370000%]: Loss = nan\n",
      "Iteration 103800 [10380000%]: Loss = nan\n",
      "Iteration 103900 [10390000%]: Loss = nan\n",
      "Iteration 104000 [10400000%]: Loss = nan\n",
      "Iteration 104100 [10410000%]: Loss = nan\n",
      "Iteration 104200 [10420000%]: Loss = nan\n",
      "Iteration 104300 [10430000%]: Loss = nan\n",
      "Iteration 104400 [10440000%]: Loss = nan\n",
      "Iteration 104500 [10450000%]: Loss = nan\n",
      "Iteration 104600 [10460000%]: Loss = nan\n",
      "Iteration 104700 [10470000%]: Loss = nan\n",
      "Iteration 104800 [10480000%]: Loss = nan\n",
      "Iteration 104900 [10490000%]: Loss = nan\n",
      "Iteration 105000 [10500000%]: Loss = nan\n",
      "Iteration 105100 [10510000%]: Loss = nan\n",
      "Iteration 105200 [10520000%]: Loss = nan\n",
      "Iteration 105300 [10530000%]: Loss = nan\n",
      "Iteration 105400 [10540000%]: Loss = nan\n",
      "Iteration 105500 [10550000%]: Loss = nan\n",
      "Iteration 105600 [10560000%]: Loss = nan\n",
      "Iteration 105700 [10570000%]: Loss = nan\n",
      "Iteration 105800 [10580000%]: Loss = nan\n",
      "Iteration 105900 [10590000%]: Loss = nan\n",
      "Iteration 106000 [10600000%]: Loss = nan\n",
      "Iteration 106100 [10610000%]: Loss = nan\n",
      "Iteration 106200 [10620000%]: Loss = nan\n",
      "Iteration 106300 [10630000%]: Loss = nan\n",
      "Iteration 106400 [10640000%]: Loss = nan\n",
      "Iteration 106500 [10650000%]: Loss = nan\n",
      "Iteration 106600 [10660000%]: Loss = nan\n",
      "Iteration 106700 [10670000%]: Loss = nan\n",
      "Iteration 106800 [10680000%]: Loss = nan\n",
      "Iteration 106900 [10690000%]: Loss = nan\n",
      "Iteration 107000 [10700000%]: Loss = nan\n",
      "Iteration 107100 [10710000%]: Loss = nan\n",
      "Iteration 107200 [10720000%]: Loss = nan\n",
      "Iteration 107300 [10730000%]: Loss = nan\n",
      "Iteration 107400 [10740000%]: Loss = nan\n",
      "Iteration 107500 [10750000%]: Loss = nan\n",
      "Iteration 107600 [10760000%]: Loss = nan\n",
      "Iteration 107700 [10770000%]: Loss = nan\n",
      "Iteration 107800 [10780000%]: Loss = nan\n",
      "Iteration 107900 [10790000%]: Loss = nan\n",
      "Iteration 108000 [10800000%]: Loss = nan\n",
      "Iteration 108100 [10810000%]: Loss = nan\n",
      "Iteration 108200 [10820000%]: Loss = nan\n",
      "Iteration 108300 [10830000%]: Loss = nan\n",
      "Iteration 108400 [10840000%]: Loss = nan\n",
      "Iteration 108500 [10850000%]: Loss = nan\n",
      "Iteration 108600 [10860000%]: Loss = nan\n",
      "Iteration 108700 [10870000%]: Loss = nan\n",
      "Iteration 108800 [10880000%]: Loss = nan\n",
      "Iteration 108900 [10890000%]: Loss = nan\n",
      "Iteration 109000 [10900000%]: Loss = nan\n",
      "Iteration 109100 [10910000%]: Loss = nan\n",
      "Iteration 109200 [10920000%]: Loss = nan\n",
      "Iteration 109300 [10930000%]: Loss = nan\n",
      "Iteration 109400 [10940000%]: Loss = nan\n",
      "Iteration 109500 [10950000%]: Loss = nan\n",
      "Iteration 109600 [10960000%]: Loss = nan\n",
      "Iteration 109700 [10970000%]: Loss = nan\n",
      "Iteration 109800 [10980000%]: Loss = nan\n",
      "Iteration 109900 [10990000%]: Loss = nan\n",
      "Iteration 110000 [11000000%]: Loss = nan\n",
      "Iteration 110100 [11010000%]: Loss = nan\n",
      "Iteration 110200 [11020000%]: Loss = nan\n",
      "Iteration 110300 [11030000%]: Loss = nan\n",
      "Iteration 110400 [11040000%]: Loss = nan\n",
      "Iteration 110500 [11050000%]: Loss = nan\n",
      "Iteration 110600 [11060000%]: Loss = nan\n",
      "Iteration 110700 [11070000%]: Loss = nan\n",
      "Iteration 110800 [11080000%]: Loss = nan\n",
      "Iteration 110900 [11090000%]: Loss = nan\n",
      "Iteration 111000 [11100000%]: Loss = nan\n",
      "Iteration 111100 [11110000%]: Loss = nan\n",
      "Iteration 111200 [11120000%]: Loss = nan\n",
      "Iteration 111300 [11130000%]: Loss = nan\n",
      "Iteration 111400 [11140000%]: Loss = nan\n",
      "Iteration 111500 [11150000%]: Loss = nan\n",
      "Iteration 111600 [11160000%]: Loss = nan\n",
      "Iteration 111700 [11170000%]: Loss = nan\n",
      "Iteration 111800 [11180000%]: Loss = nan\n",
      "Iteration 111900 [11190000%]: Loss = nan\n",
      "Iteration 112000 [11200000%]: Loss = nan\n",
      "Iteration 112100 [11210000%]: Loss = nan\n",
      "Iteration 112200 [11220000%]: Loss = nan\n",
      "Iteration 112300 [11230000%]: Loss = nan\n",
      "Iteration 112400 [11240000%]: Loss = nan\n",
      "Iteration 112500 [11250000%]: Loss = nan\n",
      "Iteration 112600 [11260000%]: Loss = nan\n",
      "Iteration 112700 [11270000%]: Loss = nan\n",
      "Iteration 112800 [11280000%]: Loss = nan\n",
      "Iteration 112900 [11290000%]: Loss = nan\n",
      "Iteration 113000 [11300000%]: Loss = nan\n",
      "Iteration 113100 [11310000%]: Loss = nan\n",
      "Iteration 113200 [11320000%]: Loss = nan\n",
      "Iteration 113300 [11330000%]: Loss = nan\n",
      "Iteration 113400 [11340000%]: Loss = nan\n",
      "Iteration 113500 [11350000%]: Loss = nan\n",
      "Iteration 113600 [11360000%]: Loss = nan\n",
      "Iteration 113700 [11370000%]: Loss = nan\n",
      "Iteration 113800 [11380000%]: Loss = nan\n",
      "Iteration 113900 [11390000%]: Loss = nan\n",
      "Iteration 114000 [11400000%]: Loss = nan\n",
      "Iteration 114100 [11410000%]: Loss = nan\n",
      "Iteration 114200 [11420000%]: Loss = nan\n",
      "Iteration 114300 [11430000%]: Loss = nan\n",
      "Iteration 114400 [11440000%]: Loss = nan\n",
      "Iteration 114500 [11450000%]: Loss = nan\n",
      "Iteration 114600 [11460000%]: Loss = nan\n",
      "Iteration 114700 [11470000%]: Loss = nan\n",
      "Iteration 114800 [11480000%]: Loss = nan\n",
      "Iteration 114900 [11490000%]: Loss = nan\n",
      "Iteration 115000 [11500000%]: Loss = nan\n",
      "Iteration 115100 [11510000%]: Loss = nan\n",
      "Iteration 115200 [11520000%]: Loss = nan\n",
      "Iteration 115300 [11530000%]: Loss = nan\n",
      "Iteration 115400 [11540000%]: Loss = nan\n",
      "Iteration 115500 [11550000%]: Loss = nan\n",
      "Iteration 115600 [11560000%]: Loss = nan\n",
      "Iteration 115700 [11570000%]: Loss = nan\n",
      "Iteration 115800 [11580000%]: Loss = nan\n",
      "Iteration 115900 [11590000%]: Loss = nan\n",
      "Iteration 116000 [11600000%]: Loss = nan\n",
      "Iteration 116100 [11610000%]: Loss = nan\n",
      "Iteration 116200 [11620000%]: Loss = nan\n",
      "Iteration 116300 [11630000%]: Loss = nan\n",
      "Iteration 116400 [11640000%]: Loss = nan\n",
      "Iteration 116500 [11650000%]: Loss = nan\n",
      "Iteration 116600 [11660000%]: Loss = nan\n",
      "Iteration 116700 [11670000%]: Loss = nan\n",
      "Iteration 116800 [11680000%]: Loss = nan\n",
      "Iteration 116900 [11690000%]: Loss = nan\n",
      "Iteration 117000 [11700000%]: Loss = nan\n",
      "Iteration 117100 [11710000%]: Loss = nan\n",
      "Iteration 117200 [11720000%]: Loss = nan\n",
      "Iteration 117300 [11730000%]: Loss = nan\n",
      "Iteration 117400 [11740000%]: Loss = nan\n",
      "Iteration 117500 [11750000%]: Loss = nan\n",
      "Iteration 117600 [11760000%]: Loss = nan\n",
      "Iteration 117700 [11770000%]: Loss = nan\n",
      "Iteration 117800 [11780000%]: Loss = nan\n",
      "Iteration 117900 [11790000%]: Loss = nan\n",
      "Iteration 118000 [11800000%]: Loss = nan\n",
      "Iteration 118100 [11810000%]: Loss = nan\n",
      "Iteration 118200 [11820000%]: Loss = nan\n",
      "Iteration 118300 [11830000%]: Loss = nan\n",
      "Iteration 118400 [11840000%]: Loss = nan\n",
      "Iteration 118500 [11850000%]: Loss = nan\n",
      "Iteration 118600 [11860000%]: Loss = nan\n",
      "Iteration 118700 [11870000%]: Loss = nan\n",
      "Iteration 118800 [11880000%]: Loss = nan\n",
      "Iteration 118900 [11890000%]: Loss = nan\n",
      "Iteration 119000 [11900000%]: Loss = nan\n",
      "Iteration 119100 [11910000%]: Loss = nan\n",
      "Iteration 119200 [11920000%]: Loss = nan\n",
      "Iteration 119300 [11930000%]: Loss = nan\n",
      "Iteration 119400 [11940000%]: Loss = nan\n",
      "Iteration 119500 [11950000%]: Loss = nan\n",
      "Iteration 119600 [11960000%]: Loss = nan\n",
      "Iteration 119700 [11970000%]: Loss = nan\n",
      "Iteration 119800 [11980000%]: Loss = nan\n",
      "Iteration 119900 [11990000%]: Loss = nan\n",
      "Iteration 120000 [12000000%]: Loss = nan\n",
      "Iteration 120100 [12010000%]: Loss = nan\n",
      "Iteration 120200 [12020000%]: Loss = nan\n",
      "Iteration 120300 [12030000%]: Loss = nan\n",
      "Iteration 120400 [12040000%]: Loss = nan\n",
      "Iteration 120500 [12050000%]: Loss = nan\n",
      "Iteration 120600 [12060000%]: Loss = nan\n",
      "Iteration 120700 [12070000%]: Loss = nan\n",
      "Iteration 120800 [12080000%]: Loss = nan\n",
      "Iteration 120900 [12090000%]: Loss = nan\n",
      "Iteration 121000 [12100000%]: Loss = nan\n",
      "Iteration 121100 [12110000%]: Loss = nan\n",
      "Iteration 121200 [12120000%]: Loss = nan\n",
      "Iteration 121300 [12130000%]: Loss = nan\n",
      "Iteration 121400 [12140000%]: Loss = nan\n",
      "Iteration 121500 [12150000%]: Loss = nan\n",
      "Iteration 121600 [12160000%]: Loss = nan\n",
      "Iteration 121700 [12170000%]: Loss = nan\n",
      "Iteration 121800 [12180000%]: Loss = nan\n",
      "Iteration 121900 [12190000%]: Loss = nan\n",
      "Iteration 122000 [12200000%]: Loss = nan\n",
      "Iteration 122100 [12210000%]: Loss = nan\n",
      "Iteration 122200 [12220000%]: Loss = nan\n",
      "Iteration 122300 [12230000%]: Loss = nan\n",
      "Iteration 122400 [12240000%]: Loss = nan\n",
      "Iteration 122500 [12250000%]: Loss = nan\n",
      "Iteration 122600 [12260000%]: Loss = nan\n",
      "Iteration 122700 [12270000%]: Loss = nan\n",
      "Iteration 122800 [12280000%]: Loss = nan\n",
      "Iteration 122900 [12290000%]: Loss = nan\n",
      "Iteration 123000 [12300000%]: Loss = nan\n",
      "Iteration 123100 [12310000%]: Loss = nan\n",
      "Iteration 123200 [12320000%]: Loss = nan\n",
      "Iteration 123300 [12330000%]: Loss = nan\n",
      "Iteration 123400 [12340000%]: Loss = nan\n",
      "Iteration 123500 [12350000%]: Loss = nan\n",
      "Iteration 123600 [12360000%]: Loss = nan\n",
      "Iteration 123700 [12370000%]: Loss = nan\n",
      "Iteration 123800 [12380000%]: Loss = nan\n",
      "Iteration 123900 [12390000%]: Loss = nan\n",
      "Iteration 124000 [12400000%]: Loss = nan\n",
      "Iteration 124100 [12410000%]: Loss = nan\n",
      "Iteration 124200 [12420000%]: Loss = nan\n",
      "Iteration 124300 [12430000%]: Loss = nan\n",
      "Iteration 124400 [12440000%]: Loss = nan\n",
      "Iteration 124500 [12450000%]: Loss = nan\n",
      "Iteration 124600 [12460000%]: Loss = nan\n",
      "Iteration 124700 [12470000%]: Loss = nan\n",
      "Iteration 124800 [12480000%]: Loss = nan\n",
      "Iteration 124900 [12490000%]: Loss = nan\n",
      "Iteration 125000 [12500000%]: Loss = nan\n",
      "Iteration 125100 [12510000%]: Loss = nan\n",
      "Iteration 125200 [12520000%]: Loss = nan\n",
      "Iteration 125300 [12530000%]: Loss = nan\n",
      "Iteration 125400 [12540000%]: Loss = nan\n",
      "Iteration 125500 [12550000%]: Loss = nan\n",
      "Iteration 125600 [12560000%]: Loss = nan\n",
      "Iteration 125700 [12570000%]: Loss = nan\n",
      "Iteration 125800 [12580000%]: Loss = nan\n",
      "Iteration 125900 [12590000%]: Loss = nan\n",
      "Iteration 126000 [12600000%]: Loss = nan\n",
      "Iteration 126100 [12610000%]: Loss = nan\n",
      "Iteration 126200 [12620000%]: Loss = nan\n",
      "Iteration 126300 [12630000%]: Loss = nan\n",
      "Iteration 126400 [12640000%]: Loss = nan\n",
      "Iteration 126500 [12650000%]: Loss = nan\n",
      "Iteration 126600 [12660000%]: Loss = nan\n",
      "Iteration 126700 [12670000%]: Loss = nan\n",
      "Iteration 126800 [12680000%]: Loss = nan\n",
      "Iteration 126900 [12690000%]: Loss = nan\n",
      "Iteration 127000 [12700000%]: Loss = nan\n",
      "Iteration 127100 [12710000%]: Loss = nan\n",
      "Iteration 127200 [12720000%]: Loss = nan\n",
      "Iteration 127300 [12730000%]: Loss = nan\n",
      "Iteration 127400 [12740000%]: Loss = nan\n",
      "Iteration 127500 [12750000%]: Loss = nan\n",
      "Iteration 127600 [12760000%]: Loss = nan\n",
      "Iteration 127700 [12770000%]: Loss = nan\n",
      "Iteration 127800 [12780000%]: Loss = nan\n",
      "Iteration 127900 [12790000%]: Loss = nan\n",
      "Iteration 128000 [12800000%]: Loss = nan\n",
      "Iteration 128100 [12810000%]: Loss = nan\n",
      "Iteration 128200 [12820000%]: Loss = nan\n",
      "Iteration 128300 [12830000%]: Loss = nan\n",
      "Iteration 128400 [12840000%]: Loss = nan\n",
      "Iteration 128500 [12850000%]: Loss = nan\n",
      "Iteration 128600 [12860000%]: Loss = nan\n",
      "Iteration 128700 [12870000%]: Loss = nan\n",
      "Iteration 128800 [12880000%]: Loss = nan\n",
      "Iteration 128900 [12890000%]: Loss = nan\n",
      "Iteration 129000 [12900000%]: Loss = nan\n",
      "Iteration 129100 [12910000%]: Loss = nan\n",
      "Iteration 129200 [12920000%]: Loss = nan\n",
      "Iteration 129300 [12930000%]: Loss = nan\n",
      "Iteration 129400 [12940000%]: Loss = nan\n",
      "Iteration 129500 [12950000%]: Loss = nan\n",
      "Iteration 129600 [12960000%]: Loss = nan\n",
      "Iteration 129700 [12970000%]: Loss = nan\n",
      "Iteration 129800 [12980000%]: Loss = nan\n",
      "Iteration 129900 [12990000%]: Loss = nan\n",
      "Iteration 130000 [13000000%]: Loss = nan\n",
      "Iteration 130100 [13010000%]: Loss = nan\n",
      "Iteration 130200 [13020000%]: Loss = nan\n",
      "Iteration 130300 [13030000%]: Loss = nan\n",
      "Iteration 130400 [13040000%]: Loss = nan\n",
      "Iteration 130500 [13050000%]: Loss = nan\n",
      "Iteration 130600 [13060000%]: Loss = nan\n",
      "Iteration 130700 [13070000%]: Loss = nan\n",
      "Iteration 130800 [13080000%]: Loss = nan\n",
      "Iteration 130900 [13090000%]: Loss = nan\n",
      "Iteration 131000 [13100000%]: Loss = nan\n",
      "Iteration 131100 [13110000%]: Loss = nan\n",
      "Iteration 131200 [13120000%]: Loss = nan\n",
      "Iteration 131300 [13130000%]: Loss = nan\n",
      "Iteration 131400 [13140000%]: Loss = nan\n",
      "Iteration 131500 [13150000%]: Loss = nan\n",
      "Iteration 131600 [13160000%]: Loss = nan\n",
      "Iteration 131700 [13170000%]: Loss = nan\n",
      "Iteration 131800 [13180000%]: Loss = nan\n",
      "Iteration 131900 [13190000%]: Loss = nan\n",
      "Iteration 132000 [13200000%]: Loss = nan\n",
      "Iteration 132100 [13210000%]: Loss = nan\n",
      "Iteration 132200 [13220000%]: Loss = nan\n",
      "Iteration 132300 [13230000%]: Loss = nan\n",
      "Iteration 132400 [13240000%]: Loss = nan\n",
      "Iteration 132500 [13250000%]: Loss = nan\n",
      "Iteration 132600 [13260000%]: Loss = nan\n",
      "Iteration 132700 [13270000%]: Loss = nan\n",
      "Iteration 132800 [13280000%]: Loss = nan\n",
      "Iteration 132900 [13290000%]: Loss = nan\n",
      "Iteration 133000 [13300000%]: Loss = nan\n",
      "Iteration 133100 [13310000%]: Loss = nan\n",
      "Iteration 133200 [13320000%]: Loss = nan\n",
      "Iteration 133300 [13330000%]: Loss = nan\n",
      "Iteration 133400 [13340000%]: Loss = nan\n",
      "Iteration 133500 [13350000%]: Loss = nan\n",
      "Iteration 133600 [13360000%]: Loss = nan\n",
      "Iteration 133700 [13370000%]: Loss = nan\n",
      "Iteration 133800 [13380000%]: Loss = nan\n",
      "Iteration 133900 [13390000%]: Loss = nan\n",
      "Iteration 134000 [13400000%]: Loss = nan\n",
      "Iteration 134100 [13410000%]: Loss = nan\n",
      "Iteration 134200 [13420000%]: Loss = nan\n",
      "Iteration 134300 [13430000%]: Loss = nan\n",
      "Iteration 134400 [13440000%]: Loss = nan\n",
      "Iteration 134500 [13450000%]: Loss = nan\n",
      "Iteration 134600 [13460000%]: Loss = nan\n",
      "Iteration 134700 [13470000%]: Loss = nan\n",
      "Iteration 134800 [13480000%]: Loss = nan\n",
      "Iteration 134900 [13490000%]: Loss = nan\n",
      "Iteration 135000 [13500000%]: Loss = nan\n",
      "Iteration 135100 [13510000%]: Loss = nan\n",
      "Iteration 135200 [13520000%]: Loss = nan\n",
      "Iteration 135300 [13530000%]: Loss = nan\n",
      "Iteration 135400 [13540000%]: Loss = nan\n",
      "Iteration 135500 [13550000%]: Loss = nan\n",
      "Iteration 135600 [13560000%]: Loss = nan\n",
      "Iteration 135700 [13570000%]: Loss = nan\n",
      "Iteration 135800 [13580000%]: Loss = nan\n",
      "Iteration 135900 [13590000%]: Loss = nan\n",
      "Iteration 136000 [13600000%]: Loss = nan\n",
      "Iteration 136100 [13610000%]: Loss = nan\n",
      "Iteration 136200 [13620000%]: Loss = nan\n",
      "Iteration 136300 [13630000%]: Loss = nan\n",
      "Iteration 136400 [13640000%]: Loss = nan\n",
      "Iteration 136500 [13650000%]: Loss = nan\n",
      "Iteration 136600 [13660000%]: Loss = nan\n",
      "Iteration 136700 [13670000%]: Loss = nan\n",
      "Iteration 136800 [13680000%]: Loss = nan\n",
      "Iteration 136900 [13690000%]: Loss = nan\n",
      "Iteration 137000 [13700000%]: Loss = nan\n",
      "Iteration 137100 [13710000%]: Loss = nan\n",
      "Iteration 137200 [13720000%]: Loss = nan\n",
      "Iteration 137300 [13730000%]: Loss = nan\n",
      "Iteration 137400 [13740000%]: Loss = nan\n",
      "Iteration 137500 [13750000%]: Loss = nan\n",
      "Iteration 137600 [13760000%]: Loss = nan\n",
      "Iteration 137700 [13770000%]: Loss = nan\n",
      "Iteration 137800 [13780000%]: Loss = nan\n",
      "Iteration 137900 [13790000%]: Loss = nan\n",
      "Iteration 138000 [13800000%]: Loss = nan\n",
      "Iteration 138100 [13810000%]: Loss = nan\n",
      "Iteration 138200 [13820000%]: Loss = nan\n",
      "Iteration 138300 [13830000%]: Loss = nan\n",
      "Iteration 138400 [13840000%]: Loss = nan\n",
      "Iteration 138500 [13850000%]: Loss = nan\n",
      "Iteration 138600 [13860000%]: Loss = nan\n",
      "Iteration 138700 [13870000%]: Loss = nan\n",
      "Iteration 138800 [13880000%]: Loss = nan\n",
      "Iteration 138900 [13890000%]: Loss = nan\n",
      "Iteration 139000 [13900000%]: Loss = nan\n",
      "Iteration 139100 [13910000%]: Loss = nan\n",
      "Iteration 139200 [13920000%]: Loss = nan\n",
      "Iteration 139300 [13930000%]: Loss = nan\n",
      "Iteration 139400 [13940000%]: Loss = nan\n",
      "Iteration 139500 [13950000%]: Loss = nan\n",
      "Iteration 139600 [13960000%]: Loss = nan\n",
      "Iteration 139700 [13970000%]: Loss = nan\n",
      "Iteration 139800 [13980000%]: Loss = nan\n",
      "Iteration 139900 [13990000%]: Loss = nan\n",
      "Iteration 140000 [14000000%]: Loss = nan\n",
      "Iteration 140100 [14010000%]: Loss = nan\n",
      "Iteration 140200 [14020000%]: Loss = nan\n",
      "Iteration 140300 [14030000%]: Loss = nan\n",
      "Iteration 140400 [14040000%]: Loss = nan\n",
      "Iteration 140500 [14050000%]: Loss = nan\n",
      "Iteration 140600 [14060000%]: Loss = nan\n",
      "Iteration 140700 [14070000%]: Loss = nan\n",
      "Iteration 140800 [14080000%]: Loss = nan\n",
      "Iteration 140900 [14090000%]: Loss = nan\n",
      "Iteration 141000 [14100000%]: Loss = nan\n",
      "Iteration 141100 [14110000%]: Loss = nan\n",
      "Iteration 141200 [14120000%]: Loss = nan\n",
      "Iteration 141300 [14130000%]: Loss = nan\n",
      "Iteration 141400 [14140000%]: Loss = nan\n",
      "Iteration 141500 [14150000%]: Loss = nan\n",
      "Iteration 141600 [14160000%]: Loss = nan\n",
      "Iteration 141700 [14170000%]: Loss = nan\n",
      "Iteration 141800 [14180000%]: Loss = nan\n",
      "Iteration 141900 [14190000%]: Loss = nan\n",
      "Iteration 142000 [14200000%]: Loss = nan\n",
      "Iteration 142100 [14210000%]: Loss = nan\n",
      "Iteration 142200 [14220000%]: Loss = nan\n",
      "Iteration 142300 [14230000%]: Loss = nan\n",
      "Iteration 142400 [14240000%]: Loss = nan\n",
      "Iteration 142500 [14250000%]: Loss = nan\n",
      "Iteration 142600 [14260000%]: Loss = nan\n",
      "Iteration 142700 [14270000%]: Loss = nan\n",
      "Iteration 142800 [14280000%]: Loss = nan\n",
      "Iteration 142900 [14290000%]: Loss = nan\n",
      "Iteration 143000 [14300000%]: Loss = nan\n",
      "Iteration 143100 [14310000%]: Loss = nan\n",
      "Iteration 143200 [14320000%]: Loss = nan\n",
      "Iteration 143300 [14330000%]: Loss = nan\n",
      "Iteration 143400 [14340000%]: Loss = nan\n",
      "Iteration 143500 [14350000%]: Loss = nan\n",
      "Iteration 143600 [14360000%]: Loss = nan\n",
      "Iteration 143700 [14370000%]: Loss = nan\n",
      "Iteration 143800 [14380000%]: Loss = nan\n",
      "Iteration 143900 [14390000%]: Loss = nan\n",
      "Iteration 144000 [14400000%]: Loss = nan\n",
      "Iteration 144100 [14410000%]: Loss = nan\n",
      "Iteration 144200 [14420000%]: Loss = nan\n",
      "Iteration 144300 [14430000%]: Loss = nan\n",
      "Iteration 144400 [14440000%]: Loss = nan\n",
      "Iteration 144500 [14450000%]: Loss = nan\n",
      "Iteration 144600 [14460000%]: Loss = nan\n",
      "Iteration 144700 [14470000%]: Loss = nan\n",
      "Iteration 144800 [14480000%]: Loss = nan\n",
      "Iteration 144900 [14490000%]: Loss = nan\n",
      "Iteration 145000 [14500000%]: Loss = nan\n",
      "Iteration 145100 [14510000%]: Loss = nan\n",
      "Iteration 145200 [14520000%]: Loss = nan\n",
      "Iteration 145300 [14530000%]: Loss = nan\n",
      "Iteration 145400 [14540000%]: Loss = nan\n",
      "Iteration 145500 [14550000%]: Loss = nan\n",
      "Iteration 145600 [14560000%]: Loss = nan\n",
      "Iteration 145700 [14570000%]: Loss = nan\n",
      "Iteration 145800 [14580000%]: Loss = nan\n",
      "Iteration 145900 [14590000%]: Loss = nan\n",
      "Iteration 146000 [14600000%]: Loss = nan\n",
      "Iteration 146100 [14610000%]: Loss = nan\n",
      "Iteration 146200 [14620000%]: Loss = nan\n",
      "Iteration 146300 [14630000%]: Loss = nan\n",
      "Iteration 146400 [14640000%]: Loss = nan\n",
      "Iteration 146500 [14650000%]: Loss = nan\n",
      "Iteration 146600 [14660000%]: Loss = nan\n",
      "Iteration 146700 [14670000%]: Loss = nan\n",
      "Iteration 146800 [14680000%]: Loss = nan\n",
      "Iteration 146900 [14690000%]: Loss = nan\n",
      "Iteration 147000 [14700000%]: Loss = nan\n",
      "Iteration 147100 [14710000%]: Loss = nan\n",
      "Iteration 147200 [14720000%]: Loss = nan\n",
      "Iteration 147300 [14730000%]: Loss = nan\n",
      "Iteration 147400 [14740000%]: Loss = nan\n",
      "Iteration 147500 [14750000%]: Loss = nan\n",
      "Iteration 147600 [14760000%]: Loss = nan\n",
      "Iteration 147700 [14770000%]: Loss = nan\n",
      "Iteration 147800 [14780000%]: Loss = nan\n",
      "Iteration 147900 [14790000%]: Loss = nan\n",
      "Iteration 148000 [14800000%]: Loss = nan\n",
      "Iteration 148100 [14810000%]: Loss = nan\n",
      "Iteration 148200 [14820000%]: Loss = nan\n",
      "Iteration 148300 [14830000%]: Loss = nan\n",
      "Iteration 148400 [14840000%]: Loss = nan\n",
      "Iteration 148500 [14850000%]: Loss = nan\n",
      "Iteration 148600 [14860000%]: Loss = nan\n",
      "Iteration 148700 [14870000%]: Loss = nan\n",
      "Iteration 148800 [14880000%]: Loss = nan\n",
      "Iteration 148900 [14890000%]: Loss = nan\n",
      "Iteration 149000 [14900000%]: Loss = nan\n",
      "Iteration 149100 [14910000%]: Loss = nan\n",
      "Iteration 149200 [14920000%]: Loss = nan\n",
      "Iteration 149300 [14930000%]: Loss = nan\n",
      "Iteration 149400 [14940000%]: Loss = nan\n",
      "Iteration 149500 [14950000%]: Loss = nan\n",
      "Iteration 149600 [14960000%]: Loss = nan\n",
      "Iteration 149700 [14970000%]: Loss = nan\n",
      "Iteration 149800 [14980000%]: Loss = nan\n",
      "Iteration 149900 [14990000%]: Loss = nan\n",
      "Iteration 150000 [15000000%]: Loss = nan\n",
      "Iteration 150100 [15010000%]: Loss = nan\n",
      "Iteration 150200 [15020000%]: Loss = nan\n",
      "Iteration 150300 [15030000%]: Loss = nan\n",
      "Iteration 150400 [15040000%]: Loss = nan\n",
      "Iteration 150500 [15050000%]: Loss = nan\n",
      "Iteration 150600 [15060000%]: Loss = nan\n",
      "Iteration 150700 [15070000%]: Loss = nan\n",
      "Iteration 150800 [15080000%]: Loss = nan\n",
      "Iteration 150900 [15090000%]: Loss = nan\n",
      "Iteration 151000 [15100000%]: Loss = nan\n",
      "Iteration 151100 [15110000%]: Loss = nan\n",
      "Iteration 151200 [15120000%]: Loss = nan\n",
      "Iteration 151300 [15130000%]: Loss = nan\n",
      "Iteration 151400 [15140000%]: Loss = nan\n",
      "Iteration 151500 [15150000%]: Loss = nan\n",
      "Iteration 151600 [15160000%]: Loss = nan\n",
      "Iteration 151700 [15170000%]: Loss = nan\n",
      "Iteration 151800 [15180000%]: Loss = nan\n",
      "Iteration 151900 [15190000%]: Loss = nan\n",
      "Iteration 152000 [15200000%]: Loss = nan\n",
      "Iteration 152100 [15210000%]: Loss = nan\n",
      "Iteration 152200 [15220000%]: Loss = nan\n",
      "Iteration 152300 [15230000%]: Loss = nan\n",
      "Iteration 152400 [15240000%]: Loss = nan\n",
      "Iteration 152500 [15250000%]: Loss = nan\n",
      "Iteration 152600 [15260000%]: Loss = nan\n",
      "Iteration 152700 [15270000%]: Loss = nan\n",
      "Iteration 152800 [15280000%]: Loss = nan\n",
      "Iteration 152900 [15290000%]: Loss = nan\n",
      "Iteration 153000 [15300000%]: Loss = nan\n",
      "Iteration 153100 [15310000%]: Loss = nan\n",
      "Iteration 153200 [15320000%]: Loss = nan\n",
      "Iteration 153300 [15330000%]: Loss = nan\n",
      "Iteration 153400 [15340000%]: Loss = nan\n",
      "Iteration 153500 [15350000%]: Loss = nan\n",
      "Iteration 153600 [15360000%]: Loss = nan\n",
      "Iteration 153700 [15370000%]: Loss = nan\n",
      "Iteration 153800 [15380000%]: Loss = nan\n",
      "Iteration 153900 [15390000%]: Loss = nan\n",
      "Iteration 154000 [15400000%]: Loss = nan\n",
      "Iteration 154100 [15410000%]: Loss = nan\n",
      "Iteration 154200 [15420000%]: Loss = nan\n",
      "Iteration 154300 [15430000%]: Loss = nan\n",
      "Iteration 154400 [15440000%]: Loss = nan\n",
      "Iteration 154500 [15450000%]: Loss = nan\n",
      "Iteration 154600 [15460000%]: Loss = nan\n",
      "Iteration 154700 [15470000%]: Loss = nan\n",
      "Iteration 154800 [15480000%]: Loss = nan\n",
      "Iteration 154900 [15490000%]: Loss = nan\n",
      "Iteration 155000 [15500000%]: Loss = nan\n",
      "Iteration 155100 [15510000%]: Loss = nan\n",
      "Iteration 155200 [15520000%]: Loss = nan\n",
      "Iteration 155300 [15530000%]: Loss = nan\n",
      "Iteration 155400 [15540000%]: Loss = nan\n",
      "Iteration 155500 [15550000%]: Loss = nan\n",
      "Iteration 155600 [15560000%]: Loss = nan\n",
      "Iteration 155700 [15570000%]: Loss = nan\n",
      "Iteration 155800 [15580000%]: Loss = nan\n",
      "Iteration 155900 [15590000%]: Loss = nan\n",
      "Iteration 156000 [15600000%]: Loss = nan\n",
      "Iteration 156100 [15610000%]: Loss = nan\n",
      "Iteration 156200 [15620000%]: Loss = nan\n",
      "Iteration 156300 [15630000%]: Loss = nan\n",
      "Iteration 156400 [15640000%]: Loss = nan\n",
      "Iteration 156500 [15650000%]: Loss = nan\n",
      "Iteration 156600 [15660000%]: Loss = nan\n",
      "Iteration 156700 [15670000%]: Loss = nan\n",
      "Iteration 156800 [15680000%]: Loss = nan\n",
      "Iteration 156900 [15690000%]: Loss = nan\n",
      "Iteration 157000 [15700000%]: Loss = nan\n",
      "Iteration 157100 [15710000%]: Loss = nan\n",
      "Iteration 157200 [15720000%]: Loss = nan\n",
      "Iteration 157300 [15730000%]: Loss = nan\n",
      "Iteration 157400 [15740000%]: Loss = nan\n",
      "Iteration 157500 [15750000%]: Loss = nan\n",
      "Iteration 157600 [15760000%]: Loss = nan\n",
      "Iteration 157700 [15770000%]: Loss = nan\n",
      "Iteration 157800 [15780000%]: Loss = nan\n",
      "Iteration 157900 [15790000%]: Loss = nan\n",
      "Iteration 158000 [15800000%]: Loss = nan\n",
      "Iteration 158100 [15810000%]: Loss = nan\n",
      "Iteration 158200 [15820000%]: Loss = nan\n",
      "Iteration 158300 [15830000%]: Loss = nan\n",
      "Iteration 158400 [15840000%]: Loss = nan\n",
      "Iteration 158500 [15850000%]: Loss = nan\n",
      "Iteration 158600 [15860000%]: Loss = nan\n",
      "Iteration 158700 [15870000%]: Loss = nan\n",
      "Iteration 158800 [15880000%]: Loss = nan\n",
      "Iteration 158900 [15890000%]: Loss = nan\n",
      "Iteration 159000 [15900000%]: Loss = nan\n",
      "Iteration 159100 [15910000%]: Loss = nan\n",
      "Iteration 159200 [15920000%]: Loss = nan\n",
      "Iteration 159300 [15930000%]: Loss = nan\n",
      "Iteration 159400 [15940000%]: Loss = nan\n",
      "Iteration 159500 [15950000%]: Loss = nan\n",
      "Iteration 159600 [15960000%]: Loss = nan\n",
      "Iteration 159700 [15970000%]: Loss = nan\n",
      "Iteration 159800 [15980000%]: Loss = nan\n",
      "Iteration 159900 [15990000%]: Loss = nan\n",
      "Iteration 160000 [16000000%]: Loss = nan\n",
      "Iteration 160100 [16010000%]: Loss = nan\n",
      "Iteration 160200 [16020000%]: Loss = nan\n",
      "Iteration 160300 [16030000%]: Loss = nan\n",
      "Iteration 160400 [16040000%]: Loss = nan\n",
      "Iteration 160500 [16050000%]: Loss = nan\n",
      "Iteration 160600 [16060000%]: Loss = nan\n",
      "Iteration 160700 [16070000%]: Loss = nan\n",
      "Iteration 160800 [16080000%]: Loss = nan\n",
      "Iteration 160900 [16090000%]: Loss = nan\n",
      "Iteration 161000 [16100000%]: Loss = nan\n",
      "Iteration 161100 [16110000%]: Loss = nan\n",
      "Iteration 161200 [16120000%]: Loss = nan\n",
      "Iteration 161300 [16130000%]: Loss = nan\n",
      "Iteration 161400 [16140000%]: Loss = nan\n",
      "Iteration 161500 [16150000%]: Loss = nan\n",
      "Iteration 161600 [16160000%]: Loss = nan\n",
      "Iteration 161700 [16170000%]: Loss = nan\n",
      "Iteration 161800 [16180000%]: Loss = nan\n",
      "Iteration 161900 [16190000%]: Loss = nan\n",
      "Iteration 162000 [16200000%]: Loss = nan\n",
      "Iteration 162100 [16210000%]: Loss = nan\n",
      "Iteration 162200 [16220000%]: Loss = nan\n",
      "Iteration 162300 [16230000%]: Loss = nan\n",
      "Iteration 162400 [16240000%]: Loss = nan\n",
      "Iteration 162500 [16250000%]: Loss = nan\n",
      "Iteration 162600 [16260000%]: Loss = nan\n",
      "Iteration 162700 [16270000%]: Loss = nan\n",
      "Iteration 162800 [16280000%]: Loss = nan\n",
      "Iteration 162900 [16290000%]: Loss = nan\n",
      "Iteration 163000 [16300000%]: Loss = nan\n",
      "Iteration 163100 [16310000%]: Loss = nan\n",
      "Iteration 163200 [16320000%]: Loss = nan\n",
      "Iteration 163300 [16330000%]: Loss = nan\n",
      "Iteration 163400 [16340000%]: Loss = nan\n",
      "Iteration 163500 [16350000%]: Loss = nan\n",
      "Iteration 163600 [16360000%]: Loss = nan\n",
      "Iteration 163700 [16370000%]: Loss = nan\n",
      "Iteration 163800 [16380000%]: Loss = nan\n",
      "Iteration 163900 [16390000%]: Loss = nan\n",
      "Iteration 164000 [16400000%]: Loss = nan\n",
      "Iteration 164100 [16410000%]: Loss = nan\n",
      "Iteration 164200 [16420000%]: Loss = nan\n",
      "Iteration 164300 [16430000%]: Loss = nan\n",
      "Iteration 164400 [16440000%]: Loss = nan\n",
      "Iteration 164500 [16450000%]: Loss = nan\n",
      "Iteration 164600 [16460000%]: Loss = nan\n",
      "Iteration 164700 [16470000%]: Loss = nan\n",
      "Iteration 164800 [16480000%]: Loss = nan\n",
      "Iteration 164900 [16490000%]: Loss = nan\n",
      "Iteration 165000 [16500000%]: Loss = nan\n",
      "Iteration 165100 [16510000%]: Loss = nan\n",
      "Iteration 165200 [16520000%]: Loss = nan\n",
      "Iteration 165300 [16530000%]: Loss = nan\n",
      "Iteration 165400 [16540000%]: Loss = nan\n",
      "Iteration 165500 [16550000%]: Loss = nan\n",
      "Iteration 165600 [16560000%]: Loss = nan\n",
      "Iteration 165700 [16570000%]: Loss = nan\n",
      "Iteration 165800 [16580000%]: Loss = nan\n",
      "Iteration 165900 [16590000%]: Loss = nan\n",
      "Iteration 166000 [16600000%]: Loss = nan\n",
      "Iteration 166100 [16610000%]: Loss = nan\n",
      "Iteration 166200 [16620000%]: Loss = nan\n",
      "Iteration 166300 [16630000%]: Loss = nan\n",
      "Iteration 166400 [16640000%]: Loss = nan\n",
      "Iteration 166500 [16650000%]: Loss = nan\n",
      "Iteration 166600 [16660000%]: Loss = nan\n",
      "Iteration 166700 [16670000%]: Loss = nan\n",
      "Iteration 166800 [16680000%]: Loss = nan\n",
      "Iteration 166900 [16690000%]: Loss = nan\n",
      "Iteration 167000 [16700000%]: Loss = nan\n",
      "Iteration 167100 [16710000%]: Loss = nan\n",
      "Iteration 167200 [16720000%]: Loss = nan\n",
      "Iteration 167300 [16730000%]: Loss = nan\n",
      "Iteration 167400 [16740000%]: Loss = nan\n",
      "Iteration 167500 [16750000%]: Loss = nan\n",
      "Iteration 167600 [16760000%]: Loss = nan\n",
      "Iteration 167700 [16770000%]: Loss = nan\n",
      "Iteration 167800 [16780000%]: Loss = nan\n",
      "Iteration 167900 [16790000%]: Loss = nan\n",
      "Iteration 168000 [16800000%]: Loss = nan\n",
      "Iteration 168100 [16810000%]: Loss = nan\n",
      "Iteration 168200 [16820000%]: Loss = nan\n",
      "Iteration 168300 [16830000%]: Loss = nan\n",
      "Iteration 168400 [16840000%]: Loss = nan\n",
      "Iteration 168500 [16850000%]: Loss = nan\n",
      "Iteration 168600 [16860000%]: Loss = nan\n",
      "Iteration 168700 [16870000%]: Loss = nan\n",
      "Iteration 168800 [16880000%]: Loss = nan\n",
      "Iteration 168900 [16890000%]: Loss = nan\n",
      "Iteration 169000 [16900000%]: Loss = nan\n",
      "Iteration 169100 [16910000%]: Loss = nan\n",
      "Iteration 169200 [16920000%]: Loss = nan\n",
      "Iteration 169300 [16930000%]: Loss = nan\n",
      "Iteration 169400 [16940000%]: Loss = nan\n",
      "Iteration 169500 [16950000%]: Loss = nan\n",
      "Iteration 169600 [16960000%]: Loss = nan\n",
      "Iteration 169700 [16970000%]: Loss = nan\n",
      "Iteration 169800 [16980000%]: Loss = nan\n",
      "Iteration 169900 [16990000%]: Loss = nan\n",
      "Iteration 170000 [17000000%]: Loss = nan\n",
      "Iteration 170100 [17010000%]: Loss = nan\n",
      "Iteration 170200 [17020000%]: Loss = nan\n",
      "Iteration 170300 [17030000%]: Loss = nan\n",
      "Iteration 170400 [17040000%]: Loss = nan\n",
      "Iteration 170500 [17050000%]: Loss = nan\n",
      "Iteration 170600 [17060000%]: Loss = nan\n",
      "Iteration 170700 [17070000%]: Loss = nan\n",
      "Iteration 170800 [17080000%]: Loss = nan\n",
      "Iteration 170900 [17090000%]: Loss = nan\n",
      "Iteration 171000 [17100000%]: Loss = nan\n",
      "Iteration 171100 [17110000%]: Loss = nan\n",
      "Iteration 171200 [17120000%]: Loss = nan\n",
      "Iteration 171300 [17130000%]: Loss = nan\n",
      "Iteration 171400 [17140000%]: Loss = nan\n",
      "Iteration 171500 [17150000%]: Loss = nan\n",
      "Iteration 171600 [17160000%]: Loss = nan\n",
      "Iteration 171700 [17170000%]: Loss = nan\n",
      "Iteration 171800 [17180000%]: Loss = nan\n",
      "Iteration 171900 [17190000%]: Loss = nan\n",
      "Iteration 172000 [17200000%]: Loss = nan\n",
      "Iteration 172100 [17210000%]: Loss = nan\n",
      "Iteration 172200 [17220000%]: Loss = nan\n",
      "Iteration 172300 [17230000%]: Loss = nan\n",
      "Iteration 172400 [17240000%]: Loss = nan\n",
      "Iteration 172500 [17250000%]: Loss = nan\n",
      "Iteration 172600 [17260000%]: Loss = nan\n",
      "Iteration 172700 [17270000%]: Loss = nan\n",
      "Iteration 172800 [17280000%]: Loss = nan\n",
      "Iteration 172900 [17290000%]: Loss = nan\n",
      "Iteration 173000 [17300000%]: Loss = nan\n",
      "Iteration 173100 [17310000%]: Loss = nan\n",
      "Iteration 173200 [17320000%]: Loss = nan\n",
      "Iteration 173300 [17330000%]: Loss = nan\n",
      "Iteration 173400 [17340000%]: Loss = nan\n",
      "Iteration 173500 [17350000%]: Loss = nan\n",
      "Iteration 173600 [17360000%]: Loss = nan\n",
      "Iteration 173700 [17370000%]: Loss = nan\n",
      "Iteration 173800 [17380000%]: Loss = nan\n",
      "Iteration 173900 [17390000%]: Loss = nan\n",
      "Iteration 174000 [17400000%]: Loss = nan\n",
      "Iteration 174100 [17410000%]: Loss = nan\n",
      "Iteration 174200 [17420000%]: Loss = nan\n",
      "Iteration 174300 [17430000%]: Loss = nan\n",
      "Iteration 174400 [17440000%]: Loss = nan\n",
      "Iteration 174500 [17450000%]: Loss = nan\n",
      "Iteration 174600 [17460000%]: Loss = nan\n",
      "Iteration 174700 [17470000%]: Loss = nan\n",
      "Iteration 174800 [17480000%]: Loss = nan\n",
      "Iteration 174900 [17490000%]: Loss = nan\n",
      "Iteration 175000 [17500000%]: Loss = nan\n",
      "Iteration 175100 [17510000%]: Loss = nan\n",
      "Iteration 175200 [17520000%]: Loss = nan\n",
      "Iteration 175300 [17530000%]: Loss = nan\n",
      "Iteration 175400 [17540000%]: Loss = nan\n",
      "Iteration 175500 [17550000%]: Loss = nan\n",
      "Iteration 175600 [17560000%]: Loss = nan\n",
      "Iteration 175700 [17570000%]: Loss = nan\n",
      "Iteration 175800 [17580000%]: Loss = nan\n",
      "Iteration 175900 [17590000%]: Loss = nan\n",
      "Iteration 176000 [17600000%]: Loss = nan\n",
      "Iteration 176100 [17610000%]: Loss = nan\n",
      "Iteration 176200 [17620000%]: Loss = nan\n",
      "Iteration 176300 [17630000%]: Loss = nan\n",
      "Iteration 176400 [17640000%]: Loss = nan\n",
      "Iteration 176500 [17650000%]: Loss = nan\n",
      "Iteration 176600 [17660000%]: Loss = nan\n",
      "Iteration 176700 [17670000%]: Loss = nan\n",
      "Iteration 176800 [17680000%]: Loss = nan\n",
      "Iteration 176900 [17690000%]: Loss = nan\n",
      "Iteration 177000 [17700000%]: Loss = nan\n",
      "Iteration 177100 [17710000%]: Loss = nan\n",
      "Iteration 177200 [17720000%]: Loss = nan\n",
      "Iteration 177300 [17730000%]: Loss = nan\n",
      "Iteration 177400 [17740000%]: Loss = nan\n",
      "Iteration 177500 [17750000%]: Loss = nan\n",
      "Iteration 177600 [17760000%]: Loss = nan\n",
      "Iteration 177700 [17770000%]: Loss = nan\n",
      "Iteration 177800 [17780000%]: Loss = nan\n",
      "Iteration 177900 [17790000%]: Loss = nan\n",
      "Iteration 178000 [17800000%]: Loss = nan\n",
      "Iteration 178100 [17810000%]: Loss = nan\n",
      "Iteration 178200 [17820000%]: Loss = nan\n",
      "Iteration 178300 [17830000%]: Loss = nan\n",
      "Iteration 178400 [17840000%]: Loss = nan\n",
      "Iteration 178500 [17850000%]: Loss = nan\n",
      "Iteration 178600 [17860000%]: Loss = nan\n",
      "Iteration 178700 [17870000%]: Loss = nan\n",
      "Iteration 178800 [17880000%]: Loss = nan\n",
      "Iteration 178900 [17890000%]: Loss = nan\n",
      "Iteration 179000 [17900000%]: Loss = nan\n",
      "Iteration 179100 [17910000%]: Loss = nan\n",
      "Iteration 179200 [17920000%]: Loss = nan\n",
      "Iteration 179300 [17930000%]: Loss = nan\n",
      "Iteration 179400 [17940000%]: Loss = nan\n",
      "Iteration 179500 [17950000%]: Loss = nan\n",
      "Iteration 179600 [17960000%]: Loss = nan\n",
      "Iteration 179700 [17970000%]: Loss = nan\n",
      "Iteration 179800 [17980000%]: Loss = nan\n",
      "Iteration 179900 [17990000%]: Loss = nan\n",
      "Iteration 180000 [18000000%]: Loss = nan\n",
      "Iteration 180100 [18010000%]: Loss = nan\n",
      "Iteration 180200 [18020000%]: Loss = nan\n",
      "Iteration 180300 [18030000%]: Loss = nan\n",
      "Iteration 180400 [18040000%]: Loss = nan\n",
      "Iteration 180500 [18050000%]: Loss = nan\n",
      "Iteration 180600 [18060000%]: Loss = nan\n",
      "Iteration 180700 [18070000%]: Loss = nan\n",
      "Iteration 180800 [18080000%]: Loss = nan\n",
      "Iteration 180900 [18090000%]: Loss = nan\n",
      "Iteration 181000 [18100000%]: Loss = nan\n",
      "Iteration 181100 [18110000%]: Loss = nan\n",
      "Iteration 181200 [18120000%]: Loss = nan\n",
      "Iteration 181300 [18130000%]: Loss = nan\n",
      "Iteration 181400 [18140000%]: Loss = nan\n",
      "Iteration 181500 [18150000%]: Loss = nan\n",
      "Iteration 181600 [18160000%]: Loss = nan\n",
      "Iteration 181700 [18170000%]: Loss = nan\n",
      "Iteration 181800 [18180000%]: Loss = nan\n",
      "Iteration 181900 [18190000%]: Loss = nan\n",
      "Iteration 182000 [18200000%]: Loss = nan\n",
      "Iteration 182100 [18210000%]: Loss = nan\n",
      "Iteration 182200 [18220000%]: Loss = nan\n",
      "Iteration 182300 [18230000%]: Loss = nan\n",
      "Iteration 182400 [18240000%]: Loss = nan\n",
      "Iteration 182500 [18250000%]: Loss = nan\n",
      "Iteration 182600 [18260000%]: Loss = nan\n",
      "Iteration 182700 [18270000%]: Loss = nan\n",
      "Iteration 182800 [18280000%]: Loss = nan\n",
      "Iteration 182900 [18290000%]: Loss = nan\n",
      "Iteration 183000 [18300000%]: Loss = nan\n",
      "Iteration 183100 [18310000%]: Loss = nan\n",
      "Iteration 183200 [18320000%]: Loss = nan\n",
      "Iteration 183300 [18330000%]: Loss = nan\n",
      "Iteration 183400 [18340000%]: Loss = nan\n",
      "Iteration 183500 [18350000%]: Loss = nan\n",
      "Iteration 183600 [18360000%]: Loss = nan\n",
      "Iteration 183700 [18370000%]: Loss = nan\n",
      "Iteration 183800 [18380000%]: Loss = nan\n",
      "Iteration 183900 [18390000%]: Loss = nan\n",
      "Iteration 184000 [18400000%]: Loss = nan\n",
      "Iteration 184100 [18410000%]: Loss = nan\n",
      "Iteration 184200 [18420000%]: Loss = nan\n",
      "Iteration 184300 [18430000%]: Loss = nan\n",
      "Iteration 184400 [18440000%]: Loss = nan\n",
      "Iteration 184500 [18450000%]: Loss = nan\n",
      "Iteration 184600 [18460000%]: Loss = nan\n",
      "Iteration 184700 [18470000%]: Loss = nan\n",
      "Iteration 184800 [18480000%]: Loss = nan\n",
      "Iteration 184900 [18490000%]: Loss = nan\n",
      "Iteration 185000 [18500000%]: Loss = nan\n",
      "Iteration 185100 [18510000%]: Loss = nan\n",
      "Iteration 185200 [18520000%]: Loss = nan\n",
      "Iteration 185300 [18530000%]: Loss = nan\n",
      "Iteration 185400 [18540000%]: Loss = nan\n",
      "Iteration 185500 [18550000%]: Loss = nan\n",
      "Iteration 185600 [18560000%]: Loss = nan\n",
      "Iteration 185700 [18570000%]: Loss = nan\n",
      "Iteration 185800 [18580000%]: Loss = nan\n",
      "Iteration 185900 [18590000%]: Loss = nan\n",
      "Iteration 186000 [18600000%]: Loss = nan\n",
      "Iteration 186100 [18610000%]: Loss = nan\n",
      "Iteration 186200 [18620000%]: Loss = nan\n",
      "Iteration 186300 [18630000%]: Loss = nan\n",
      "Iteration 186400 [18640000%]: Loss = nan\n",
      "Iteration 186500 [18650000%]: Loss = nan\n",
      "Iteration 186600 [18660000%]: Loss = nan\n",
      "Iteration 186700 [18670000%]: Loss = nan\n",
      "Iteration 186800 [18680000%]: Loss = nan\n",
      "Iteration 186900 [18690000%]: Loss = nan\n",
      "Iteration 187000 [18700000%]: Loss = nan\n",
      "Iteration 187100 [18710000%]: Loss = nan\n",
      "Iteration 187200 [18720000%]: Loss = nan\n",
      "Iteration 187300 [18730000%]: Loss = nan\n",
      "Iteration 187400 [18740000%]: Loss = nan\n",
      "Iteration 187500 [18750000%]: Loss = nan\n",
      "Iteration 187600 [18760000%]: Loss = nan\n",
      "Iteration 187700 [18770000%]: Loss = nan\n",
      "Iteration 187800 [18780000%]: Loss = nan\n",
      "Iteration 187900 [18790000%]: Loss = nan\n",
      "Iteration 188000 [18800000%]: Loss = nan\n",
      "Iteration 188100 [18810000%]: Loss = nan\n",
      "Iteration 188200 [18820000%]: Loss = nan\n",
      "Iteration 188300 [18830000%]: Loss = nan\n",
      "Iteration 188400 [18840000%]: Loss = nan\n",
      "Iteration 188500 [18850000%]: Loss = nan\n",
      "Iteration 188600 [18860000%]: Loss = nan\n",
      "Iteration 188700 [18870000%]: Loss = nan\n",
      "Iteration 188800 [18880000%]: Loss = nan\n",
      "Iteration 188900 [18890000%]: Loss = nan\n",
      "Iteration 189000 [18900000%]: Loss = nan\n",
      "Iteration 189100 [18910000%]: Loss = nan\n",
      "Iteration 189200 [18920000%]: Loss = nan\n",
      "Iteration 189300 [18930000%]: Loss = nan\n",
      "Iteration 189400 [18940000%]: Loss = nan\n",
      "Iteration 189500 [18950000%]: Loss = nan\n",
      "Iteration 189600 [18960000%]: Loss = nan\n",
      "Iteration 189700 [18970000%]: Loss = nan\n",
      "Iteration 189800 [18980000%]: Loss = nan\n",
      "Iteration 189900 [18990000%]: Loss = nan\n",
      "Iteration 190000 [19000000%]: Loss = nan\n",
      "Iteration 190100 [19010000%]: Loss = nan\n",
      "Iteration 190200 [19020000%]: Loss = nan\n",
      "Iteration 190300 [19030000%]: Loss = nan\n",
      "Iteration 190400 [19040000%]: Loss = nan\n",
      "Iteration 190500 [19050000%]: Loss = nan\n",
      "Iteration 190600 [19060000%]: Loss = nan\n",
      "Iteration 190700 [19070000%]: Loss = nan\n",
      "Iteration 190800 [19080000%]: Loss = nan\n",
      "Iteration 190900 [19090000%]: Loss = nan\n",
      "Iteration 191000 [19100000%]: Loss = nan\n",
      "Iteration 191100 [19110000%]: Loss = nan\n",
      "Iteration 191200 [19120000%]: Loss = nan\n",
      "Iteration 191300 [19130000%]: Loss = nan\n",
      "Iteration 191400 [19140000%]: Loss = nan\n",
      "Iteration 191500 [19150000%]: Loss = nan\n",
      "Iteration 191600 [19160000%]: Loss = nan\n",
      "Iteration 191700 [19170000%]: Loss = nan\n",
      "Iteration 191800 [19180000%]: Loss = nan\n",
      "Iteration 191900 [19190000%]: Loss = nan\n",
      "Iteration 192000 [19200000%]: Loss = nan\n",
      "Iteration 192100 [19210000%]: Loss = nan\n",
      "Iteration 192200 [19220000%]: Loss = nan\n",
      "Iteration 192300 [19230000%]: Loss = nan\n",
      "Iteration 192400 [19240000%]: Loss = nan\n",
      "Iteration 192500 [19250000%]: Loss = nan\n",
      "Iteration 192600 [19260000%]: Loss = nan\n",
      "Iteration 192700 [19270000%]: Loss = nan\n",
      "Iteration 192800 [19280000%]: Loss = nan\n",
      "Iteration 192900 [19290000%]: Loss = nan\n",
      "Iteration 193000 [19300000%]: Loss = nan\n",
      "Iteration 193100 [19310000%]: Loss = nan\n",
      "Iteration 193200 [19320000%]: Loss = nan\n",
      "Iteration 193300 [19330000%]: Loss = nan\n",
      "Iteration 193400 [19340000%]: Loss = nan\n",
      "Iteration 193500 [19350000%]: Loss = nan\n",
      "Iteration 193600 [19360000%]: Loss = nan\n",
      "Iteration 193700 [19370000%]: Loss = nan\n",
      "Iteration 193800 [19380000%]: Loss = nan\n",
      "Iteration 193900 [19390000%]: Loss = nan\n",
      "Iteration 194000 [19400000%]: Loss = nan\n",
      "Iteration 194100 [19410000%]: Loss = nan\n",
      "Iteration 194200 [19420000%]: Loss = nan\n",
      "Iteration 194300 [19430000%]: Loss = nan\n",
      "Iteration 194400 [19440000%]: Loss = nan\n",
      "Iteration 194500 [19450000%]: Loss = nan\n",
      "Iteration 194600 [19460000%]: Loss = nan\n",
      "Iteration 194700 [19470000%]: Loss = nan\n",
      "Iteration 194800 [19480000%]: Loss = nan\n",
      "Iteration 194900 [19490000%]: Loss = nan\n",
      "Iteration 195000 [19500000%]: Loss = nan\n",
      "Iteration 195100 [19510000%]: Loss = nan\n",
      "Iteration 195200 [19520000%]: Loss = nan\n",
      "Iteration 195300 [19530000%]: Loss = nan\n",
      "Iteration 195400 [19540000%]: Loss = nan\n",
      "Iteration 195500 [19550000%]: Loss = nan\n",
      "Iteration 195600 [19560000%]: Loss = nan\n",
      "Iteration 195700 [19570000%]: Loss = nan\n",
      "Iteration 195800 [19580000%]: Loss = nan\n",
      "Iteration 195900 [19590000%]: Loss = nan\n",
      "Iteration 196000 [19600000%]: Loss = nan\n",
      "Iteration 196100 [19610000%]: Loss = nan\n",
      "Iteration 196200 [19620000%]: Loss = nan\n",
      "Iteration 196300 [19630000%]: Loss = nan\n",
      "Iteration 196400 [19640000%]: Loss = nan\n",
      "Iteration 196500 [19650000%]: Loss = nan\n",
      "Iteration 196600 [19660000%]: Loss = nan\n",
      "Iteration 196700 [19670000%]: Loss = nan\n",
      "Iteration 196800 [19680000%]: Loss = nan\n",
      "Iteration 196900 [19690000%]: Loss = nan\n",
      "Iteration 197000 [19700000%]: Loss = nan\n",
      "Iteration 197100 [19710000%]: Loss = nan\n",
      "Iteration 197200 [19720000%]: Loss = nan\n",
      "Iteration 197300 [19730000%]: Loss = nan\n",
      "Iteration 197400 [19740000%]: Loss = nan\n",
      "Iteration 197500 [19750000%]: Loss = nan\n",
      "Iteration 197600 [19760000%]: Loss = nan\n",
      "Iteration 197700 [19770000%]: Loss = nan\n",
      "Iteration 197800 [19780000%]: Loss = nan\n",
      "Iteration 197900 [19790000%]: Loss = nan\n",
      "Iteration 198000 [19800000%]: Loss = nan\n",
      "Iteration 198100 [19810000%]: Loss = nan\n",
      "Iteration 198200 [19820000%]: Loss = nan\n",
      "Iteration 198300 [19830000%]: Loss = nan\n",
      "Iteration 198400 [19840000%]: Loss = nan\n",
      "Iteration 198500 [19850000%]: Loss = nan\n",
      "Iteration 198600 [19860000%]: Loss = nan\n",
      "Iteration 198700 [19870000%]: Loss = nan\n",
      "Iteration 198800 [19880000%]: Loss = nan\n",
      "Iteration 198900 [19890000%]: Loss = nan\n",
      "Iteration 199000 [19900000%]: Loss = nan\n",
      "Iteration 199100 [19910000%]: Loss = nan\n",
      "Iteration 199200 [19920000%]: Loss = nan\n",
      "Iteration 199300 [19930000%]: Loss = nan\n",
      "Iteration 199400 [19940000%]: Loss = nan\n",
      "Iteration 199500 [19950000%]: Loss = nan\n",
      "Iteration 199600 [19960000%]: Loss = nan\n",
      "Iteration 199700 [19970000%]: Loss = nan\n",
      "Iteration 199800 [19980000%]: Loss = nan\n",
      "Iteration 199900 [19990000%]: Loss = nan\n",
      "Iteration 200000 [20000000%]: Loss = nan\n",
      "Iteration 200100 [20010000%]: Loss = nan\n",
      "Iteration 200200 [20020000%]: Loss = nan\n",
      "Iteration 200300 [20030000%]: Loss = nan\n",
      "Iteration 200400 [20040000%]: Loss = nan\n",
      "Iteration 200500 [20050000%]: Loss = nan\n",
      "Iteration 200600 [20060000%]: Loss = nan\n",
      "Iteration 200700 [20070000%]: Loss = nan\n",
      "Iteration 200800 [20080000%]: Loss = nan\n",
      "Iteration 200900 [20090000%]: Loss = nan\n",
      "Iteration 201000 [20100000%]: Loss = nan\n",
      "Iteration 201100 [20110000%]: Loss = nan\n",
      "Iteration 201200 [20120000%]: Loss = nan\n",
      "Iteration 201300 [20130000%]: Loss = nan\n",
      "Iteration 201400 [20140000%]: Loss = nan\n",
      "Iteration 201500 [20150000%]: Loss = nan\n",
      "Iteration 201600 [20160000%]: Loss = nan\n",
      "Iteration 201700 [20170000%]: Loss = nan\n",
      "Iteration 201800 [20180000%]: Loss = nan\n",
      "Iteration 201900 [20190000%]: Loss = nan\n",
      "Iteration 202000 [20200000%]: Loss = nan\n",
      "Iteration 202100 [20210000%]: Loss = nan\n",
      "Iteration 202200 [20220000%]: Loss = nan\n",
      "Iteration 202300 [20230000%]: Loss = nan\n",
      "Iteration 202400 [20240000%]: Loss = nan\n",
      "Iteration 202500 [20250000%]: Loss = nan\n",
      "Iteration 202600 [20260000%]: Loss = nan\n",
      "Iteration 202700 [20270000%]: Loss = nan\n",
      "Iteration 202800 [20280000%]: Loss = nan\n",
      "Iteration 202900 [20290000%]: Loss = nan\n",
      "Iteration 203000 [20300000%]: Loss = nan\n",
      "Iteration 203100 [20310000%]: Loss = nan\n",
      "Iteration 203200 [20320000%]: Loss = nan\n",
      "Iteration 203300 [20330000%]: Loss = nan\n",
      "Iteration 203400 [20340000%]: Loss = nan\n",
      "Iteration 203500 [20350000%]: Loss = nan\n",
      "Iteration 203600 [20360000%]: Loss = nan\n",
      "Iteration 203700 [20370000%]: Loss = nan\n",
      "Iteration 203800 [20380000%]: Loss = nan\n",
      "Iteration 203900 [20390000%]: Loss = nan\n",
      "Iteration 204000 [20400000%]: Loss = nan\n",
      "Iteration 204100 [20410000%]: Loss = nan\n",
      "Iteration 204200 [20420000%]: Loss = nan\n",
      "Iteration 204300 [20430000%]: Loss = nan\n",
      "Iteration 204400 [20440000%]: Loss = nan\n",
      "Iteration 204500 [20450000%]: Loss = nan\n",
      "Iteration 204600 [20460000%]: Loss = nan\n",
      "Iteration 204700 [20470000%]: Loss = nan\n",
      "Iteration 204800 [20480000%]: Loss = nan\n",
      "Iteration 204900 [20490000%]: Loss = nan\n",
      "Iteration 205000 [20500000%]: Loss = nan\n",
      "Iteration 205100 [20510000%]: Loss = nan\n",
      "Iteration 205200 [20520000%]: Loss = nan\n",
      "Iteration 205300 [20530000%]: Loss = nan\n",
      "Iteration 205400 [20540000%]: Loss = nan\n",
      "Iteration 205500 [20550000%]: Loss = nan\n",
      "Iteration 205600 [20560000%]: Loss = nan\n",
      "Iteration 205700 [20570000%]: Loss = nan\n",
      "Iteration 205800 [20580000%]: Loss = nan\n",
      "Iteration 205900 [20590000%]: Loss = nan\n",
      "Iteration 206000 [20600000%]: Loss = nan\n",
      "Iteration 206100 [20610000%]: Loss = nan\n",
      "Iteration 206200 [20620000%]: Loss = nan\n",
      "Iteration 206300 [20630000%]: Loss = nan\n",
      "Iteration 206400 [20640000%]: Loss = nan\n",
      "Iteration 206500 [20650000%]: Loss = nan\n",
      "Iteration 206600 [20660000%]: Loss = nan\n",
      "Iteration 206700 [20670000%]: Loss = nan\n",
      "Iteration 206800 [20680000%]: Loss = nan\n",
      "Iteration 206900 [20690000%]: Loss = nan\n",
      "Iteration 207000 [20700000%]: Loss = nan\n",
      "Iteration 207100 [20710000%]: Loss = nan\n",
      "Iteration 207200 [20720000%]: Loss = nan\n",
      "Iteration 207300 [20730000%]: Loss = nan\n",
      "Iteration 207400 [20740000%]: Loss = nan\n",
      "Iteration 207500 [20750000%]: Loss = nan\n",
      "Iteration 207600 [20760000%]: Loss = nan\n",
      "Iteration 207700 [20770000%]: Loss = nan\n",
      "Iteration 207800 [20780000%]: Loss = nan\n",
      "Iteration 207900 [20790000%]: Loss = nan\n",
      "Iteration 208000 [20800000%]: Loss = nan\n",
      "Iteration 208100 [20810000%]: Loss = nan\n",
      "Iteration 208200 [20820000%]: Loss = nan\n",
      "Iteration 208300 [20830000%]: Loss = nan\n",
      "Iteration 208400 [20840000%]: Loss = nan\n",
      "Iteration 208500 [20850000%]: Loss = nan\n",
      "Iteration 208600 [20860000%]: Loss = nan\n",
      "Iteration 208700 [20870000%]: Loss = nan\n",
      "Iteration 208800 [20880000%]: Loss = nan\n",
      "Iteration 208900 [20890000%]: Loss = nan\n",
      "Iteration 209000 [20900000%]: Loss = nan\n",
      "Iteration 209100 [20910000%]: Loss = nan\n",
      "Iteration 209200 [20920000%]: Loss = nan\n",
      "Iteration 209300 [20930000%]: Loss = nan\n",
      "Iteration 209400 [20940000%]: Loss = nan\n",
      "Iteration 209500 [20950000%]: Loss = nan\n",
      "Iteration 209600 [20960000%]: Loss = nan\n",
      "Iteration 209700 [20970000%]: Loss = nan\n",
      "Iteration 209800 [20980000%]: Loss = nan\n",
      "Iteration 209900 [20990000%]: Loss = nan\n",
      "Iteration 210000 [21000000%]: Loss = nan\n",
      "Iteration 210100 [21010000%]: Loss = nan\n",
      "Iteration 210200 [21020000%]: Loss = nan\n",
      "Iteration 210300 [21030000%]: Loss = nan\n",
      "Iteration 210400 [21040000%]: Loss = nan\n",
      "Iteration 210500 [21050000%]: Loss = nan\n",
      "Iteration 210600 [21060000%]: Loss = nan\n",
      "Iteration 210700 [21070000%]: Loss = nan\n",
      "Iteration 210800 [21080000%]: Loss = nan\n",
      "Iteration 210900 [21090000%]: Loss = nan\n",
      "Iteration 211000 [21100000%]: Loss = nan\n",
      "Iteration 211100 [21110000%]: Loss = nan\n",
      "Iteration 211200 [21120000%]: Loss = nan\n",
      "Iteration 211300 [21130000%]: Loss = nan\n",
      "Iteration 211400 [21140000%]: Loss = nan\n",
      "Iteration 211500 [21150000%]: Loss = nan\n",
      "Iteration 211600 [21160000%]: Loss = nan\n",
      "Iteration 211700 [21170000%]: Loss = nan\n",
      "Iteration 211800 [21180000%]: Loss = nan\n",
      "Iteration 211900 [21190000%]: Loss = nan\n",
      "Iteration 212000 [21200000%]: Loss = nan\n",
      "Iteration 212100 [21210000%]: Loss = nan\n",
      "Iteration 212200 [21220000%]: Loss = nan\n",
      "Iteration 212300 [21230000%]: Loss = nan\n",
      "Iteration 212400 [21240000%]: Loss = nan\n",
      "Iteration 212500 [21250000%]: Loss = nan\n",
      "Iteration 212600 [21260000%]: Loss = nan\n",
      "Iteration 212700 [21270000%]: Loss = nan\n",
      "Iteration 212800 [21280000%]: Loss = nan\n",
      "Iteration 212900 [21290000%]: Loss = nan\n",
      "Iteration 213000 [21300000%]: Loss = nan\n",
      "Iteration 213100 [21310000%]: Loss = nan\n",
      "Iteration 213200 [21320000%]: Loss = nan\n",
      "Iteration 213300 [21330000%]: Loss = nan\n",
      "Iteration 213400 [21340000%]: Loss = nan\n",
      "Iteration 213500 [21350000%]: Loss = nan\n",
      "Iteration 213600 [21360000%]: Loss = nan\n",
      "Iteration 213700 [21370000%]: Loss = nan\n",
      "Iteration 213800 [21380000%]: Loss = nan\n",
      "Iteration 213900 [21390000%]: Loss = nan\n",
      "Iteration 214000 [21400000%]: Loss = nan\n",
      "Iteration 214100 [21410000%]: Loss = nan\n",
      "Iteration 214200 [21420000%]: Loss = nan\n",
      "Iteration 214300 [21430000%]: Loss = nan\n",
      "Iteration 214400 [21440000%]: Loss = nan\n",
      "Iteration 214500 [21450000%]: Loss = nan\n",
      "Iteration 214600 [21460000%]: Loss = nan\n",
      "Iteration 214700 [21470000%]: Loss = nan\n",
      "Iteration 214800 [21480000%]: Loss = nan\n",
      "Iteration 214900 [21490000%]: Loss = nan\n",
      "Iteration 215000 [21500000%]: Loss = nan\n",
      "Iteration 215100 [21510000%]: Loss = nan\n",
      "Iteration 215200 [21520000%]: Loss = nan\n",
      "Iteration 215300 [21530000%]: Loss = nan\n",
      "Iteration 215400 [21540000%]: Loss = nan\n",
      "Iteration 215500 [21550000%]: Loss = nan\n",
      "Iteration 215600 [21560000%]: Loss = nan\n",
      "Iteration 215700 [21570000%]: Loss = nan\n",
      "Iteration 215800 [21580000%]: Loss = nan\n",
      "Iteration 215900 [21590000%]: Loss = nan\n",
      "Iteration 216000 [21600000%]: Loss = nan\n",
      "Iteration 216100 [21610000%]: Loss = nan\n",
      "Iteration 216200 [21620000%]: Loss = nan\n",
      "Iteration 216300 [21630000%]: Loss = nan\n",
      "Iteration 216400 [21640000%]: Loss = nan\n",
      "Iteration 216500 [21650000%]: Loss = nan\n",
      "Iteration 216600 [21660000%]: Loss = nan\n",
      "Iteration 216700 [21670000%]: Loss = nan\n",
      "Iteration 216800 [21680000%]: Loss = nan\n",
      "Iteration 216900 [21690000%]: Loss = nan\n",
      "Iteration 217000 [21700000%]: Loss = nan\n",
      "Iteration 217100 [21710000%]: Loss = nan\n",
      "Iteration 217200 [21720000%]: Loss = nan\n",
      "Iteration 217300 [21730000%]: Loss = nan\n",
      "Iteration 217400 [21740000%]: Loss = nan\n",
      "Iteration 217500 [21750000%]: Loss = nan\n",
      "Iteration 217600 [21760000%]: Loss = nan\n",
      "Iteration 217700 [21770000%]: Loss = nan\n",
      "Iteration 217800 [21780000%]: Loss = nan\n",
      "Iteration 217900 [21790000%]: Loss = nan\n",
      "Iteration 218000 [21800000%]: Loss = nan\n",
      "Iteration 218100 [21810000%]: Loss = nan\n",
      "Iteration 218200 [21820000%]: Loss = nan\n",
      "Iteration 218300 [21830000%]: Loss = nan\n",
      "Iteration 218400 [21840000%]: Loss = nan\n",
      "Iteration 218500 [21850000%]: Loss = nan\n",
      "Iteration 218600 [21860000%]: Loss = nan\n",
      "Iteration 218700 [21870000%]: Loss = nan\n",
      "Iteration 218800 [21880000%]: Loss = nan\n",
      "Iteration 218900 [21890000%]: Loss = nan\n",
      "Iteration 219000 [21900000%]: Loss = nan\n",
      "Iteration 219100 [21910000%]: Loss = nan\n",
      "Iteration 219200 [21920000%]: Loss = nan\n",
      "Iteration 219300 [21930000%]: Loss = nan\n",
      "Iteration 219400 [21940000%]: Loss = nan\n",
      "Iteration 219500 [21950000%]: Loss = nan\n",
      "Iteration 219600 [21960000%]: Loss = nan\n",
      "Iteration 219700 [21970000%]: Loss = nan\n",
      "Iteration 219800 [21980000%]: Loss = nan\n",
      "Iteration 219900 [21990000%]: Loss = nan\n",
      "Iteration 220000 [22000000%]: Loss = nan\n",
      "Iteration 220100 [22010000%]: Loss = nan\n",
      "Iteration 220200 [22020000%]: Loss = nan\n",
      "Iteration 220300 [22030000%]: Loss = nan\n",
      "Iteration 220400 [22040000%]: Loss = nan\n",
      "Iteration 220500 [22050000%]: Loss = nan\n",
      "Iteration 220600 [22060000%]: Loss = nan\n",
      "Iteration 220700 [22070000%]: Loss = nan\n",
      "Iteration 220800 [22080000%]: Loss = nan\n",
      "Iteration 220900 [22090000%]: Loss = nan\n",
      "Iteration 221000 [22100000%]: Loss = nan\n",
      "Iteration 221100 [22110000%]: Loss = nan\n",
      "Iteration 221200 [22120000%]: Loss = nan\n",
      "Iteration 221300 [22130000%]: Loss = nan\n",
      "Iteration 221400 [22140000%]: Loss = nan\n",
      "Iteration 221500 [22150000%]: Loss = nan\n",
      "Iteration 221600 [22160000%]: Loss = nan\n",
      "Iteration 221700 [22170000%]: Loss = nan\n",
      "Iteration 221800 [22180000%]: Loss = nan\n",
      "Iteration 221900 [22190000%]: Loss = nan\n",
      "Iteration 222000 [22200000%]: Loss = nan\n",
      "Iteration 222100 [22210000%]: Loss = nan\n",
      "Iteration 222200 [22220000%]: Loss = nan\n",
      "Iteration 222300 [22230000%]: Loss = nan\n",
      "Iteration 222400 [22240000%]: Loss = nan\n",
      "Iteration 222500 [22250000%]: Loss = nan\n",
      "Iteration 222600 [22260000%]: Loss = nan\n",
      "Iteration 222700 [22270000%]: Loss = nan\n",
      "Iteration 222800 [22280000%]: Loss = nan\n",
      "Iteration 222900 [22290000%]: Loss = nan\n",
      "Iteration 223000 [22300000%]: Loss = nan\n",
      "Iteration 223100 [22310000%]: Loss = nan\n",
      "Iteration 223200 [22320000%]: Loss = nan\n",
      "Iteration 223300 [22330000%]: Loss = nan\n",
      "Iteration 223400 [22340000%]: Loss = nan\n",
      "Iteration 223500 [22350000%]: Loss = nan\n",
      "Iteration 223600 [22360000%]: Loss = nan\n",
      "Iteration 223700 [22370000%]: Loss = nan\n",
      "Iteration 223800 [22380000%]: Loss = nan\n",
      "Iteration 223900 [22390000%]: Loss = nan\n",
      "Iteration 224000 [22400000%]: Loss = nan\n",
      "Iteration 224100 [22410000%]: Loss = nan\n",
      "Iteration 224200 [22420000%]: Loss = nan\n",
      "Iteration 224300 [22430000%]: Loss = nan\n",
      "Iteration 224400 [22440000%]: Loss = nan\n",
      "Iteration 224500 [22450000%]: Loss = nan\n",
      "Iteration 224600 [22460000%]: Loss = nan\n",
      "Iteration 224700 [22470000%]: Loss = nan\n",
      "Iteration 224800 [22480000%]: Loss = nan\n",
      "Iteration 224900 [22490000%]: Loss = nan\n",
      "Iteration 225000 [22500000%]: Loss = nan\n",
      "Iteration 225100 [22510000%]: Loss = nan\n",
      "Iteration 225200 [22520000%]: Loss = nan\n",
      "Iteration 225300 [22530000%]: Loss = nan\n",
      "Iteration 225400 [22540000%]: Loss = nan\n",
      "Iteration 225500 [22550000%]: Loss = nan\n",
      "Iteration 225600 [22560000%]: Loss = nan\n",
      "Iteration 225700 [22570000%]: Loss = nan\n",
      "Iteration 225800 [22580000%]: Loss = nan\n",
      "Iteration 225900 [22590000%]: Loss = nan\n",
      "Iteration 226000 [22600000%]: Loss = nan\n",
      "Iteration 226100 [22610000%]: Loss = nan\n",
      "Iteration 226200 [22620000%]: Loss = nan\n",
      "Iteration 226300 [22630000%]: Loss = nan\n",
      "Iteration 226400 [22640000%]: Loss = nan\n",
      "Iteration 226500 [22650000%]: Loss = nan\n",
      "Iteration 226600 [22660000%]: Loss = nan\n",
      "Iteration 226700 [22670000%]: Loss = nan\n",
      "Iteration 226800 [22680000%]: Loss = nan\n",
      "Iteration 226900 [22690000%]: Loss = nan\n",
      "Iteration 227000 [22700000%]: Loss = nan\n",
      "Iteration 227100 [22710000%]: Loss = nan\n",
      "Iteration 227200 [22720000%]: Loss = nan\n",
      "Iteration 227300 [22730000%]: Loss = nan\n",
      "Iteration 227400 [22740000%]: Loss = nan\n",
      "Iteration 227500 [22750000%]: Loss = nan\n",
      "Iteration 227600 [22760000%]: Loss = nan\n",
      "Iteration 227700 [22770000%]: Loss = nan\n",
      "Iteration 227800 [22780000%]: Loss = nan\n",
      "Iteration 227900 [22790000%]: Loss = nan\n",
      "Iteration 228000 [22800000%]: Loss = nan\n",
      "Iteration 228100 [22810000%]: Loss = nan\n",
      "Iteration 228200 [22820000%]: Loss = nan\n",
      "Iteration 228300 [22830000%]: Loss = nan\n",
      "Iteration 228400 [22840000%]: Loss = nan\n",
      "Iteration 228500 [22850000%]: Loss = nan\n",
      "Iteration 228600 [22860000%]: Loss = nan\n",
      "Iteration 228700 [22870000%]: Loss = nan\n",
      "Iteration 228800 [22880000%]: Loss = nan\n",
      "Iteration 228900 [22890000%]: Loss = nan\n",
      "Iteration 229000 [22900000%]: Loss = nan\n",
      "Iteration 229100 [22910000%]: Loss = nan\n",
      "Iteration 229200 [22920000%]: Loss = nan\n",
      "Iteration 229300 [22930000%]: Loss = nan\n",
      "Iteration 229400 [22940000%]: Loss = nan\n",
      "Iteration 229500 [22950000%]: Loss = nan\n",
      "Iteration 229600 [22960000%]: Loss = nan\n",
      "Iteration 229700 [22970000%]: Loss = nan\n",
      "Iteration 229800 [22980000%]: Loss = nan\n",
      "Iteration 229900 [22990000%]: Loss = nan\n",
      "Iteration 230000 [23000000%]: Loss = nan\n",
      "Iteration 230100 [23010000%]: Loss = nan\n",
      "Iteration 230200 [23020000%]: Loss = nan\n",
      "Iteration 230300 [23030000%]: Loss = nan\n",
      "Iteration 230400 [23040000%]: Loss = nan\n",
      "Iteration 230500 [23050000%]: Loss = nan\n",
      "Iteration 230600 [23060000%]: Loss = nan\n",
      "Iteration 230700 [23070000%]: Loss = nan\n",
      "Iteration 230800 [23080000%]: Loss = nan\n",
      "Iteration 230900 [23090000%]: Loss = nan\n",
      "Iteration 231000 [23100000%]: Loss = nan\n",
      "Iteration 231100 [23110000%]: Loss = nan\n",
      "Iteration 231200 [23120000%]: Loss = nan\n",
      "Iteration 231300 [23130000%]: Loss = nan\n",
      "Iteration 231400 [23140000%]: Loss = nan\n",
      "Iteration 231500 [23150000%]: Loss = nan\n",
      "Iteration 231600 [23160000%]: Loss = nan\n",
      "Iteration 231700 [23170000%]: Loss = nan\n",
      "Iteration 231800 [23180000%]: Loss = nan\n",
      "Iteration 231900 [23190000%]: Loss = nan\n",
      "Iteration 232000 [23200000%]: Loss = nan\n",
      "Iteration 232100 [23210000%]: Loss = nan\n",
      "Iteration 232200 [23220000%]: Loss = nan\n",
      "Iteration 232300 [23230000%]: Loss = nan\n",
      "Iteration 232400 [23240000%]: Loss = nan\n",
      "Iteration 232500 [23250000%]: Loss = nan\n",
      "Iteration 232600 [23260000%]: Loss = nan\n",
      "Iteration 232700 [23270000%]: Loss = nan\n",
      "Iteration 232800 [23280000%]: Loss = nan\n",
      "Iteration 232900 [23290000%]: Loss = nan\n",
      "Iteration 233000 [23300000%]: Loss = nan\n",
      "Iteration 233100 [23310000%]: Loss = nan\n",
      "Iteration 233200 [23320000%]: Loss = nan\n",
      "Iteration 233300 [23330000%]: Loss = nan\n",
      "Iteration 233400 [23340000%]: Loss = nan\n",
      "Iteration 233500 [23350000%]: Loss = nan\n",
      "Iteration 233600 [23360000%]: Loss = nan\n",
      "Iteration 233700 [23370000%]: Loss = nan\n",
      "Iteration 233800 [23380000%]: Loss = nan\n",
      "Iteration 233900 [23390000%]: Loss = nan\n",
      "Iteration 234000 [23400000%]: Loss = nan\n",
      "Iteration 234100 [23410000%]: Loss = nan\n",
      "Iteration 234200 [23420000%]: Loss = nan\n",
      "Iteration 234300 [23430000%]: Loss = nan\n",
      "Iteration 234400 [23440000%]: Loss = nan\n",
      "Iteration 234500 [23450000%]: Loss = nan\n",
      "Iteration 234600 [23460000%]: Loss = nan\n",
      "Iteration 234700 [23470000%]: Loss = nan\n",
      "Iteration 234800 [23480000%]: Loss = nan\n",
      "Iteration 234900 [23490000%]: Loss = nan\n",
      "Iteration 235000 [23500000%]: Loss = nan\n",
      "Iteration 235100 [23510000%]: Loss = nan\n",
      "Iteration 235200 [23520000%]: Loss = nan\n",
      "Iteration 235300 [23530000%]: Loss = nan\n",
      "Iteration 235400 [23540000%]: Loss = nan\n",
      "Iteration 235500 [23550000%]: Loss = nan\n",
      "Iteration 235600 [23560000%]: Loss = nan\n",
      "Iteration 235700 [23570000%]: Loss = nan\n",
      "Iteration 235800 [23580000%]: Loss = nan\n",
      "Iteration 235900 [23590000%]: Loss = nan\n",
      "Iteration 236000 [23600000%]: Loss = nan\n",
      "Iteration 236100 [23610000%]: Loss = nan\n",
      "Iteration 236200 [23620000%]: Loss = nan\n",
      "Iteration 236300 [23630000%]: Loss = nan\n",
      "Iteration 236400 [23640000%]: Loss = nan\n",
      "Iteration 236500 [23650000%]: Loss = nan\n",
      "Iteration 236600 [23660000%]: Loss = nan\n",
      "Iteration 236700 [23670000%]: Loss = nan\n",
      "Iteration 236800 [23680000%]: Loss = nan\n",
      "Iteration 236900 [23690000%]: Loss = nan\n",
      "Iteration 237000 [23700000%]: Loss = nan\n",
      "Iteration 237100 [23710000%]: Loss = nan\n",
      "Iteration 237200 [23720000%]: Loss = nan\n",
      "Iteration 237300 [23730000%]: Loss = nan\n",
      "Iteration 237400 [23740000%]: Loss = nan\n",
      "Iteration 237500 [23750000%]: Loss = nan\n",
      "Iteration 237600 [23760000%]: Loss = nan\n",
      "Iteration 237700 [23770000%]: Loss = nan\n",
      "Iteration 237800 [23780000%]: Loss = nan\n",
      "Iteration 237900 [23790000%]: Loss = nan\n",
      "Iteration 238000 [23800000%]: Loss = nan\n",
      "Iteration 238100 [23810000%]: Loss = nan\n",
      "Iteration 238200 [23820000%]: Loss = nan\n",
      "Iteration 238300 [23830000%]: Loss = nan\n",
      "Iteration 238400 [23840000%]: Loss = nan\n",
      "Iteration 238500 [23850000%]: Loss = nan\n",
      "Iteration 238600 [23860000%]: Loss = nan\n",
      "Iteration 238700 [23870000%]: Loss = nan\n",
      "Iteration 238800 [23880000%]: Loss = nan\n",
      "Iteration 238900 [23890000%]: Loss = nan\n",
      "Iteration 239000 [23900000%]: Loss = nan\n",
      "Iteration 239100 [23910000%]: Loss = nan\n",
      "Iteration 239200 [23920000%]: Loss = nan\n",
      "Iteration 239300 [23930000%]: Loss = nan\n",
      "Iteration 239400 [23940000%]: Loss = nan\n",
      "Iteration 239500 [23950000%]: Loss = nan\n",
      "Iteration 239600 [23960000%]: Loss = nan\n",
      "Iteration 239700 [23970000%]: Loss = nan\n",
      "Iteration 239800 [23980000%]: Loss = nan\n",
      "Iteration 239900 [23990000%]: Loss = nan\n",
      "Iteration 240000 [24000000%]: Loss = nan\n",
      "Iteration 240100 [24010000%]: Loss = nan\n",
      "Iteration 240200 [24020000%]: Loss = nan\n",
      "Iteration 240300 [24030000%]: Loss = nan\n",
      "Iteration 240400 [24040000%]: Loss = nan\n",
      "Iteration 240500 [24050000%]: Loss = nan\n",
      "Iteration 240600 [24060000%]: Loss = nan\n",
      "Iteration 240700 [24070000%]: Loss = nan\n",
      "Iteration 240800 [24080000%]: Loss = nan\n",
      "Iteration 240900 [24090000%]: Loss = nan\n",
      "Iteration 241000 [24100000%]: Loss = nan\n",
      "Iteration 241100 [24110000%]: Loss = nan\n",
      "Iteration 241200 [24120000%]: Loss = nan\n",
      "Iteration 241300 [24130000%]: Loss = nan\n",
      "Iteration 241400 [24140000%]: Loss = nan\n",
      "Iteration 241500 [24150000%]: Loss = nan\n",
      "Iteration 241600 [24160000%]: Loss = nan\n",
      "Iteration 241700 [24170000%]: Loss = nan\n",
      "Iteration 241800 [24180000%]: Loss = nan\n",
      "Iteration 241900 [24190000%]: Loss = nan\n",
      "Iteration 242000 [24200000%]: Loss = nan\n",
      "Iteration 242100 [24210000%]: Loss = nan\n",
      "Iteration 242200 [24220000%]: Loss = nan\n",
      "Iteration 242300 [24230000%]: Loss = nan\n",
      "Iteration 242400 [24240000%]: Loss = nan\n",
      "Iteration 242500 [24250000%]: Loss = nan\n",
      "Iteration 242600 [24260000%]: Loss = nan\n",
      "Iteration 242700 [24270000%]: Loss = nan\n",
      "Iteration 242800 [24280000%]: Loss = nan\n",
      "Iteration 242900 [24290000%]: Loss = nan\n",
      "Iteration 243000 [24300000%]: Loss = nan\n",
      "Iteration 243100 [24310000%]: Loss = nan\n",
      "Iteration 243200 [24320000%]: Loss = nan\n",
      "Iteration 243300 [24330000%]: Loss = nan\n",
      "Iteration 243400 [24340000%]: Loss = nan\n",
      "Iteration 243500 [24350000%]: Loss = nan\n",
      "Iteration 243600 [24360000%]: Loss = nan\n",
      "Iteration 243700 [24370000%]: Loss = nan\n",
      "Iteration 243800 [24380000%]: Loss = nan\n",
      "Iteration 243900 [24390000%]: Loss = nan\n",
      "Iteration 244000 [24400000%]: Loss = nan\n",
      "Iteration 244100 [24410000%]: Loss = nan\n",
      "Iteration 244200 [24420000%]: Loss = nan\n",
      "Iteration 244300 [24430000%]: Loss = nan\n",
      "Iteration 244400 [24440000%]: Loss = nan\n",
      "Iteration 244500 [24450000%]: Loss = nan\n",
      "Iteration 244600 [24460000%]: Loss = nan\n",
      "Iteration 244700 [24470000%]: Loss = nan\n",
      "Iteration 244800 [24480000%]: Loss = nan\n",
      "Iteration 244900 [24490000%]: Loss = nan\n",
      "Iteration 245000 [24500000%]: Loss = nan\n",
      "Iteration 245100 [24510000%]: Loss = nan\n",
      "Iteration 245200 [24520000%]: Loss = nan\n",
      "Iteration 245300 [24530000%]: Loss = nan\n",
      "Iteration 245400 [24540000%]: Loss = nan\n",
      "Iteration 245500 [24550000%]: Loss = nan\n",
      "Iteration 245600 [24560000%]: Loss = nan\n",
      "Iteration 245700 [24570000%]: Loss = nan\n",
      "Iteration 245800 [24580000%]: Loss = nan\n",
      "Iteration 245900 [24590000%]: Loss = nan\n",
      "Iteration 246000 [24600000%]: Loss = nan\n",
      "Iteration 246100 [24610000%]: Loss = nan\n",
      "Iteration 246200 [24620000%]: Loss = nan\n",
      "Iteration 246300 [24630000%]: Loss = nan\n",
      "Iteration 246400 [24640000%]: Loss = nan\n",
      "Iteration 246500 [24650000%]: Loss = nan\n",
      "Iteration 246600 [24660000%]: Loss = nan\n",
      "Iteration 246700 [24670000%]: Loss = nan\n",
      "Iteration 246800 [24680000%]: Loss = nan\n",
      "Iteration 246900 [24690000%]: Loss = nan\n",
      "Iteration 247000 [24700000%]: Loss = nan\n",
      "Iteration 247100 [24710000%]: Loss = nan\n",
      "Iteration 247200 [24720000%]: Loss = nan\n",
      "Iteration 247300 [24730000%]: Loss = nan\n",
      "Iteration 247400 [24740000%]: Loss = nan\n",
      "Iteration 247500 [24750000%]: Loss = nan\n",
      "Iteration 247600 [24760000%]: Loss = nan\n",
      "Iteration 247700 [24770000%]: Loss = nan\n",
      "Iteration 247800 [24780000%]: Loss = nan\n",
      "Iteration 247900 [24790000%]: Loss = nan\n",
      "Iteration 248000 [24800000%]: Loss = nan\n",
      "Iteration 248100 [24810000%]: Loss = nan\n",
      "Iteration 248200 [24820000%]: Loss = nan\n",
      "Iteration 248300 [24830000%]: Loss = nan\n",
      "Iteration 248400 [24840000%]: Loss = nan\n",
      "Iteration 248500 [24850000%]: Loss = nan\n",
      "Iteration 248600 [24860000%]: Loss = nan\n",
      "Iteration 248700 [24870000%]: Loss = nan\n",
      "Iteration 248800 [24880000%]: Loss = nan\n",
      "Iteration 248900 [24890000%]: Loss = nan\n",
      "Iteration 249000 [24900000%]: Loss = nan\n",
      "Iteration 249100 [24910000%]: Loss = nan\n",
      "Iteration 249200 [24920000%]: Loss = nan\n",
      "Iteration 249300 [24930000%]: Loss = nan\n",
      "Iteration 249400 [24940000%]: Loss = nan\n",
      "Iteration 249500 [24950000%]: Loss = nan\n",
      "Iteration 249600 [24960000%]: Loss = nan\n",
      "Iteration 249700 [24970000%]: Loss = nan\n",
      "Iteration 249800 [24980000%]: Loss = nan\n",
      "Iteration 249900 [24990000%]: Loss = nan\n",
      "Iteration 250000 [25000000%]: Loss = nan\n",
      "Iteration 250100 [25010000%]: Loss = nan\n",
      "Iteration 250200 [25020000%]: Loss = nan\n",
      "Iteration 250300 [25030000%]: Loss = nan\n",
      "Iteration 250400 [25040000%]: Loss = nan\n",
      "Iteration 250500 [25050000%]: Loss = nan\n",
      "Iteration 250600 [25060000%]: Loss = nan\n",
      "Iteration 250700 [25070000%]: Loss = nan\n",
      "Iteration 250800 [25080000%]: Loss = nan\n",
      "Iteration 250900 [25090000%]: Loss = nan\n",
      "Iteration 251000 [25100000%]: Loss = nan\n",
      "Iteration 251100 [25110000%]: Loss = nan\n",
      "Iteration 251200 [25120000%]: Loss = nan\n",
      "Iteration 251300 [25130000%]: Loss = nan\n",
      "Iteration 251400 [25140000%]: Loss = nan\n",
      "Iteration 251500 [25150000%]: Loss = nan\n",
      "Iteration 251600 [25160000%]: Loss = nan\n",
      "Iteration 251700 [25170000%]: Loss = nan\n",
      "Iteration 251800 [25180000%]: Loss = nan\n",
      "Iteration 251900 [25190000%]: Loss = nan\n",
      "Iteration 252000 [25200000%]: Loss = nan\n",
      "Iteration 252100 [25210000%]: Loss = nan\n",
      "Iteration 252200 [25220000%]: Loss = nan\n",
      "Iteration 252300 [25230000%]: Loss = nan\n",
      "Iteration 252400 [25240000%]: Loss = nan\n",
      "Iteration 252500 [25250000%]: Loss = nan\n",
      "Iteration 252600 [25260000%]: Loss = nan\n",
      "Iteration 252700 [25270000%]: Loss = nan\n",
      "Iteration 252800 [25280000%]: Loss = nan\n",
      "Iteration 252900 [25290000%]: Loss = nan\n",
      "Iteration 253000 [25300000%]: Loss = nan\n",
      "Iteration 253100 [25310000%]: Loss = nan\n",
      "Iteration 253200 [25320000%]: Loss = nan\n",
      "Iteration 253300 [25330000%]: Loss = nan\n",
      "Iteration 253400 [25340000%]: Loss = nan\n",
      "Iteration 253500 [25350000%]: Loss = nan\n",
      "Iteration 253600 [25360000%]: Loss = nan\n",
      "Iteration 253700 [25370000%]: Loss = nan\n",
      "Iteration 253800 [25380000%]: Loss = nan\n",
      "Iteration 253900 [25390000%]: Loss = nan\n",
      "Iteration 254000 [25400000%]: Loss = nan\n",
      "Iteration 254100 [25410000%]: Loss = nan\n",
      "Iteration 254200 [25420000%]: Loss = nan\n",
      "Iteration 254300 [25430000%]: Loss = nan\n",
      "Iteration 254400 [25440000%]: Loss = nan\n",
      "Iteration 254500 [25450000%]: Loss = nan\n",
      "Iteration 254600 [25460000%]: Loss = nan\n",
      "Iteration 254700 [25470000%]: Loss = nan\n",
      "Iteration 254800 [25480000%]: Loss = nan\n",
      "Iteration 254900 [25490000%]: Loss = nan\n",
      "Iteration 255000 [25500000%]: Loss = nan\n",
      "Iteration 255100 [25510000%]: Loss = nan\n",
      "Iteration 255200 [25520000%]: Loss = nan\n",
      "Iteration 255300 [25530000%]: Loss = nan\n",
      "Iteration 255400 [25540000%]: Loss = nan\n",
      "Iteration 255500 [25550000%]: Loss = nan\n",
      "Iteration 255600 [25560000%]: Loss = nan\n",
      "Iteration 255700 [25570000%]: Loss = nan\n",
      "Iteration 255800 [25580000%]: Loss = nan\n",
      "Iteration 255900 [25590000%]: Loss = nan\n",
      "Iteration 256000 [25600000%]: Loss = nan\n",
      "Iteration 256100 [25610000%]: Loss = nan\n",
      "Iteration 256200 [25620000%]: Loss = nan\n",
      "Iteration 256300 [25630000%]: Loss = nan\n",
      "Iteration 256400 [25640000%]: Loss = nan\n",
      "Iteration 256500 [25650000%]: Loss = nan\n",
      "Iteration 256600 [25660000%]: Loss = nan\n",
      "Iteration 256700 [25670000%]: Loss = nan\n",
      "Iteration 256800 [25680000%]: Loss = nan\n",
      "Iteration 256900 [25690000%]: Loss = nan\n",
      "Iteration 257000 [25700000%]: Loss = nan\n",
      "Iteration 257100 [25710000%]: Loss = nan\n",
      "Iteration 257200 [25720000%]: Loss = nan\n",
      "Iteration 257300 [25730000%]: Loss = nan\n",
      "Iteration 257400 [25740000%]: Loss = nan\n",
      "Iteration 257500 [25750000%]: Loss = nan\n",
      "Iteration 257600 [25760000%]: Loss = nan\n",
      "Iteration 257700 [25770000%]: Loss = nan\n",
      "Iteration 257800 [25780000%]: Loss = nan\n",
      "Iteration 257900 [25790000%]: Loss = nan\n",
      "Iteration 258000 [25800000%]: Loss = nan\n",
      "Iteration 258100 [25810000%]: Loss = nan\n",
      "Iteration 258200 [25820000%]: Loss = nan\n",
      "Iteration 258300 [25830000%]: Loss = nan\n",
      "Iteration 258400 [25840000%]: Loss = nan\n",
      "Iteration 258500 [25850000%]: Loss = nan\n",
      "Iteration 258600 [25860000%]: Loss = nan\n",
      "Iteration 258700 [25870000%]: Loss = nan\n",
      "Iteration 258800 [25880000%]: Loss = nan\n",
      "Iteration 258900 [25890000%]: Loss = nan\n",
      "Iteration 259000 [25900000%]: Loss = nan\n",
      "Iteration 259100 [25910000%]: Loss = nan\n",
      "Iteration 259200 [25920000%]: Loss = nan\n",
      "Iteration 259300 [25930000%]: Loss = nan\n",
      "Iteration 259400 [25940000%]: Loss = nan\n",
      "Iteration 259500 [25950000%]: Loss = nan\n",
      "Iteration 259600 [25960000%]: Loss = nan\n",
      "Iteration 259700 [25970000%]: Loss = nan\n",
      "Iteration 259800 [25980000%]: Loss = nan\n",
      "Iteration 259900 [25990000%]: Loss = nan\n",
      "Iteration 260000 [26000000%]: Loss = nan\n",
      "Iteration 260100 [26010000%]: Loss = nan\n",
      "Iteration 260200 [26020000%]: Loss = nan\n",
      "Iteration 260300 [26030000%]: Loss = nan\n",
      "Iteration 260400 [26040000%]: Loss = nan\n",
      "Iteration 260500 [26050000%]: Loss = nan\n",
      "Iteration 260600 [26060000%]: Loss = nan\n",
      "Iteration 260700 [26070000%]: Loss = nan\n",
      "Iteration 260800 [26080000%]: Loss = nan\n",
      "Iteration 260900 [26090000%]: Loss = nan\n",
      "Iteration 261000 [26100000%]: Loss = nan\n",
      "Iteration 261100 [26110000%]: Loss = nan\n",
      "Iteration 261200 [26120000%]: Loss = nan\n",
      "Iteration 261300 [26130000%]: Loss = nan\n",
      "Iteration 261400 [26140000%]: Loss = nan\n",
      "Iteration 261500 [26150000%]: Loss = nan\n",
      "Iteration 261600 [26160000%]: Loss = nan\n",
      "Iteration 261700 [26170000%]: Loss = nan\n",
      "Iteration 261800 [26180000%]: Loss = nan\n",
      "Iteration 261900 [26190000%]: Loss = nan\n",
      "Iteration 262000 [26200000%]: Loss = nan\n",
      "Iteration 262100 [26210000%]: Loss = nan\n",
      "Iteration 262200 [26220000%]: Loss = nan\n",
      "Iteration 262300 [26230000%]: Loss = nan\n",
      "Iteration 262400 [26240000%]: Loss = nan\n",
      "Iteration 262500 [26250000%]: Loss = nan\n",
      "Iteration 262600 [26260000%]: Loss = nan\n",
      "Iteration 262700 [26270000%]: Loss = nan\n",
      "Iteration 262800 [26280000%]: Loss = nan\n",
      "Iteration 262900 [26290000%]: Loss = nan\n",
      "Iteration 263000 [26300000%]: Loss = nan\n",
      "Iteration 263100 [26310000%]: Loss = nan\n",
      "Iteration 263200 [26320000%]: Loss = nan\n",
      "Iteration 263300 [26330000%]: Loss = nan\n",
      "Iteration 263400 [26340000%]: Loss = nan\n",
      "Iteration 263500 [26350000%]: Loss = nan\n",
      "Iteration 263600 [26360000%]: Loss = nan\n",
      "Iteration 263700 [26370000%]: Loss = nan\n",
      "Iteration 263800 [26380000%]: Loss = nan\n",
      "Iteration 263900 [26390000%]: Loss = nan\n",
      "Iteration 264000 [26400000%]: Loss = nan\n",
      "Iteration 264100 [26410000%]: Loss = nan\n",
      "Iteration 264200 [26420000%]: Loss = nan\n",
      "Iteration 264300 [26430000%]: Loss = nan\n",
      "Iteration 264400 [26440000%]: Loss = nan\n",
      "Iteration 264500 [26450000%]: Loss = nan\n",
      "Iteration 264600 [26460000%]: Loss = nan\n",
      "Iteration 264700 [26470000%]: Loss = nan\n",
      "Iteration 264800 [26480000%]: Loss = nan\n",
      "Iteration 264900 [26490000%]: Loss = nan\n",
      "Iteration 265000 [26500000%]: Loss = nan\n",
      "Iteration 265100 [26510000%]: Loss = nan\n",
      "Iteration 265200 [26520000%]: Loss = nan\n",
      "Iteration 265300 [26530000%]: Loss = nan\n",
      "Iteration 265400 [26540000%]: Loss = nan\n",
      "Iteration 265500 [26550000%]: Loss = nan\n",
      "Iteration 265600 [26560000%]: Loss = nan\n",
      "Iteration 265700 [26570000%]: Loss = nan\n",
      "Iteration 265800 [26580000%]: Loss = nan\n",
      "Iteration 265900 [26590000%]: Loss = nan\n",
      "Iteration 266000 [26600000%]: Loss = nan\n",
      "Iteration 266100 [26610000%]: Loss = nan\n",
      "Iteration 266200 [26620000%]: Loss = nan\n",
      "Iteration 266300 [26630000%]: Loss = nan\n",
      "Iteration 266400 [26640000%]: Loss = nan\n",
      "Iteration 266500 [26650000%]: Loss = nan\n",
      "Iteration 266600 [26660000%]: Loss = nan\n",
      "Iteration 266700 [26670000%]: Loss = nan\n",
      "Iteration 266800 [26680000%]: Loss = nan\n",
      "Iteration 266900 [26690000%]: Loss = nan\n",
      "Iteration 267000 [26700000%]: Loss = nan\n",
      "Iteration 267100 [26710000%]: Loss = nan\n",
      "Iteration 267200 [26720000%]: Loss = nan\n",
      "Iteration 267300 [26730000%]: Loss = nan\n",
      "Iteration 267400 [26740000%]: Loss = nan\n",
      "Iteration 267500 [26750000%]: Loss = nan\n",
      "Iteration 267600 [26760000%]: Loss = nan\n",
      "Iteration 267700 [26770000%]: Loss = nan\n",
      "Iteration 267800 [26780000%]: Loss = nan\n",
      "Iteration 267900 [26790000%]: Loss = nan\n",
      "Iteration 268000 [26800000%]: Loss = nan\n",
      "Iteration 268100 [26810000%]: Loss = nan\n",
      "Iteration 268200 [26820000%]: Loss = nan\n",
      "Iteration 268300 [26830000%]: Loss = nan\n",
      "Iteration 268400 [26840000%]: Loss = nan\n",
      "Iteration 268500 [26850000%]: Loss = nan\n",
      "Iteration 268600 [26860000%]: Loss = nan\n",
      "Iteration 268700 [26870000%]: Loss = nan\n",
      "Iteration 268800 [26880000%]: Loss = nan\n",
      "Iteration 268900 [26890000%]: Loss = nan\n",
      "Iteration 269000 [26900000%]: Loss = nan\n",
      "Iteration 269100 [26910000%]: Loss = nan\n",
      "Iteration 269200 [26920000%]: Loss = nan\n",
      "Iteration 269300 [26930000%]: Loss = nan\n",
      "Iteration 269400 [26940000%]: Loss = nan\n",
      "Iteration 269500 [26950000%]: Loss = nan\n",
      "Iteration 269600 [26960000%]: Loss = nan\n",
      "Iteration 269700 [26970000%]: Loss = nan\n",
      "Iteration 269800 [26980000%]: Loss = nan\n",
      "Iteration 269900 [26990000%]: Loss = nan\n",
      "Iteration 270000 [27000000%]: Loss = nan\n"
     ]
    }
   ],
   "source": [
    "n_iter = 200000\n",
    "for _ in range(n_iter):\n",
    "    for inf in [inference_lam, inference_coeffs, inference_latents]:\n",
    "        for _ in range(1):  # make multiple steps along each set of coords\n",
    "            info_dict = inf.update()\n",
    "        if inf is inference_lam:\n",
    "            inf.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13e48eda0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmxp/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:4144: UserWarning: Warning: converting a masked element to nan.\n",
      "  warnings.warn(\"Warning: converting a masked element to nan.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPa/PX45SwA6T8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1282fc390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5UAAACMCAYAAADsmY0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e5a0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Zmat = q_Z.mean().eval()\n",
    "\n",
    "plt.matshow(dZ, aspect='auto', cmap='gray')\n",
    "plt.matshow(Zmat.T, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD1hJREFUeJzt3X2sZPVdx/HPZx9YoLBr1ZRRrrAlBhsbzEoi1IB0AIGV\npuAfNVKqpJgY21jZ1IRA0WSHP2qKiamo+AdZILQWNVID2Ie4Jes0wQrlYRcQFqTBQrt1R7cFCbIl\nu+zXP2bYbGf3zsP5nXvnzPe+X8lk5+F3z/nOd+987pkzc87PESEAQA6rZl0AAKA+hDoAJEKoA0Ai\nhDoAJEKoA0AihDoAJDI21G3fYbtn+6kj7vtT27tt77L9Rdvrl7ZMAMAkJtlSv0vSZUP3bZf03ojY\nJOkFSZ+quzAAwPTGhnpEPCTplaH7HoyIQ4ObD0taWILaAABTqmOf+u9I+moNywEAFCoKddt/JOlA\nRNxTUz0AgAJrqv6g7Y9KulzSRWPGcXIZAKggIjztz0y6pe7BpX/D3izpeklXRMSbExQ2t5etW7fO\nvIaVWv881079010kSZ0Rlwo5Mu/9r2qSrzTeI+kbks60/bLtayX9paSTJH3N9hO2/7pyBQCA2ozd\n/RIRVx/j7ruWoBYAQCGOKB2j3W7PuoQi81z/PNcuUf+szXv9Vblk381EK7BjqdcBYL7ZPrzv/Jg6\nKtrPPI9sK5bwg1IAwBwg1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUId\nABIh1AEgEUIdwEithZZsL3ppLbRmXSKOUHk6OwArQ29Pb+QZFHud3rLVgvHYUgeARAh1AEiEUAeA\nRAh1AEiEUAeARAh1AEiEUAeARAh1AEiEUAeARAh1AEiEUAeARAh1AEhkbKjbvsN2z/ZTR9z3Ttvb\nbT9v+59tb1jaMgEAk5hkS/0uSZcN3XejpAcj4uck7ZD0qboLAwBMb2yoR8RDkl4ZuvtKSXcPrt8t\n6ddrrgsAUEHVfervioieJEXEXknvqq8kAEBVdX1QGjUtBwBQoOrMRz3bp0REz3ZL0n+PGtzpdA5f\nb7fbarfbFVcLADl1u111u93i5Thi/Ea27Y2S/ikizhrcvkXSDyLiFts3SHpnRNy4yM/GJOsA0Ey2\nR05np45U+hpfjnXMG9uKCE/7c5N8pfEeSd+QdKbtl21fK+kzki6x/bykiwe3AQAzNnb3S0RcvchD\nv1pzLQCAQhxRCgCJEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJ\nEOoAkAihDgCJEOoAkAihjiXXam2U7eJLq7Vx1k8FaLyq09kBE+v1XlId09j2elNPAgOsOGypA0Ai\nhDoAJEKoA0AihDoAJEKoA0AihDoAJEKoA0AihDoAJEKoA0AihDoAJEKoA0AihDoAJEKoA0AiRaFu\n+5O2/932U7a/YPu4ugoDAEyvcqjb/mlJfyDp7Ij4BfVP43tVXYUBAKZXej711ZLeYfuQpBMlfa+8\nJABAVZW31CPie5L+TNLLkvZIejUiHqyrMADA9Cpvqdv+MUlXSjpd0v9Kutf21RFxz/DYTqdz+Hq7\n3Va73a66WgBIqdvtqtvtFi/HEdWmGbP9IUmXRcTvDm7/tqRzI+ITQ+Oi6jqQg23VMZ2dZPG7tPxs\nS50RAzoq/n9ZjnXMG9uKiKnncCz59svLkt5n+3j3X7UXS9pdsDwAQKGSferflHSvpJ2SnpRkSbfX\nVBcAoIKib79ExM2Sbq6pFgBAIY4oBYBECHUASIRQB4BECHUASIRQB4BECHUASIRQB4BECHUASIRQ\nB4BECHUASIRQB4BECHUASIRQB4BECHUASIRQx6JarY2yXXxpmrqeV6u1cdZPBThK0fnUkVuv95Lq\nmoauSep6Xr1es54XILGlDgCpEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJEOoA\nkAihDgCJEOoAkEhRqNveYPsfbO+2/Yztc+sqDAAwvdKzNN4q6SsR8Ru210g6sYaaAAAVVQ512+sl\n/UpEfFSSIuKgpNdqqgsAUEHJ7pd3S9pn+y7bT9i+3fYJdRUGAJheye6XNZLOlvT7EfGY7T+XdKOk\nrcMDO53O4evtdlvtdrtgtVi51jVyJqV511poqbenN+syVrxut6tut1u8HEdUmwHG9imS/i0izhjc\nPl/SDRHxwaFxUXUdmK1+gNY181HO5WT43bYtdUYM6Gjs46V9mKSGDL2ehm1FxNRbMZV3v0RET9J3\nbJ85uOtiSc9WXR4AoFzpt1+uk/QF22slvSjp2vKSAABVFYV6RDwp6ZdqqgUAUIgjSgEgEUIdABIh\n1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1Bui\n1doo28WX1avfUctymDZuEutq6XOrtXHWT6TMao1/jgutWVe5YpROkoGa9HovqY4p1g4dqmuqNqk/\n7RsW96bq6HWvN+d9fkujp6KT1OswB+pyYUsdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh\n1AEgEUIdABIh1AEgEUIdABIh1AEgkeJQt73K9hO2H6ijIABAdXVsqW+R9GwNywEAFCoKddsLki6X\ntK2ecgAAJUq31D8r6XrVdwJvAECBypNk2P6ApF5E7LLd1ogZFTqdzuHr7XZb7Xa76moBzKPB7EhL\n9fOnnHqK9n53b/XlN0C321W32y1ejiOqbWTb/hNJvyXpoKQTJJ0s6R8j4pqhcVF1HStJ/xe2jj7V\nPfNRk2rKu5xZvkZsj565qKOyx+tYxgSPZ8sZ24qIqf8SVt79EhE3RcRpEXGGpKsk7RgOdADA8uJ7\n6gCQSC0TT0fE1yV9vY5lAQCqY0sdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUIdABIh1AEgEUId\nABIh1AEgEUIdABIh1AEgEUIdABKp5SyN82bfvn267bbbajmp/kUXXaQLLrighqqA6USEtm3bpj17\n9iw65uSTT9aWLVu0Zs2KfKmvSCvyf/q+++7Tpz99vw4cuKJwSS/qS196SI899mAtdQHT2L9/vz72\n8Y/p0PmHFh2zbuc6XXrppTrrrLOWsbIZWAHT3U1qRYa6JK1de7YOHOgULuVBSZ+poRqgmlVrVunQ\nhSNC/cV1y1jNDL2lkdPd9Tq95apk5tinDgCJEOoAkAihDgCJEOoAkAihDgCJEOoAkAihDgCJEOoA\nkAihDgCJEOoAkAihDgCJEOoAkEjlULe9YHuH7WdsP237ujoLAwBMr+QsjQcl/WFE7LJ9kqTHbW+P\niOdqqg0AMKXKW+oRsTcidg2uvy5pt6RT6yoMADC9Wvap294oaZOkR+pYHgCgmuJQH+x6uVfSlsEW\nOwBgRopmPrK9Rv1A/3xE3L/YuE6nc/h6u91Wu90uWS2QzLqRU7GNxPfX0uh2u+p2u8XLKZ3O7k5J\nz0bEraMGHRnqAIa9KanKJOhvSNqg/ncWMO+GN3hvvvnmSssp+UrjeZI+Iuki2zttP2F7c9XlAQDK\nVd5Sj4h/lbS6xloAAIXYIwcAiRDqAJAIoQ4AiRDqAJAIoQ4AiRDqAJAIoQ4AiRDqAJAIoQ4AiRDq\nAJAIoQ4AiRDqAJAIoQ4AiRDqAJAIoV7oyScfle3iC7BULrzkQn73Vmvsa7C10Jp1lbUonfloxTt4\n8DVVm7Vm2Ap5cWHZfb/3fakzYsCox7J4S2OfZ6/TW45Klhxb6gCQCKEOAIkQ6gCQCKEOAIkQ6gCQ\nCKEOAIkQ6gCQCKEOAIkQ6gCQCKEOAIkQ6gCQCKEOAIkUhbrtzbafs/0ftm+oqygAQDWVQ932Kkl/\nJekySe+V9GHb76mrsObozrqAQt1ZF1CgO+sCCnVnXUCZ/5x1AYXmvf6KSrbUz5H0QkS8FBEHJP2d\npCvrKatJurMuoFB31gUU6M66gELdWRdQ5tuzLqDQt2ddwGyUhPqpkr5zxO3vDu4DAMzIipwkY+3a\ntTp0aLvWr//g2LE//OHzOv74x4/52MGD/6M33qi7OmBSliKkO9cPbr+m9RvW/8iI/fv2L39ZmClH\nVJu1x/b7JHUiYvPg9o2SIiJuGRpXx7RAALDiRMTUU6KVhPpqSc9LuljSf0n6pqQPR8TuSgsEABSr\nvPslIt6y/QlJ29XfN38HgQ4As1V5Sx0A0Dy1HVE67kAk2++3/artJwaXP65r3aVs32G7Z/upEWP+\nwvYLtnfZ3rSc9Y0zrv6G937B9g7bz9h+2vZ1i4xrZP8nqb/h/V9n+xHbOwf1b11kXFP7P7b+Jvdf\n6h/zM6jrgUUen673EVF8Uf+Pw7cknS5praRdkt4zNOb9kh6oY311XySdL2mTpKcWefzXJH15cP1c\nSQ/PuuYp629y71uSNg2un6T+5zTDvzuN7f+E9Te2/4P6Thz8u1rSw5LOmZf+T1h/0/v/SUl/c6wa\nq/S+ri31SQ9EmvqT3OUQEQ9JemXEkCslfW4w9hFJG2yfshy1TWKC+qXm9n5vROwaXH9d0m4dfbxD\nY/s/Yf1SQ/svSRHx9hdz16n/OdvwPtnG9l+aqH6pof23vSDpcknbFhkyde/rCvVJD0T65cFbiC/b\n/vma1r0chp/fHs3fgVaN773tjeq/43hk6KG56P+I+qUG93/w9n+npL2SvhYRjw4NaXT/J6hfam7/\nPyvpeh37D5FUoffLeZbGxyWdFhGb1D9nzH3LuO6VrvG9t32SpHslbRls8c6VMfU3uv8RcSgiflHS\ngqRzGxZ6Y01QfyP7b/sDknqDd3pWTe8m6gr1PZJOO+L2wuC+wyLi9bffJkXEVyWttf3jNa1/qe2R\n9DNH3D7q+TVZ03tve436gfj5iLj/GEMa3f9x9Te9/2+LiNck/YukzUMPNbr/b1us/gb3/zxJV9h+\nUdLfSrrQ9ueGxkzd+7pC/VFJP2v7dNvHSbpK0o98knvkfiDb56j/dcof1LT+Ooz6S/mApGukw0fS\nvhoRveUqbEKL1j8Hvb9T0rMRcesijze9/yPrb3L/bf+k7Q2D6ydIukTSc0PDGtv/Sepvav8j4qaI\nOC0izlA/M3dExDVDw6bufS3nfolFDkSy/Xv9h+N2SR+y/XFJByTtl/Sbday7DrbvkdSW9BO2X5a0\nVdJxGtQeEV+xfbntb0n6P0nXzq7ao42rX83u/XmSPiLp6cF+0ZB0k/rfpGp8/yepXw3uv6SfknS3\n+6fSXiXp7wf9PvzabXL/NUH9anb/j1Laew4+AoBEmM4OABIh1AEgEUIdABIh1AEgEUIdABIh1AEg\nEUIdABIh1AEgkf8HQkvhiVONCgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e702a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_A.mean().eval().ravel()); plt.hist(dA.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a53c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_B.mean().eval().ravel(), 200), plt.hist(dB.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e6f3278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_C.mean().eval().ravel(), 200), plt.hist(dC.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n38ysQRz6ZmYN4tA3M2sQh76ZWYP8PwsWeHq2qYtIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a9465f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lam_mu.eval(), 200), plt.hist(dlam, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.88979483,  2.45703912,  0.32039168, ...,  2.60091019,\n",
       "        2.78731441,  2.51170897], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_mu.value().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.88979483,  2.45703912,  0.32039168, ...,  2.60091019,\n",
       "        2.78731441,  2.51170897], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lam.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25033447], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sig.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
