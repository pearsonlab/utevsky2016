{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}((\\lambda)_+)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B \\cdot X)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-7, 1^2) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "Nrep = 50  # number of observations per unit per stim\n",
    "NB = 10  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dA = np.log(softplus(25 + .00 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.00 * np.random.randn(NU, P))  # regressor effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = np.log(softplus(25 + .00 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.00 * np.random.randn(NU, P))  # regressor effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dX = 0.25 * np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = np.tile(dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1), Nrep)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcount = stats.poisson.rvs(np.exp(dlam))\n",
    "dcount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJlZxv4/lWH22DNjTI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cc01ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.exp(dlam), bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128295400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount.copy()\n",
    "Xdat = np.tile(dX[:, dS], (1, Nrep)).T\n",
    "unit = np.tile(dU, Nrep)\n",
    "stim = np.tile(dS, Nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(125000), Dimension(3)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "batch_inds = tf.constant(np.arange(NB))\n",
    "batch_counts = tf.train.batch(tf.train.slice_input_producer([counts]), NB)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_inds, batch_counts = tf.train.batch(tf.train.slice_input_producer([allinds, counts]), NB, \n",
    "                                          name='batches')\n",
    "tf.scalar_summary('total count', tf.reduce_sum(batch_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inds = np.arange(N)\n",
    "batch_counts = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=5 * tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=3 * tf.ones((NU, P)), name='B')\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-7.0], sigma=[1.], name='sig')\n",
    "\n",
    "    lam_vars = tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1)\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "    tf.scalar_summary('lam_mean', tf.reduce_mean(lam.mean()))\n",
    "\n",
    "#     cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(NB), name='cnt')\n",
    "    cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(N), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=np.log(25) + tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    tf.scalar_summary('q_A', tf.reduce_mean(q_A.mean()))\n",
    "\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    tf.scalar_summary('q_B', tf.reduce_mean(q_B.mean()))\n",
    "\n",
    "    lam_mu = tf.Variable(2 + tf.random_normal((N,)))\n",
    "    tf.scalar_summary('lam_mu_mean', tf.reduce_mean(tf.gather(lam_mu, batch_inds)))\n",
    "    lam_sig = tf.Variable(3 * tf.random_uniform((N,)) + 2)\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-7. * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, sig: q_sig, lam: q_lam}, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      1 [  0%]: Loss = 9682154496.000\n",
      "Iteration    100 [  0%]: Loss = 3896829952.000\n",
      "Iteration    200 [  0%]: Loss = 3607755776.000\n",
      "Iteration    300 [  0%]: Loss = 7317085184.000\n",
      "Iteration    400 [  0%]: Loss = 4013392640.000\n",
      "Iteration    500 [  0%]: Loss = 6096502784.000\n",
      "Iteration    600 [  0%]: Loss = 15990236160.000\n",
      "Iteration    700 [  0%]: Loss = 4016857600.000\n",
      "Iteration    800 [  0%]: Loss = 6072032256.000\n",
      "Iteration    900 [  0%]: Loss = 5586869760.000\n",
      "Iteration   1000 [  1%]: Loss = 3873890048.000\n",
      "Iteration   1100 [  1%]: Loss = 2951305728.000\n",
      "Iteration   1200 [  1%]: Loss = 9904172032.000\n",
      "Iteration   1300 [  1%]: Loss = 5080839168.000\n",
      "Iteration   1400 [  1%]: Loss = 4540058112.000\n",
      "Iteration   1500 [  1%]: Loss = 5744488448.000\n",
      "Iteration   1600 [  1%]: Loss = 33235902464.000\n",
      "Iteration   1700 [  1%]: Loss = 1768337664.000\n",
      "Iteration   1800 [  1%]: Loss = 10770117632.000\n",
      "Iteration   1900 [  1%]: Loss = 6366599168.000\n",
      "Iteration   2000 [  2%]: Loss = 1948628608.000\n",
      "Iteration   2100 [  2%]: Loss = 5196722176.000\n",
      "Iteration   2200 [  2%]: Loss = 2376648960.000\n",
      "Iteration   2300 [  2%]: Loss = 2581439744.000\n",
      "Iteration   2400 [  2%]: Loss = 7787237376.000\n",
      "Iteration   2500 [  2%]: Loss = 15325812736.000\n",
      "Iteration   2600 [  2%]: Loss = 5548728320.000\n",
      "Iteration   2700 [  2%]: Loss = 2358774016.000\n",
      "Iteration   2800 [  2%]: Loss = 2544446464.000\n",
      "Iteration   2900 [  2%]: Loss = 5371097088.000\n",
      "Iteration   3000 [  3%]: Loss = 3378650368.000\n",
      "Iteration   3100 [  3%]: Loss = 2157836288.000\n",
      "Iteration   3200 [  3%]: Loss = 3852548864.000\n",
      "Iteration   3300 [  3%]: Loss = 3240022784.000\n",
      "Iteration   3400 [  3%]: Loss = 11718517760.000\n",
      "Iteration   3500 [  3%]: Loss = 7871898624.000\n",
      "Iteration   3600 [  3%]: Loss = 1878582144.000\n",
      "Iteration   3700 [  3%]: Loss = 6132219392.000\n",
      "Iteration   3800 [  3%]: Loss = 1451710720.000\n",
      "Iteration   3900 [  3%]: Loss = 11103127552.000\n",
      "Iteration   4000 [  4%]: Loss = 4536034816.000\n",
      "Iteration   4100 [  4%]: Loss = 6515241984.000\n",
      "Iteration   4200 [  4%]: Loss = 5054609920.000\n",
      "Iteration   4300 [  4%]: Loss = 7011187712.000\n",
      "Iteration   4400 [  4%]: Loss = 2925285888.000\n",
      "Iteration   4500 [  4%]: Loss = 33264996352.000\n",
      "Iteration   4600 [  4%]: Loss = 7610456576.000\n",
      "Iteration   4700 [  4%]: Loss = 2205813760.000\n",
      "Iteration   4800 [  4%]: Loss = 1967789952.000\n",
      "Iteration   4900 [  4%]: Loss = 3043154176.000\n",
      "Iteration   5000 [  5%]: Loss = 2375094016.000\n",
      "Iteration   5100 [  5%]: Loss = 29715423232.000\n",
      "Iteration   5200 [  5%]: Loss = 1744331776.000\n",
      "Iteration   5300 [  5%]: Loss = 5184380928.000\n",
      "Iteration   5400 [  5%]: Loss = 2360053504.000\n",
      "Iteration   5500 [  5%]: Loss = 9979133952.000\n",
      "Iteration   5600 [  5%]: Loss = 4376362496.000\n",
      "Iteration   5700 [  5%]: Loss = 3541321472.000\n",
      "Iteration   5800 [  5%]: Loss = 3391163136.000\n",
      "Iteration   5900 [  5%]: Loss = 5430084608.000\n",
      "Iteration   6000 [  6%]: Loss = 1735022976.000\n",
      "Iteration   6100 [  6%]: Loss = 2146575232.000\n",
      "Iteration   6200 [  6%]: Loss = 1959225472.000\n",
      "Iteration   6300 [  6%]: Loss = 1693085184.000\n",
      "Iteration   6400 [  6%]: Loss = 13248308224.000\n",
      "Iteration   6500 [  6%]: Loss = 1599379328.000\n",
      "Iteration   6600 [  6%]: Loss = 2092096768.000\n",
      "Iteration   6700 [  6%]: Loss = 1861624960.000\n",
      "Iteration   6800 [  6%]: Loss = 2329169408.000\n",
      "Iteration   6900 [  6%]: Loss = 1347920128.000\n",
      "Iteration   7000 [  7%]: Loss = 1324874496.000\n",
      "Iteration   7100 [  7%]: Loss = 2051483392.000\n",
      "Iteration   7200 [  7%]: Loss = 2960958208.000\n",
      "Iteration   7300 [  7%]: Loss = 7822830592.000\n",
      "Iteration   7400 [  7%]: Loss = 9909754880.000\n",
      "Iteration   7500 [  7%]: Loss = 28069206016.000\n",
      "Iteration   7600 [  7%]: Loss = 3020899072.000\n",
      "Iteration   7700 [  7%]: Loss = 1809938432.000\n",
      "Iteration   7800 [  7%]: Loss = 4837470720.000\n",
      "Iteration   7900 [  7%]: Loss = 8249545728.000\n",
      "Iteration   8000 [  8%]: Loss = 2242400768.000\n",
      "Iteration   8100 [  8%]: Loss = 2196532224.000\n",
      "Iteration   8200 [  8%]: Loss = 3549242880.000\n",
      "Iteration   8300 [  8%]: Loss = 2794651904.000\n",
      "Iteration   8400 [  8%]: Loss = 9373490176.000\n",
      "Iteration   8500 [  8%]: Loss = 1423416704.000\n",
      "Iteration   8600 [  8%]: Loss = 6457491968.000\n",
      "Iteration   8700 [  8%]: Loss = 2741001984.000\n",
      "Iteration   8800 [  8%]: Loss = 1215636736.000\n",
      "Iteration   8900 [  8%]: Loss = 2665897728.000\n",
      "Iteration   9000 [  9%]: Loss = 2054820480.000\n",
      "Iteration   9100 [  9%]: Loss = 1591900928.000\n",
      "Iteration   9200 [  9%]: Loss = 3707618304.000\n",
      "Iteration   9300 [  9%]: Loss = 2041452800.000\n",
      "Iteration   9400 [  9%]: Loss = 1656379136.000\n",
      "Iteration   9500 [  9%]: Loss = 1359193216.000\n",
      "Iteration   9600 [  9%]: Loss = 1810452352.000\n",
      "Iteration   9700 [  9%]: Loss = 1963923456.000\n",
      "Iteration   9800 [  9%]: Loss = 1023576704.000\n",
      "Iteration   9900 [  9%]: Loss = 1947775104.000\n",
      "Iteration  10000 [ 10%]: Loss = 6325159424.000\n",
      "Iteration  10100 [ 10%]: Loss = 2567011328.000\n",
      "Iteration  10200 [ 10%]: Loss = 1108772992.000\n",
      "Iteration  10300 [ 10%]: Loss = 3377188864.000\n",
      "Iteration  10400 [ 10%]: Loss = 1475187840.000\n",
      "Iteration  10500 [ 10%]: Loss = 4236261376.000\n",
      "Iteration  10600 [ 10%]: Loss = 2088472832.000\n",
      "Iteration  10700 [ 10%]: Loss = 2297046528.000\n",
      "Iteration  10800 [ 10%]: Loss = 2653572864.000\n",
      "Iteration  10900 [ 10%]: Loss = 4027320832.000\n",
      "Iteration  11000 [ 11%]: Loss = 1775454848.000\n",
      "Iteration  11100 [ 11%]: Loss = 2166616320.000\n",
      "Iteration  11200 [ 11%]: Loss = 1581751552.000\n",
      "Iteration  11300 [ 11%]: Loss = 1377794816.000\n",
      "Iteration  11400 [ 11%]: Loss = 2576247808.000\n",
      "Iteration  11500 [ 11%]: Loss = 1202282496.000\n",
      "Iteration  11600 [ 11%]: Loss = 2944536064.000\n",
      "Iteration  11700 [ 11%]: Loss = 2153213440.000\n",
      "Iteration  11800 [ 11%]: Loss = 20198047744.000\n",
      "Iteration  11900 [ 11%]: Loss = 1013845696.000\n",
      "Iteration  12000 [ 12%]: Loss = 1084356224.000\n",
      "Iteration  12100 [ 12%]: Loss = 2244728320.000\n",
      "Iteration  12200 [ 12%]: Loss = 3717693952.000\n",
      "Iteration  12300 [ 12%]: Loss = 2319725312.000\n",
      "Iteration  12400 [ 12%]: Loss = 1520612992.000\n",
      "Iteration  12500 [ 12%]: Loss = 1348414848.000\n",
      "Iteration  12600 [ 12%]: Loss = 1260616448.000\n",
      "Iteration  12700 [ 12%]: Loss = 879522432.000\n",
      "Iteration  12800 [ 12%]: Loss = 1078257280.000\n",
      "Iteration  12900 [ 12%]: Loss = 961590656.000\n",
      "Iteration  13000 [ 13%]: Loss = 740757056.000\n",
      "Iteration  13100 [ 13%]: Loss = 967076096.000\n",
      "Iteration  13200 [ 13%]: Loss = 1048402560.000\n",
      "Iteration  13300 [ 13%]: Loss = 1242879488.000\n",
      "Iteration  13400 [ 13%]: Loss = 4379400704.000\n",
      "Iteration  13500 [ 13%]: Loss = 632513472.000\n",
      "Iteration  13600 [ 13%]: Loss = 2133020928.000\n",
      "Iteration  13700 [ 13%]: Loss = 708407360.000\n",
      "Iteration  13800 [ 13%]: Loss = 3797328128.000\n",
      "Iteration  13900 [ 13%]: Loss = 2500357376.000\n",
      "Iteration  14000 [ 14%]: Loss = 814270912.000\n",
      "Iteration  14100 [ 14%]: Loss = 1516319616.000\n",
      "Iteration  14200 [ 14%]: Loss = 1414338048.000\n",
      "Iteration  14300 [ 14%]: Loss = 639221504.000\n",
      "Iteration  14400 [ 14%]: Loss = 554075264.000\n",
      "Iteration  14500 [ 14%]: Loss = 640685056.000\n",
      "Iteration  14600 [ 14%]: Loss = 583203968.000\n",
      "Iteration  14700 [ 14%]: Loss = 1173451776.000\n",
      "Iteration  14800 [ 14%]: Loss = 3276569088.000\n",
      "Iteration  14900 [ 14%]: Loss = 716990144.000\n",
      "Iteration  15000 [ 15%]: Loss = 936973120.000\n",
      "Iteration  15100 [ 15%]: Loss = 704397504.000\n",
      "Iteration  15200 [ 15%]: Loss = 1108325504.000\n",
      "Iteration  15300 [ 15%]: Loss = 1105931648.000\n",
      "Iteration  15400 [ 15%]: Loss = 3763034112.000\n",
      "Iteration  15500 [ 15%]: Loss = 668910912.000\n",
      "Iteration  15600 [ 15%]: Loss = 1132817920.000\n",
      "Iteration  15700 [ 15%]: Loss = 2065888768.000\n",
      "Iteration  15800 [ 15%]: Loss = 893068800.000\n",
      "Iteration  15900 [ 15%]: Loss = 1785890560.000\n",
      "Iteration  16000 [ 16%]: Loss = 739273152.000\n",
      "Iteration  16100 [ 16%]: Loss = 1025101440.000\n",
      "Iteration  16200 [ 16%]: Loss = 1498887680.000\n",
      "Iteration  16300 [ 16%]: Loss = 799693632.000\n",
      "Iteration  16400 [ 16%]: Loss = 2180573184.000\n",
      "Iteration  16500 [ 16%]: Loss = 3191514880.000\n",
      "Iteration  16600 [ 16%]: Loss = 1033733440.000\n",
      "Iteration  16700 [ 16%]: Loss = 843630272.000\n",
      "Iteration  16800 [ 16%]: Loss = 978111424.000\n",
      "Iteration  16900 [ 16%]: Loss = 534198432.000\n",
      "Iteration  17000 [ 17%]: Loss = 657194368.000\n",
      "Iteration  17100 [ 17%]: Loss = 752407808.000\n",
      "Iteration  17200 [ 17%]: Loss = 674912704.000\n",
      "Iteration  17300 [ 17%]: Loss = 965830848.000\n",
      "Iteration  17400 [ 17%]: Loss = 488218400.000\n",
      "Iteration  17500 [ 17%]: Loss = 1487044224.000\n",
      "Iteration  17600 [ 17%]: Loss = 642801216.000\n",
      "Iteration  17700 [ 17%]: Loss = 852164096.000\n",
      "Iteration  17800 [ 17%]: Loss = 825563776.000\n",
      "Iteration  17900 [ 17%]: Loss = 1143907328.000\n",
      "Iteration  18000 [ 18%]: Loss = 911480960.000\n",
      "Iteration  18100 [ 18%]: Loss = 544855680.000\n",
      "Iteration  18200 [ 18%]: Loss = 605380928.000\n",
      "Iteration  18300 [ 18%]: Loss = 827542784.000\n",
      "Iteration  18400 [ 18%]: Loss = 1341704320.000\n",
      "Iteration  18500 [ 18%]: Loss = 408026752.000\n",
      "Iteration  18600 [ 18%]: Loss = 1006119744.000\n",
      "Iteration  18700 [ 18%]: Loss = 2034633856.000\n",
      "Iteration  18800 [ 18%]: Loss = 968086400.000\n",
      "Iteration  18900 [ 18%]: Loss = 530038400.000\n",
      "Iteration  19000 [ 19%]: Loss = 961735744.000\n",
      "Iteration  19100 [ 19%]: Loss = 615633984.000\n",
      "Iteration  19200 [ 19%]: Loss = 10816132096.000\n",
      "Iteration  19300 [ 19%]: Loss = 648415872.000\n",
      "Iteration  19400 [ 19%]: Loss = 4270289920.000\n",
      "Iteration  19500 [ 19%]: Loss = 7330343936.000\n",
      "Iteration  19600 [ 19%]: Loss = 522389536.000\n",
      "Iteration  19700 [ 19%]: Loss = 379515168.000\n",
      "Iteration  19800 [ 19%]: Loss = 2028695424.000\n",
      "Iteration  19900 [ 19%]: Loss = 539866624.000\n",
      "Iteration  20000 [ 20%]: Loss = 561424320.000\n",
      "Iteration  20100 [ 20%]: Loss = 950104704.000\n",
      "Iteration  20200 [ 20%]: Loss = 512104704.000\n",
      "Iteration  20300 [ 20%]: Loss = 463652288.000\n",
      "Iteration  20400 [ 20%]: Loss = 1592137344.000\n",
      "Iteration  20500 [ 20%]: Loss = 507239712.000\n",
      "Iteration  20600 [ 20%]: Loss = 625970048.000\n",
      "Iteration  20700 [ 20%]: Loss = 628081856.000\n",
      "Iteration  20800 [ 20%]: Loss = 528688096.000\n",
      "Iteration  20900 [ 20%]: Loss = 458342656.000\n",
      "Iteration  21000 [ 21%]: Loss = 479613952.000\n",
      "Iteration  21100 [ 21%]: Loss = 533980128.000\n",
      "Iteration  21200 [ 21%]: Loss = 507948832.000\n",
      "Iteration  21300 [ 21%]: Loss = 661655040.000\n",
      "Iteration  21400 [ 21%]: Loss = 825097472.000\n",
      "Iteration  21500 [ 21%]: Loss = 793037376.000\n",
      "Iteration  21600 [ 21%]: Loss = 1085225344.000\n",
      "Iteration  21700 [ 21%]: Loss = 2490526720.000\n",
      "Iteration  21800 [ 21%]: Loss = 765287680.000\n",
      "Iteration  21900 [ 21%]: Loss = 1303369216.000\n",
      "Iteration  22000 [ 22%]: Loss = 2931412480.000\n",
      "Iteration  22100 [ 22%]: Loss = 343087648.000\n",
      "Iteration  22200 [ 22%]: Loss = 600188416.000\n",
      "Iteration  22300 [ 22%]: Loss = 2431821056.000\n",
      "Iteration  22400 [ 22%]: Loss = 635824960.000\n",
      "Iteration  22500 [ 22%]: Loss = 294975936.000\n",
      "Iteration  22600 [ 22%]: Loss = 1482572672.000\n",
      "Iteration  22700 [ 22%]: Loss = 652454976.000\n",
      "Iteration  22800 [ 22%]: Loss = 1071681344.000\n",
      "Iteration  22900 [ 22%]: Loss = 499490432.000\n",
      "Iteration  23000 [ 23%]: Loss = 2596870656.000\n",
      "Iteration  23100 [ 23%]: Loss = 397196096.000\n",
      "Iteration  23200 [ 23%]: Loss = 344942560.000\n",
      "Iteration  23300 [ 23%]: Loss = 405430432.000\n",
      "Iteration  23400 [ 23%]: Loss = 171907696.000\n",
      "Iteration  23500 [ 23%]: Loss = 351140864.000\n",
      "Iteration  23600 [ 23%]: Loss = 1430457088.000\n",
      "Iteration  23700 [ 23%]: Loss = 380227424.000\n",
      "Iteration  23800 [ 23%]: Loss = 1494033408.000\n",
      "Iteration  23900 [ 23%]: Loss = 568779520.000\n",
      "Iteration  24000 [ 24%]: Loss = 263880736.000\n",
      "Iteration  24100 [ 24%]: Loss = 317229888.000\n",
      "Iteration  24200 [ 24%]: Loss = 318329952.000\n",
      "Iteration  24300 [ 24%]: Loss = 1784739840.000\n",
      "Iteration  24400 [ 24%]: Loss = 354117504.000\n",
      "Iteration  24500 [ 24%]: Loss = 416815488.000\n",
      "Iteration  24600 [ 24%]: Loss = 362734592.000\n",
      "Iteration  24700 [ 24%]: Loss = 1350291968.000\n",
      "Iteration  24800 [ 24%]: Loss = 419658880.000\n",
      "Iteration  24900 [ 24%]: Loss = 560447040.000\n",
      "Iteration  25000 [ 25%]: Loss = 252635760.000\n",
      "Iteration  25100 [ 25%]: Loss = 397646944.000\n",
      "Iteration  25200 [ 25%]: Loss = 644022400.000\n",
      "Iteration  25300 [ 25%]: Loss = 1092047872.000\n",
      "Iteration  25400 [ 25%]: Loss = 483533920.000\n",
      "Iteration  25500 [ 25%]: Loss = 434837152.000\n",
      "Iteration  25600 [ 25%]: Loss = 1242848768.000\n",
      "Iteration  25700 [ 25%]: Loss = 240371760.000\n",
      "Iteration  25800 [ 25%]: Loss = 241575584.000\n",
      "Iteration  25900 [ 25%]: Loss = 214336384.000\n",
      "Iteration  26000 [ 26%]: Loss = 592458624.000\n",
      "Iteration  26100 [ 26%]: Loss = 208400176.000\n",
      "Iteration  26200 [ 26%]: Loss = 459845536.000\n",
      "Iteration  26300 [ 26%]: Loss = 237068896.000\n",
      "Iteration  26400 [ 26%]: Loss = 304047456.000\n",
      "Iteration  26500 [ 26%]: Loss = 556844736.000\n",
      "Iteration  26600 [ 26%]: Loss = 230229600.000\n",
      "Iteration  26700 [ 26%]: Loss = 286556192.000\n",
      "Iteration  26800 [ 26%]: Loss = 547881024.000\n",
      "Iteration  26900 [ 26%]: Loss = 296195616.000\n",
      "Iteration  27000 [ 27%]: Loss = 415616416.000\n",
      "Iteration  27100 [ 27%]: Loss = 307314208.000\n",
      "Iteration  27200 [ 27%]: Loss = 223276960.000\n",
      "Iteration  27300 [ 27%]: Loss = 475221888.000\n",
      "Iteration  27400 [ 27%]: Loss = 472043104.000\n",
      "Iteration  27500 [ 27%]: Loss = 263597456.000\n",
      "Iteration  27600 [ 27%]: Loss = 240002096.000\n",
      "Iteration  27700 [ 27%]: Loss = 667510912.000\n",
      "Iteration  27800 [ 27%]: Loss = 223790704.000\n",
      "Iteration  27900 [ 27%]: Loss = 252173632.000\n",
      "Iteration  28000 [ 28%]: Loss = 218825392.000\n",
      "Iteration  28100 [ 28%]: Loss = 803662336.000\n",
      "Iteration  28200 [ 28%]: Loss = 941492608.000\n",
      "Iteration  28300 [ 28%]: Loss = 173003008.000\n",
      "Iteration  28400 [ 28%]: Loss = 241260256.000\n",
      "Iteration  28500 [ 28%]: Loss = 551388352.000\n",
      "Iteration  28600 [ 28%]: Loss = 630829696.000\n",
      "Iteration  28700 [ 28%]: Loss = 441964224.000\n",
      "Iteration  28800 [ 28%]: Loss = 352576928.000\n",
      "Iteration  28900 [ 28%]: Loss = 291163392.000\n",
      "Iteration  29000 [ 28%]: Loss = 423336576.000\n",
      "Iteration  29100 [ 29%]: Loss = 667792960.000\n",
      "Iteration  29200 [ 29%]: Loss = 311889344.000\n",
      "Iteration  29300 [ 29%]: Loss = 1050343680.000\n",
      "Iteration  29400 [ 29%]: Loss = 204095872.000\n",
      "Iteration  29500 [ 29%]: Loss = 267145792.000\n",
      "Iteration  29600 [ 29%]: Loss = 193872880.000\n",
      "Iteration  29700 [ 29%]: Loss = 1020027712.000\n",
      "Iteration  29800 [ 29%]: Loss = 246303312.000\n",
      "Iteration  29900 [ 29%]: Loss = 160512032.000\n",
      "Iteration  30000 [ 30%]: Loss = 852409664.000\n",
      "Iteration  30100 [ 30%]: Loss = 267544160.000\n",
      "Iteration  30200 [ 30%]: Loss = 113537584.000\n",
      "Iteration  30300 [ 30%]: Loss = 228947728.000\n",
      "Iteration  30400 [ 30%]: Loss = 211331824.000\n",
      "Iteration  30500 [ 30%]: Loss = 264209328.000\n",
      "Iteration  30600 [ 30%]: Loss = 672938240.000\n",
      "Iteration  30700 [ 30%]: Loss = 309534816.000\n",
      "Iteration  30800 [ 30%]: Loss = 3199638528.000\n",
      "Iteration  30900 [ 30%]: Loss = 166618448.000\n",
      "Iteration  31000 [ 31%]: Loss = 819841088.000\n",
      "Iteration  31100 [ 31%]: Loss = 185110592.000\n",
      "Iteration  31200 [ 31%]: Loss = 450967584.000\n",
      "Iteration  31300 [ 31%]: Loss = 153613312.000\n",
      "Iteration  31400 [ 31%]: Loss = 426897024.000\n",
      "Iteration  31500 [ 31%]: Loss = 261472496.000\n",
      "Iteration  31600 [ 31%]: Loss = 211491728.000\n",
      "Iteration  31700 [ 31%]: Loss = 279312864.000\n",
      "Iteration  31800 [ 31%]: Loss = 221300784.000\n",
      "Iteration  31900 [ 31%]: Loss = 312090976.000\n",
      "Iteration  32000 [ 32%]: Loss = 137132240.000\n",
      "Iteration  32100 [ 32%]: Loss = 291245248.000\n",
      "Iteration  32200 [ 32%]: Loss = 289637376.000\n",
      "Iteration  32300 [ 32%]: Loss = 277898528.000\n",
      "Iteration  32400 [ 32%]: Loss = 620524544.000\n",
      "Iteration  32500 [ 32%]: Loss = 278821760.000\n",
      "Iteration  32600 [ 32%]: Loss = 736246400.000\n",
      "Iteration  32700 [ 32%]: Loss = 152230384.000\n",
      "Iteration  32800 [ 32%]: Loss = 87735280.000\n",
      "Iteration  32900 [ 32%]: Loss = 163675408.000\n",
      "Iteration  33000 [ 33%]: Loss = 155354752.000\n",
      "Iteration  33100 [ 33%]: Loss = 176276240.000\n",
      "Iteration  33200 [ 33%]: Loss = 342619360.000\n",
      "Iteration  33300 [ 33%]: Loss = 175935008.000\n",
      "Iteration  33400 [ 33%]: Loss = 205646000.000\n",
      "Iteration  33500 [ 33%]: Loss = 206899280.000\n",
      "Iteration  33600 [ 33%]: Loss = 252497264.000\n",
      "Iteration  33700 [ 33%]: Loss = 142728128.000\n",
      "Iteration  33800 [ 33%]: Loss = 81965928.000\n",
      "Iteration  33900 [ 33%]: Loss = 4141743616.000\n",
      "Iteration  34000 [ 34%]: Loss = 141978384.000\n",
      "Iteration  34100 [ 34%]: Loss = 95500096.000\n",
      "Iteration  34200 [ 34%]: Loss = 95818112.000\n",
      "Iteration  34300 [ 34%]: Loss = 118332608.000\n",
      "Iteration  34400 [ 34%]: Loss = 637720960.000\n",
      "Iteration  34500 [ 34%]: Loss = 5737149952.000\n",
      "Iteration  34600 [ 34%]: Loss = 328637408.000\n",
      "Iteration  34700 [ 34%]: Loss = 102522488.000\n",
      "Iteration  34800 [ 34%]: Loss = 134891968.000\n",
      "Iteration  34900 [ 34%]: Loss = 103421320.000\n",
      "Iteration  35000 [ 35%]: Loss = 96914120.000\n",
      "Iteration  35100 [ 35%]: Loss = 108975552.000\n",
      "Iteration  35200 [ 35%]: Loss = 314114496.000\n",
      "Iteration  35300 [ 35%]: Loss = 125438040.000\n",
      "Iteration  35400 [ 35%]: Loss = 354452064.000\n",
      "Iteration  35500 [ 35%]: Loss = 395415040.000\n",
      "Iteration  35600 [ 35%]: Loss = 109904632.000\n",
      "Iteration  35700 [ 35%]: Loss = 125719080.000\n",
      "Iteration  35800 [ 35%]: Loss = 100505816.000\n",
      "Iteration  35900 [ 35%]: Loss = 262938496.000\n",
      "Iteration  36000 [ 36%]: Loss = 83745080.000\n",
      "Iteration  36100 [ 36%]: Loss = 313113568.000\n",
      "Iteration  36200 [ 36%]: Loss = 138161216.000\n",
      "Iteration  36300 [ 36%]: Loss = 293467712.000\n",
      "Iteration  36400 [ 36%]: Loss = 297872224.000\n",
      "Iteration  36500 [ 36%]: Loss = 65708484.000\n",
      "Iteration  36600 [ 36%]: Loss = 308968192.000\n",
      "Iteration  36700 [ 36%]: Loss = 378638784.000\n",
      "Iteration  36800 [ 36%]: Loss = 88951232.000\n",
      "Iteration  36900 [ 36%]: Loss = 90377984.000\n",
      "Iteration  37000 [ 37%]: Loss = 77467152.000\n",
      "Iteration  37100 [ 37%]: Loss = 264064512.000\n",
      "Iteration  37200 [ 37%]: Loss = 249077488.000\n",
      "Iteration  37300 [ 37%]: Loss = 129477224.000\n",
      "Iteration  37400 [ 37%]: Loss = 67594464.000\n",
      "Iteration  37500 [ 37%]: Loss = 87344264.000\n",
      "Iteration  37600 [ 37%]: Loss = 657117632.000\n",
      "Iteration  37700 [ 37%]: Loss = 317903104.000\n",
      "Iteration  37800 [ 37%]: Loss = 80533064.000\n",
      "Iteration  37900 [ 37%]: Loss = 578478272.000\n",
      "Iteration  38000 [ 38%]: Loss = 69181712.000\n",
      "Iteration  38100 [ 38%]: Loss = 198294208.000\n",
      "Iteration  38200 [ 38%]: Loss = 112789856.000\n",
      "Iteration  38300 [ 38%]: Loss = 239854864.000\n",
      "Iteration  38400 [ 38%]: Loss = 413243296.000\n",
      "Iteration  38500 [ 38%]: Loss = 72474920.000\n",
      "Iteration  38600 [ 38%]: Loss = 94010128.000\n",
      "Iteration  38700 [ 38%]: Loss = 330327296.000\n",
      "Iteration  38800 [ 38%]: Loss = 77618656.000\n",
      "Iteration  38900 [ 38%]: Loss = 84588344.000\n",
      "Iteration  39000 [ 39%]: Loss = 97807752.000\n",
      "Iteration  39100 [ 39%]: Loss = 221650608.000\n",
      "Iteration  39200 [ 39%]: Loss = 62847120.000\n",
      "Iteration  39300 [ 39%]: Loss = 77388680.000\n",
      "Iteration  39400 [ 39%]: Loss = 138377024.000\n",
      "Iteration  39500 [ 39%]: Loss = 166269552.000\n",
      "Iteration  39600 [ 39%]: Loss = 98971896.000\n",
      "Iteration  39700 [ 39%]: Loss = 235069200.000\n",
      "Iteration  39800 [ 39%]: Loss = 89945288.000\n",
      "Iteration  39900 [ 39%]: Loss = 87504576.000\n",
      "Iteration  40000 [ 40%]: Loss = 74678328.000\n",
      "Iteration  40100 [ 40%]: Loss = 43034392.000\n",
      "Iteration  40200 [ 40%]: Loss = 90659920.000\n",
      "Iteration  40300 [ 40%]: Loss = 73971224.000\n",
      "Iteration  40400 [ 40%]: Loss = 76772024.000\n",
      "Iteration  40500 [ 40%]: Loss = 52828652.000\n",
      "Iteration  40600 [ 40%]: Loss = 66094364.000\n",
      "Iteration  40700 [ 40%]: Loss = 181199808.000\n",
      "Iteration  40800 [ 40%]: Loss = 84481936.000\n",
      "Iteration  40900 [ 40%]: Loss = 74369048.000\n",
      "Iteration  41000 [ 41%]: Loss = 200968224.000\n",
      "Iteration  41100 [ 41%]: Loss = 52513092.000\n",
      "Iteration  41200 [ 41%]: Loss = 79417352.000\n",
      "Iteration  41300 [ 41%]: Loss = 58963804.000\n",
      "Iteration  41400 [ 41%]: Loss = 65191864.000\n",
      "Iteration  41500 [ 41%]: Loss = 55784648.000\n",
      "Iteration  41600 [ 41%]: Loss = 73787232.000\n",
      "Iteration  41700 [ 41%]: Loss = 76094912.000\n",
      "Iteration  41800 [ 41%]: Loss = 65213620.000\n",
      "Iteration  41900 [ 41%]: Loss = 58135356.000\n",
      "Iteration  42000 [ 42%]: Loss = 66953068.000\n",
      "Iteration  42100 [ 42%]: Loss = 74466496.000\n",
      "Iteration  42200 [ 42%]: Loss = 2643661824.000\n",
      "Iteration  42300 [ 42%]: Loss = 160776224.000\n",
      "Iteration  42400 [ 42%]: Loss = 49070484.000\n",
      "Iteration  42500 [ 42%]: Loss = 72559640.000\n",
      "Iteration  42600 [ 42%]: Loss = 48401672.000\n",
      "Iteration  42700 [ 42%]: Loss = 39301720.000\n",
      "Iteration  42800 [ 42%]: Loss = 49215224.000\n",
      "Iteration  42900 [ 42%]: Loss = 155363536.000\n",
      "Iteration  43000 [ 43%]: Loss = 85889672.000\n",
      "Iteration  43100 [ 43%]: Loss = 44227856.000\n",
      "Iteration  43200 [ 43%]: Loss = 48423724.000\n",
      "Iteration  43300 [ 43%]: Loss = 75575880.000\n",
      "Iteration  43400 [ 43%]: Loss = 75872368.000\n",
      "Iteration  43500 [ 43%]: Loss = 41160648.000\n",
      "Iteration  43600 [ 43%]: Loss = 41703072.000\n",
      "Iteration  43700 [ 43%]: Loss = 34586744.000\n",
      "Iteration  43800 [ 43%]: Loss = 63043848.000\n",
      "Iteration  43900 [ 43%]: Loss = 41177864.000\n",
      "Iteration  44000 [ 44%]: Loss = 145234240.000\n",
      "Iteration  44100 [ 44%]: Loss = 1535114240.000\n",
      "Iteration  44200 [ 44%]: Loss = 67328272.000\n",
      "Iteration  44300 [ 44%]: Loss = 59545784.000\n",
      "Iteration  44400 [ 44%]: Loss = 36080736.000\n",
      "Iteration  44500 [ 44%]: Loss = 55790764.000\n",
      "Iteration  44600 [ 44%]: Loss = 76279208.000\n",
      "Iteration  44700 [ 44%]: Loss = 89382656.000\n",
      "Iteration  44800 [ 44%]: Loss = 66916224.000\n",
      "Iteration  44900 [ 44%]: Loss = 76122984.000\n",
      "Iteration  45000 [ 45%]: Loss = 35395828.000\n",
      "Iteration  45100 [ 45%]: Loss = 129908296.000\n",
      "Iteration  45200 [ 45%]: Loss = 24552950.000\n",
      "Iteration  45300 [ 45%]: Loss = 52201820.000\n",
      "Iteration  45400 [ 45%]: Loss = 52236128.000\n",
      "Iteration  45500 [ 45%]: Loss = 35330968.000\n",
      "Iteration  45600 [ 45%]: Loss = 40409756.000\n",
      "Iteration  45700 [ 45%]: Loss = 31853892.000\n",
      "Iteration  45800 [ 45%]: Loss = 27863518.000\n",
      "Iteration  45900 [ 45%]: Loss = 38200012.000\n",
      "Iteration  46000 [ 46%]: Loss = 97040136.000\n",
      "Iteration  46100 [ 46%]: Loss = 57540588.000\n",
      "Iteration  46200 [ 46%]: Loss = 49168248.000\n",
      "Iteration  46300 [ 46%]: Loss = 48390324.000\n",
      "Iteration  46400 [ 46%]: Loss = 37312348.000\n",
      "Iteration  46500 [ 46%]: Loss = 74792392.000\n",
      "Iteration  46600 [ 46%]: Loss = 31562958.000\n",
      "Iteration  46700 [ 46%]: Loss = 39117064.000\n",
      "Iteration  46800 [ 46%]: Loss = 34958316.000\n",
      "Iteration  46900 [ 46%]: Loss = 31368756.000\n",
      "Iteration  47000 [ 47%]: Loss = 56317128.000\n",
      "Iteration  47100 [ 47%]: Loss = 40090880.000\n",
      "Iteration  47200 [ 47%]: Loss = 27698548.000\n",
      "Iteration  47300 [ 47%]: Loss = 28262532.000\n",
      "Iteration  47400 [ 47%]: Loss = 42418704.000\n",
      "Iteration  47500 [ 47%]: Loss = 62720820.000\n",
      "Iteration  47600 [ 47%]: Loss = 56092244.000\n",
      "Iteration  47700 [ 47%]: Loss = 35115864.000\n",
      "Iteration  47800 [ 47%]: Loss = 122154480.000\n",
      "Iteration  47900 [ 47%]: Loss = 27522822.000\n",
      "Iteration  48000 [ 48%]: Loss = 52343732.000\n",
      "Iteration  48100 [ 48%]: Loss = 27546274.000\n",
      "Iteration  48200 [ 48%]: Loss = 21578162.000\n",
      "Iteration  48300 [ 48%]: Loss = 28424602.000\n",
      "Iteration  48400 [ 48%]: Loss = 31234880.000\n",
      "Iteration  48500 [ 48%]: Loss = 35525376.000\n",
      "Iteration  48600 [ 48%]: Loss = 41800552.000\n",
      "Iteration  48700 [ 48%]: Loss = 30477510.000\n",
      "Iteration  48800 [ 48%]: Loss = 71592616.000\n",
      "Iteration  48900 [ 48%]: Loss = 97136192.000\n",
      "Iteration  49000 [ 49%]: Loss = 29475110.000\n",
      "Iteration  49100 [ 49%]: Loss = 28933726.000\n",
      "Iteration  49200 [ 49%]: Loss = 34074284.000\n",
      "Iteration  49300 [ 49%]: Loss = 28636980.000\n",
      "Iteration  49400 [ 49%]: Loss = 28181846.000\n",
      "Iteration  49500 [ 49%]: Loss = 69931352.000\n",
      "Iteration  49600 [ 49%]: Loss = 37365720.000\n",
      "Iteration  49700 [ 49%]: Loss = 29887156.000\n",
      "Iteration  49800 [ 49%]: Loss = 24081036.000\n",
      "Iteration  49900 [ 49%]: Loss = 317185984.000\n",
      "Iteration  50000 [ 50%]: Loss = 98001960.000\n",
      "Iteration  50100 [ 50%]: Loss = 69019800.000\n",
      "Iteration  50200 [ 50%]: Loss = 43861552.000\n",
      "Iteration  50300 [ 50%]: Loss = 15232043.000\n",
      "Iteration  50400 [ 50%]: Loss = 25729750.000\n",
      "Iteration  50500 [ 50%]: Loss = 21942324.000\n",
      "Iteration  50600 [ 50%]: Loss = 37677360.000\n",
      "Iteration  50700 [ 50%]: Loss = 36049656.000\n",
      "Iteration  50800 [ 50%]: Loss = 28241590.000\n",
      "Iteration  50900 [ 50%]: Loss = 25942772.000\n",
      "Iteration  51000 [ 51%]: Loss = 20894040.000\n",
      "Iteration  51100 [ 51%]: Loss = 56994776.000\n",
      "Iteration  51200 [ 51%]: Loss = 28816096.000\n",
      "Iteration  51300 [ 51%]: Loss = 143471936.000\n",
      "Iteration  51400 [ 51%]: Loss = 22130316.000\n",
      "Iteration  51500 [ 51%]: Loss = 21583106.000\n",
      "Iteration  51600 [ 51%]: Loss = 18869208.000\n",
      "Iteration  51700 [ 51%]: Loss = 19243408.000\n",
      "Iteration  51800 [ 51%]: Loss = 16376731.000\n",
      "Iteration  51900 [ 51%]: Loss = 18593448.000\n",
      "Iteration  52000 [ 52%]: Loss = 33038138.000\n",
      "Iteration  52100 [ 52%]: Loss = 14188037.000\n",
      "Iteration  52200 [ 52%]: Loss = 91992600.000\n",
      "Iteration  52300 [ 52%]: Loss = 17006922.000\n",
      "Iteration  52400 [ 52%]: Loss = 40104932.000\n",
      "Iteration  52500 [ 52%]: Loss = 52809980.000\n",
      "Iteration  52600 [ 52%]: Loss = 91364872.000\n",
      "Iteration  52700 [ 52%]: Loss = 121011712.000\n",
      "Iteration  52800 [ 52%]: Loss = 28028332.000\n",
      "Iteration  52900 [ 52%]: Loss = 12049118.000\n",
      "Iteration  53000 [ 53%]: Loss = 19649588.000\n",
      "Iteration  53100 [ 53%]: Loss = 36891780.000\n",
      "Iteration  53200 [ 53%]: Loss = 32393032.000\n",
      "Iteration  53300 [ 53%]: Loss = 17841454.000\n",
      "Iteration  53400 [ 53%]: Loss = 24030352.000\n",
      "Iteration  53500 [ 53%]: Loss = 20853860.000\n",
      "Iteration  53600 [ 53%]: Loss = 17159538.000\n",
      "Iteration  53700 [ 53%]: Loss = 19370008.000\n",
      "Iteration  53800 [ 53%]: Loss = 17912246.000\n",
      "Iteration  53900 [ 53%]: Loss = 25539404.000\n",
      "Iteration  54000 [ 54%]: Loss = 17506868.000\n",
      "Iteration  54100 [ 54%]: Loss = 40579552.000\n",
      "Iteration  54200 [ 54%]: Loss = 12054570.000\n",
      "Iteration  54300 [ 54%]: Loss = 18359038.000\n",
      "Iteration  54400 [ 54%]: Loss = 19978696.000\n",
      "Iteration  54500 [ 54%]: Loss = 22274060.000\n",
      "Iteration  54600 [ 54%]: Loss = 10250374.000\n",
      "Iteration  54700 [ 54%]: Loss = 21165262.000\n",
      "Iteration  54800 [ 54%]: Loss = 38407812.000\n",
      "Iteration  54900 [ 54%]: Loss = 20919920.000\n",
      "Iteration  55000 [ 55%]: Loss = 160032064.000\n",
      "Iteration  55100 [ 55%]: Loss = 24871482.000\n",
      "Iteration  55200 [ 55%]: Loss = 11400980.000\n",
      "Iteration  55300 [ 55%]: Loss = 20652618.000\n",
      "Iteration  55400 [ 55%]: Loss = 27184772.000\n",
      "Iteration  55500 [ 55%]: Loss = 10912877.000\n",
      "Iteration  55600 [ 55%]: Loss = 11970143.000\n",
      "Iteration  55700 [ 55%]: Loss = 11532701.000\n",
      "Iteration  55800 [ 55%]: Loss = 40791220.000\n",
      "Iteration  55900 [ 55%]: Loss = 21149286.000\n",
      "Iteration  56000 [ 56%]: Loss = 13071992.000\n",
      "Iteration  56100 [ 56%]: Loss = 26715928.000\n",
      "Iteration  56200 [ 56%]: Loss = 10450319.000\n",
      "Iteration  56300 [ 56%]: Loss = 11071832.000\n",
      "Iteration  56400 [ 56%]: Loss = 13676137.000\n",
      "Iteration  56500 [ 56%]: Loss = 15190465.000\n",
      "Iteration  56600 [ 56%]: Loss = 14050482.000\n",
      "Iteration  56700 [ 56%]: Loss = 20093820.000\n",
      "Iteration  56800 [ 56%]: Loss = 29218032.000\n",
      "Iteration  56900 [ 56%]: Loss = 13822064.000\n",
      "Iteration  57000 [ 56%]: Loss = 11854790.000\n",
      "Iteration  57100 [ 57%]: Loss = 11787908.000\n",
      "Iteration  57200 [ 57%]: Loss = 22174444.000\n",
      "Iteration  57300 [ 57%]: Loss = 14070576.000\n",
      "Iteration  57400 [ 57%]: Loss = 7802942.000\n",
      "Iteration  57500 [ 57%]: Loss = 30321866.000\n",
      "Iteration  57600 [ 57%]: Loss = 15221095.000\n",
      "Iteration  57700 [ 57%]: Loss = 12309550.000\n",
      "Iteration  57800 [ 57%]: Loss = 9112360.000\n",
      "Iteration  57900 [ 57%]: Loss = 10354010.000\n",
      "Iteration  58000 [ 57%]: Loss = 11916946.000\n",
      "Iteration  58100 [ 58%]: Loss = 34293100.000\n",
      "Iteration  58200 [ 58%]: Loss = 8830149.000\n",
      "Iteration  58300 [ 58%]: Loss = 9877098.000\n",
      "Iteration  58400 [ 58%]: Loss = 7185591.000\n",
      "Iteration  58500 [ 58%]: Loss = 26115272.000\n",
      "Iteration  58600 [ 58%]: Loss = 9357920.000\n",
      "Iteration  58700 [ 58%]: Loss = 13956548.000\n",
      "Iteration  58800 [ 58%]: Loss = 6318922.000\n",
      "Iteration  58900 [ 58%]: Loss = 20259412.000\n",
      "Iteration  59000 [ 59%]: Loss = 14382029.000\n",
      "Iteration  59100 [ 59%]: Loss = 15780916.000\n",
      "Iteration  59200 [ 59%]: Loss = 39548664.000\n",
      "Iteration  59300 [ 59%]: Loss = 14066774.000\n",
      "Iteration  59400 [ 59%]: Loss = 14925209.000\n",
      "Iteration  59500 [ 59%]: Loss = 7225390.500\n",
      "Iteration  59600 [ 59%]: Loss = 8831693.000\n",
      "Iteration  59700 [ 59%]: Loss = 8211180.000\n",
      "Iteration  59800 [ 59%]: Loss = 34099108.000\n",
      "Iteration  59900 [ 59%]: Loss = 12285816.000\n",
      "Iteration  60000 [ 60%]: Loss = 9841012.000\n",
      "Iteration  60100 [ 60%]: Loss = 11790177.000\n",
      "Iteration  60200 [ 60%]: Loss = 10207728.000\n",
      "Iteration  60300 [ 60%]: Loss = 13072027.000\n",
      "Iteration  60400 [ 60%]: Loss = 10162655.000\n",
      "Iteration  60500 [ 60%]: Loss = 19379370.000\n",
      "Iteration  60600 [ 60%]: Loss = 9800349.000\n",
      "Iteration  60700 [ 60%]: Loss = 6520296.500\n",
      "Iteration  60800 [ 60%]: Loss = 8886339.000\n",
      "Iteration  60900 [ 60%]: Loss = 14629830.000\n",
      "Iteration  61000 [ 61%]: Loss = 12130469.000\n",
      "Iteration  61100 [ 61%]: Loss = 9234588.000\n",
      "Iteration  61200 [ 61%]: Loss = 6478005.500\n",
      "Iteration  61300 [ 61%]: Loss = 9563554.000\n",
      "Iteration  61400 [ 61%]: Loss = 11682481.000\n",
      "Iteration  61500 [ 61%]: Loss = 9796848.000\n",
      "Iteration  61600 [ 61%]: Loss = 7215738.000\n",
      "Iteration  61700 [ 61%]: Loss = 7461119.500\n",
      "Iteration  61800 [ 61%]: Loss = 8806995.000\n",
      "Iteration  61900 [ 61%]: Loss = 7255784.000\n",
      "Iteration  62000 [ 62%]: Loss = 7745003.000\n",
      "Iteration  62100 [ 62%]: Loss = 9358278.000\n",
      "Iteration  62200 [ 62%]: Loss = 6288967.000\n",
      "Iteration  62300 [ 62%]: Loss = 6913590.500\n",
      "Iteration  62400 [ 62%]: Loss = 6072137.000\n",
      "Iteration  62500 [ 62%]: Loss = 6553301.500\n",
      "Iteration  62600 [ 62%]: Loss = 6502955.500\n",
      "Iteration  62700 [ 62%]: Loss = 9158646.000\n",
      "Iteration  62800 [ 62%]: Loss = 4996534.000\n",
      "Iteration  62900 [ 62%]: Loss = 8787899.000\n",
      "Iteration  63000 [ 63%]: Loss = 7255499.500\n",
      "Iteration  63100 [ 63%]: Loss = 384646848.000\n",
      "Iteration  63200 [ 63%]: Loss = 9108743.000\n",
      "Iteration  63300 [ 63%]: Loss = 8477808.000\n",
      "Iteration  63400 [ 63%]: Loss = 6919493.500\n",
      "Iteration  63500 [ 63%]: Loss = 7972555.000\n",
      "Iteration  63600 [ 63%]: Loss = 5969468.500\n",
      "Iteration  63700 [ 63%]: Loss = 5105451.000\n",
      "Iteration  63800 [ 63%]: Loss = 6815787.500\n",
      "Iteration  63900 [ 63%]: Loss = 10737336.000\n",
      "Iteration  64000 [ 64%]: Loss = 6139808.000\n",
      "Iteration  64100 [ 64%]: Loss = 5700004.000\n",
      "Iteration  64200 [ 64%]: Loss = 6161268.500\n",
      "Iteration  64300 [ 64%]: Loss = 9175554.000\n",
      "Iteration  64400 [ 64%]: Loss = 5639426.000\n",
      "Iteration  64500 [ 64%]: Loss = 11015977.000\n",
      "Iteration  64600 [ 64%]: Loss = 5072128.000\n",
      "Iteration  64700 [ 64%]: Loss = 4742368.000\n",
      "Iteration  64800 [ 64%]: Loss = 6276960.500\n",
      "Iteration  64900 [ 64%]: Loss = 4935607.500\n",
      "Iteration  65000 [ 65%]: Loss = 5990669.000\n",
      "Iteration  65100 [ 65%]: Loss = 6684251.500\n",
      "Iteration  65200 [ 65%]: Loss = 5277608.500\n",
      "Iteration  65300 [ 65%]: Loss = 8766332.000\n",
      "Iteration  65400 [ 65%]: Loss = 7691035.500\n",
      "Iteration  65500 [ 65%]: Loss = 8043321.500\n",
      "Iteration  65600 [ 65%]: Loss = 10625303.000\n",
      "Iteration  65700 [ 65%]: Loss = 7146192.000\n",
      "Iteration  65800 [ 65%]: Loss = 5430858.000\n",
      "Iteration  65900 [ 65%]: Loss = 6058279.500\n",
      "Iteration  66000 [ 66%]: Loss = 7361536.000\n",
      "Iteration  66100 [ 66%]: Loss = 4725144.500\n",
      "Iteration  66200 [ 66%]: Loss = 4065027.500\n",
      "Iteration  66300 [ 66%]: Loss = 5331539.000\n",
      "Iteration  66400 [ 66%]: Loss = 7012407.000\n",
      "Iteration  66500 [ 66%]: Loss = 6151023.000\n",
      "Iteration  66600 [ 66%]: Loss = 7860678.500\n",
      "Iteration  66700 [ 66%]: Loss = 6179238.500\n",
      "Iteration  66800 [ 66%]: Loss = 5073634.500\n",
      "Iteration  66900 [ 66%]: Loss = 5956759.000\n",
      "Iteration  67000 [ 67%]: Loss = 4816308.500\n",
      "Iteration  67100 [ 67%]: Loss = 7483054.500\n",
      "Iteration  67200 [ 67%]: Loss = 5646917.500\n",
      "Iteration  67300 [ 67%]: Loss = 5272172.000\n",
      "Iteration  67400 [ 67%]: Loss = 4333888.000\n",
      "Iteration  67500 [ 67%]: Loss = 4243616.000\n",
      "Iteration  67600 [ 67%]: Loss = 4788600.500\n",
      "Iteration  67700 [ 67%]: Loss = 4305154.500\n",
      "Iteration  67800 [ 67%]: Loss = 4670296.500\n",
      "Iteration  67900 [ 67%]: Loss = 4621123.000\n",
      "Iteration  68000 [ 68%]: Loss = 5494221.500\n",
      "Iteration  68100 [ 68%]: Loss = 6528245.500\n",
      "Iteration  68200 [ 68%]: Loss = 23938920.000\n",
      "Iteration  68300 [ 68%]: Loss = 4608013.500\n",
      "Iteration  68400 [ 68%]: Loss = 3813140.000\n",
      "Iteration  68500 [ 68%]: Loss = 4048581.500\n",
      "Iteration  68600 [ 68%]: Loss = 4082264.000\n",
      "Iteration  68700 [ 68%]: Loss = 4007066.250\n",
      "Iteration  68800 [ 68%]: Loss = 3604847.500\n",
      "Iteration  68900 [ 68%]: Loss = 4178276.500\n",
      "Iteration  69000 [ 69%]: Loss = 4309752.000\n",
      "Iteration  69100 [ 69%]: Loss = 3176369.500\n",
      "Iteration  69200 [ 69%]: Loss = 3359732.250\n",
      "Iteration  69300 [ 69%]: Loss = 4463320.500\n",
      "Iteration  69400 [ 69%]: Loss = 3780973.000\n",
      "Iteration  69500 [ 69%]: Loss = 17650398.000\n",
      "Iteration  69600 [ 69%]: Loss = 3391300.000\n",
      "Iteration  69700 [ 69%]: Loss = 4924305.000\n",
      "Iteration  69800 [ 69%]: Loss = 5184710.000\n",
      "Iteration  69900 [ 69%]: Loss = 4654318.500\n",
      "Iteration  70000 [ 70%]: Loss = 3685837.250\n",
      "Iteration  70100 [ 70%]: Loss = 3476009.000\n",
      "Iteration  70200 [ 70%]: Loss = 8468945.000\n",
      "Iteration  70300 [ 70%]: Loss = 3753783.250\n",
      "Iteration  70400 [ 70%]: Loss = 3178769.250\n",
      "Iteration  70500 [ 70%]: Loss = 5352577.000\n",
      "Iteration  70600 [ 70%]: Loss = 3757090.500\n",
      "Iteration  70700 [ 70%]: Loss = 11587902.000\n",
      "Iteration  70800 [ 70%]: Loss = 4060626.750\n",
      "Iteration  70900 [ 70%]: Loss = 3144762.250\n",
      "Iteration  71000 [ 71%]: Loss = 3156146.000\n",
      "Iteration  71100 [ 71%]: Loss = 3020680.750\n",
      "Iteration  71200 [ 71%]: Loss = 4172927.250\n",
      "Iteration  71300 [ 71%]: Loss = 3356011.500\n",
      "Iteration  71400 [ 71%]: Loss = 14174851.000\n",
      "Iteration  71500 [ 71%]: Loss = 2887513.000\n",
      "Iteration  71600 [ 71%]: Loss = 12219282.000\n",
      "Iteration  71700 [ 71%]: Loss = 3403544.750\n",
      "Iteration  71800 [ 71%]: Loss = 6725108.500\n",
      "Iteration  71900 [ 71%]: Loss = 4412907.500\n",
      "Iteration  72000 [ 72%]: Loss = 3468202.000\n",
      "Iteration  72100 [ 72%]: Loss = 9856654.000\n",
      "Iteration  72200 [ 72%]: Loss = 5567068.000\n",
      "Iteration  72300 [ 72%]: Loss = 2739498.250\n",
      "Iteration  72400 [ 72%]: Loss = 5291472.500\n",
      "Iteration  72500 [ 72%]: Loss = 4444082.500\n",
      "Iteration  72600 [ 72%]: Loss = 2928497.500\n",
      "Iteration  72700 [ 72%]: Loss = 3715827.500\n",
      "Iteration  72800 [ 72%]: Loss = 2856862.750\n",
      "Iteration  72900 [ 72%]: Loss = 3131347.500\n",
      "Iteration  73000 [ 73%]: Loss = 4053579.250\n",
      "Iteration  73100 [ 73%]: Loss = 2691837.000\n",
      "Iteration  73200 [ 73%]: Loss = 7156438.500\n",
      "Iteration  73300 [ 73%]: Loss = 8414751.000\n",
      "Iteration  73400 [ 73%]: Loss = 2966284.750\n",
      "Iteration  73500 [ 73%]: Loss = 2398376.000\n",
      "Iteration  73600 [ 73%]: Loss = 2837778.500\n",
      "Iteration  73700 [ 73%]: Loss = 6133575.500\n",
      "Iteration  73800 [ 73%]: Loss = 4365837.000\n",
      "Iteration  73900 [ 73%]: Loss = 3136140.250\n",
      "Iteration  74000 [ 74%]: Loss = 2744420.500\n",
      "Iteration  74100 [ 74%]: Loss = 2677967.250\n",
      "Iteration  74200 [ 74%]: Loss = 2588228.250\n",
      "Iteration  74300 [ 74%]: Loss = 4462327.000\n",
      "Iteration  74400 [ 74%]: Loss = 2874074.000\n",
      "Iteration  74500 [ 74%]: Loss = 2970844.750\n",
      "Iteration  74600 [ 74%]: Loss = 3104971.500\n",
      "Iteration  74700 [ 74%]: Loss = 2927139.750\n",
      "Iteration  74800 [ 74%]: Loss = 2652825.000\n",
      "Iteration  74900 [ 74%]: Loss = 2553280.250\n",
      "Iteration  75000 [ 75%]: Loss = 2764010.000\n",
      "Iteration  75100 [ 75%]: Loss = 2626132.500\n",
      "Iteration  75200 [ 75%]: Loss = 2512254.250\n",
      "Iteration  75300 [ 75%]: Loss = 2917322.500\n",
      "Iteration  75400 [ 75%]: Loss = 2330659.500\n",
      "Iteration  75500 [ 75%]: Loss = 2396346.500\n",
      "Iteration  75600 [ 75%]: Loss = 2496283.500\n",
      "Iteration  75700 [ 75%]: Loss = 6412860.500\n",
      "Iteration  75800 [ 75%]: Loss = 2267211.000\n",
      "Iteration  75900 [ 75%]: Loss = 2288465.500\n",
      "Iteration  76000 [ 76%]: Loss = 7703540.000\n",
      "Iteration  76100 [ 76%]: Loss = 2487495.750\n",
      "Iteration  76200 [ 76%]: Loss = 2380196.500\n",
      "Iteration  76300 [ 76%]: Loss = 2401802.500\n",
      "Iteration  76400 [ 76%]: Loss = 2387374.000\n",
      "Iteration  76500 [ 76%]: Loss = 3043650.750\n",
      "Iteration  76600 [ 76%]: Loss = 2841691.000\n",
      "Iteration  76700 [ 76%]: Loss = 2502194.000\n",
      "Iteration  76800 [ 76%]: Loss = 2562254.750\n",
      "Iteration  76900 [ 76%]: Loss = 2155735.000\n",
      "Iteration  77000 [ 77%]: Loss = 2459560.000\n",
      "Iteration  77100 [ 77%]: Loss = 2334428.000\n",
      "Iteration  77200 [ 77%]: Loss = 2492867.750\n",
      "Iteration  77300 [ 77%]: Loss = 2428028.750\n",
      "Iteration  77400 [ 77%]: Loss = 2843731.750\n",
      "Iteration  77500 [ 77%]: Loss = 2104257.250\n",
      "Iteration  77600 [ 77%]: Loss = 3231132.750\n",
      "Iteration  77700 [ 77%]: Loss = 1998599.625\n",
      "Iteration  77800 [ 77%]: Loss = 2164108.000\n",
      "Iteration  77900 [ 77%]: Loss = 2070388.750\n",
      "Iteration  78000 [ 78%]: Loss = 2694060.750\n",
      "Iteration  78100 [ 78%]: Loss = 2181148.000\n",
      "Iteration  78200 [ 78%]: Loss = 2470869.750\n",
      "Iteration  78300 [ 78%]: Loss = 2062174.875\n",
      "Iteration  78400 [ 78%]: Loss = 2880839.000\n",
      "Iteration  78500 [ 78%]: Loss = 5564658.500\n",
      "Iteration  78600 [ 78%]: Loss = 2266558.750\n",
      "Iteration  78700 [ 78%]: Loss = 1932775.500\n",
      "Iteration  78800 [ 78%]: Loss = 2044000.000\n",
      "Iteration  78900 [ 78%]: Loss = 1858612.500\n",
      "Iteration  79000 [ 79%]: Loss = 2063260.500\n",
      "Iteration  79100 [ 79%]: Loss = 1937105.500\n",
      "Iteration  79200 [ 79%]: Loss = 1947735.375\n",
      "Iteration  79300 [ 79%]: Loss = 1905083.125\n",
      "Iteration  79400 [ 79%]: Loss = 2711159.750\n",
      "Iteration  79500 [ 79%]: Loss = 1995782.375\n",
      "Iteration  79600 [ 79%]: Loss = 2113864.250\n",
      "Iteration  79700 [ 79%]: Loss = 1831704.375\n",
      "Iteration  79800 [ 79%]: Loss = 1997225.750\n",
      "Iteration  79900 [ 79%]: Loss = 1758035.250\n",
      "Iteration  80000 [ 80%]: Loss = 1902760.875\n",
      "Iteration  80100 [ 80%]: Loss = 1694630.000\n",
      "Iteration  80200 [ 80%]: Loss = 2119075.250\n",
      "Iteration  80300 [ 80%]: Loss = 1768227.750\n",
      "Iteration  80400 [ 80%]: Loss = 1731567.625\n",
      "Iteration  80500 [ 80%]: Loss = 1641016.500\n",
      "Iteration  80600 [ 80%]: Loss = 1796219.875\n",
      "Iteration  80700 [ 80%]: Loss = 1708433.000\n",
      "Iteration  80800 [ 80%]: Loss = 1637371.250\n",
      "Iteration  80900 [ 80%]: Loss = 1785736.875\n",
      "Iteration  81000 [ 81%]: Loss = 1724865.125\n",
      "Iteration  81100 [ 81%]: Loss = 1536414.500\n",
      "Iteration  81200 [ 81%]: Loss = 1772052.500\n",
      "Iteration  81300 [ 81%]: Loss = 1835867.625\n",
      "Iteration  81400 [ 81%]: Loss = 2219340.000\n",
      "Iteration  81500 [ 81%]: Loss = 1573612.875\n",
      "Iteration  81600 [ 81%]: Loss = 1692942.000\n",
      "Iteration  81700 [ 81%]: Loss = 1641350.875\n",
      "Iteration  81800 [ 81%]: Loss = 1713013.875\n",
      "Iteration  81900 [ 81%]: Loss = 1577116.375\n",
      "Iteration  82000 [ 82%]: Loss = 2007801.500\n",
      "Iteration  82100 [ 82%]: Loss = 1540861.500\n",
      "Iteration  82200 [ 82%]: Loss = 1660047.375\n",
      "Iteration  82300 [ 82%]: Loss = 1598736.375\n",
      "Iteration  82400 [ 82%]: Loss = 1487971.500\n",
      "Iteration  82500 [ 82%]: Loss = 1578937.875\n",
      "Iteration  82600 [ 82%]: Loss = 1567310.750\n",
      "Iteration  82700 [ 82%]: Loss = 1616372.500\n",
      "Iteration  82800 [ 82%]: Loss = 2034975.250\n",
      "Iteration  82900 [ 82%]: Loss = 1623250.375\n",
      "Iteration  83000 [ 83%]: Loss = 1508869.625\n",
      "Iteration  83100 [ 83%]: Loss = 1504516.250\n",
      "Iteration  83200 [ 83%]: Loss = 1765723.625\n",
      "Iteration  83300 [ 83%]: Loss = 1598022.625\n",
      "Iteration  83400 [ 83%]: Loss = 1633034.250\n",
      "Iteration  83500 [ 83%]: Loss = 1512504.250\n",
      "Iteration  83600 [ 83%]: Loss = 1516318.875\n",
      "Iteration  83700 [ 83%]: Loss = 1496734.875\n",
      "Iteration  83800 [ 83%]: Loss = 1474069.625\n",
      "Iteration  83900 [ 83%]: Loss = 1505182.750\n",
      "Iteration  84000 [ 84%]: Loss = 1453168.375\n",
      "Iteration  84100 [ 84%]: Loss = 1599798.500\n",
      "Iteration  84200 [ 84%]: Loss = 1402590.875\n",
      "Iteration  84300 [ 84%]: Loss = 1371972.125\n",
      "Iteration  84400 [ 84%]: Loss = 1297392.500\n",
      "Iteration  84500 [ 84%]: Loss = 1349349.500\n",
      "Iteration  84600 [ 84%]: Loss = 1335249.000\n",
      "Iteration  84700 [ 84%]: Loss = 1474270.000\n",
      "Iteration  84800 [ 84%]: Loss = 1410219.125\n",
      "Iteration  84900 [ 84%]: Loss = 1353641.375\n",
      "Iteration  85000 [ 85%]: Loss = 1353068.625\n",
      "Iteration  85100 [ 85%]: Loss = 1758210.500\n",
      "Iteration  85200 [ 85%]: Loss = 1517966.500\n",
      "Iteration  85300 [ 85%]: Loss = 1368260.375\n",
      "Iteration  85400 [ 85%]: Loss = 1676602.250\n",
      "Iteration  85500 [ 85%]: Loss = 1309047.250\n",
      "Iteration  85600 [ 85%]: Loss = 1285546.000\n",
      "Iteration  85700 [ 85%]: Loss = 1258034.375\n",
      "Iteration  85800 [ 85%]: Loss = 1603337.250\n",
      "Iteration  85900 [ 85%]: Loss = 1572481.000\n",
      "Iteration  86000 [ 86%]: Loss = 1240913.375\n",
      "Iteration  86100 [ 86%]: Loss = 1306715.375\n",
      "Iteration  86200 [ 86%]: Loss = 2008326.125\n",
      "Iteration  86300 [ 86%]: Loss = 1244885.000\n",
      "Iteration  86400 [ 86%]: Loss = 1246445.500\n",
      "Iteration  86500 [ 86%]: Loss = 1190641.250\n",
      "Iteration  86600 [ 86%]: Loss = 1263492.875\n",
      "Iteration  86700 [ 86%]: Loss = 1436772.500\n",
      "Iteration  86800 [ 86%]: Loss = 1180025.500\n",
      "Iteration  86900 [ 86%]: Loss = 1226716.875\n",
      "Iteration  87000 [ 87%]: Loss = 1181790.125\n",
      "Iteration  87100 [ 87%]: Loss = 1258185.500\n",
      "Iteration  87200 [ 87%]: Loss = 1173263.750\n",
      "Iteration  87300 [ 87%]: Loss = 1161706.375\n",
      "Iteration  87400 [ 87%]: Loss = 1982104.750\n",
      "Iteration  87500 [ 87%]: Loss = 1230098.250\n",
      "Iteration  87600 [ 87%]: Loss = 1309162.250\n",
      "Iteration  87700 [ 87%]: Loss = 1119130.250\n",
      "Iteration  87800 [ 87%]: Loss = 1278000.250\n",
      "Iteration  87900 [ 87%]: Loss = 1114351.000\n",
      "Iteration  88000 [ 88%]: Loss = 1129320.875\n",
      "Iteration  88100 [ 88%]: Loss = 1160501.375\n",
      "Iteration  88200 [ 88%]: Loss = 1111563.625\n",
      "Iteration  88300 [ 88%]: Loss = 1170954.875\n",
      "Iteration  88400 [ 88%]: Loss = 1096411.000\n",
      "Iteration  88500 [ 88%]: Loss = 1069120.375\n",
      "Iteration  88600 [ 88%]: Loss = 1102608.625\n",
      "Iteration  88700 [ 88%]: Loss = 1160210.625\n",
      "Iteration  88800 [ 88%]: Loss = 1122356.500\n",
      "Iteration  88900 [ 88%]: Loss = 1427681.875\n",
      "Iteration  89000 [ 89%]: Loss = 1061828.875\n",
      "Iteration  89100 [ 89%]: Loss = 1034193.250\n",
      "Iteration  89200 [ 89%]: Loss = 1042632.875\n",
      "Iteration  89300 [ 89%]: Loss = 1230346.500\n",
      "Iteration  89400 [ 89%]: Loss = 998035.875\n",
      "Iteration  89500 [ 89%]: Loss = 1124346.375\n",
      "Iteration  89600 [ 89%]: Loss = 1069249.750\n",
      "Iteration  89700 [ 89%]: Loss = 1057734.500\n",
      "Iteration  89800 [ 89%]: Loss = 1027046.938\n",
      "Iteration  89900 [ 89%]: Loss = 1005387.625\n",
      "Iteration  90000 [ 90%]: Loss = 996672.312\n",
      "Iteration  90100 [ 90%]: Loss = 1016497.438\n",
      "Iteration  90200 [ 90%]: Loss = 1118479.750\n",
      "Iteration  90300 [ 90%]: Loss = 964760.500\n",
      "Iteration  90400 [ 90%]: Loss = 983188.812\n",
      "Iteration  90500 [ 90%]: Loss = 1017111.812\n",
      "Iteration  90600 [ 90%]: Loss = 967956.750\n",
      "Iteration  90700 [ 90%]: Loss = 948328.062\n",
      "Iteration  90800 [ 90%]: Loss = 959616.688\n",
      "Iteration  90900 [ 90%]: Loss = 953425.812\n",
      "Iteration  91000 [ 91%]: Loss = 985823.938\n",
      "Iteration  91100 [ 91%]: Loss = 961722.000\n",
      "Iteration  91200 [ 91%]: Loss = 941759.812\n",
      "Iteration  91300 [ 91%]: Loss = 963759.812\n",
      "Iteration  91400 [ 91%]: Loss = 953749.688\n",
      "Iteration  91500 [ 91%]: Loss = 1033552.688\n",
      "Iteration  91600 [ 91%]: Loss = 945838.562\n",
      "Iteration  91700 [ 91%]: Loss = 936773.375\n",
      "Iteration  91800 [ 91%]: Loss = 931236.750\n",
      "Iteration  91900 [ 91%]: Loss = 932220.062\n",
      "Iteration  92000 [ 92%]: Loss = 899373.875\n",
      "Iteration  92100 [ 92%]: Loss = 898437.375\n",
      "Iteration  92200 [ 92%]: Loss = 917478.500\n",
      "Iteration  92300 [ 92%]: Loss = 880771.562\n",
      "Iteration  92400 [ 92%]: Loss = 875598.188\n",
      "Iteration  92500 [ 92%]: Loss = 847018.625\n",
      "Iteration  92600 [ 92%]: Loss = 867920.250\n",
      "Iteration  92700 [ 92%]: Loss = 919887.500\n",
      "Iteration  92800 [ 92%]: Loss = 875283.688\n",
      "Iteration  92900 [ 92%]: Loss = 871673.000\n",
      "Iteration  93000 [ 93%]: Loss = 882503.688\n",
      "Iteration  93100 [ 93%]: Loss = 848209.188\n",
      "Iteration  93200 [ 93%]: Loss = 838399.312\n",
      "Iteration  93300 [ 93%]: Loss = 945112.875\n",
      "Iteration  93400 [ 93%]: Loss = 820880.438\n",
      "Iteration  93500 [ 93%]: Loss = 866954.750\n",
      "Iteration  93600 [ 93%]: Loss = 839335.688\n",
      "Iteration  93700 [ 93%]: Loss = 850731.812\n",
      "Iteration  93800 [ 93%]: Loss = 806210.500\n",
      "Iteration  93900 [ 93%]: Loss = 811206.812\n",
      "Iteration  94000 [ 94%]: Loss = 834686.438\n",
      "Iteration  94100 [ 94%]: Loss = 819417.000\n",
      "Iteration  94200 [ 94%]: Loss = 814700.500\n",
      "Iteration  94300 [ 94%]: Loss = 793244.562\n",
      "Iteration  94400 [ 94%]: Loss = 781444.312\n",
      "Iteration  94500 [ 94%]: Loss = 1037015.125\n",
      "Iteration  94600 [ 94%]: Loss = 778849.875\n",
      "Iteration  94700 [ 94%]: Loss = 804222.750\n",
      "Iteration  94800 [ 94%]: Loss = 799471.312\n",
      "Iteration  94900 [ 94%]: Loss = 844158.562\n",
      "Iteration  95000 [ 95%]: Loss = 784747.375\n",
      "Iteration  95100 [ 95%]: Loss = 768116.938\n",
      "Iteration  95200 [ 95%]: Loss = 832513.500\n",
      "Iteration  95300 [ 95%]: Loss = 749611.250\n",
      "Iteration  95400 [ 95%]: Loss = 755676.688\n",
      "Iteration  95500 [ 95%]: Loss = 744881.938\n",
      "Iteration  95600 [ 95%]: Loss = 749398.438\n",
      "Iteration  95700 [ 95%]: Loss = 764952.500\n",
      "Iteration  95800 [ 95%]: Loss = 742591.062\n",
      "Iteration  95900 [ 95%]: Loss = 737639.125\n",
      "Iteration  96000 [ 96%]: Loss = 733883.062\n",
      "Iteration  96100 [ 96%]: Loss = 719123.625\n",
      "Iteration  96200 [ 96%]: Loss = 769413.938\n",
      "Iteration  96300 [ 96%]: Loss = 725449.125\n",
      "Iteration  96400 [ 96%]: Loss = 744730.250\n",
      "Iteration  96500 [ 96%]: Loss = 754248.938\n",
      "Iteration  96600 [ 96%]: Loss = 773420.812\n",
      "Iteration  96700 [ 96%]: Loss = 723409.562\n",
      "Iteration  96800 [ 96%]: Loss = 695279.875\n",
      "Iteration  96900 [ 96%]: Loss = 709303.625\n",
      "Iteration  97000 [ 97%]: Loss = 735536.062\n",
      "Iteration  97100 [ 97%]: Loss = 724895.000\n",
      "Iteration  97200 [ 97%]: Loss = 716525.750\n",
      "Iteration  97300 [ 97%]: Loss = 706562.562\n",
      "Iteration  97400 [ 97%]: Loss = 717206.125\n",
      "Iteration  97500 [ 97%]: Loss = 684487.250\n",
      "Iteration  97600 [ 97%]: Loss = 685788.750\n",
      "Iteration  97700 [ 97%]: Loss = 681504.375\n",
      "Iteration  97800 [ 97%]: Loss = 685040.000\n",
      "Iteration  97900 [ 97%]: Loss = 673985.125\n",
      "Iteration  98000 [ 98%]: Loss = 706872.375\n",
      "Iteration  98100 [ 98%]: Loss = 683915.875\n",
      "Iteration  98200 [ 98%]: Loss = 658470.562\n",
      "Iteration  98300 [ 98%]: Loss = 654684.000\n",
      "Iteration  98400 [ 98%]: Loss = 652726.688\n",
      "Iteration  98500 [ 98%]: Loss = 651752.438\n",
      "Iteration  98600 [ 98%]: Loss = 650445.625\n",
      "Iteration  98700 [ 98%]: Loss = 663176.875\n",
      "Iteration  98800 [ 98%]: Loss = 651656.188\n",
      "Iteration  98900 [ 98%]: Loss = 655044.375\n",
      "Iteration  99000 [ 99%]: Loss = 650612.875\n",
      "Iteration  99100 [ 99%]: Loss = 639614.250\n",
      "Iteration  99200 [ 99%]: Loss = 628848.438\n",
      "Iteration  99300 [ 99%]: Loss = 640281.250\n",
      "Iteration  99400 [ 99%]: Loss = 622310.000\n",
      "Iteration  99500 [ 99%]: Loss = 632808.500\n",
      "Iteration  99600 [ 99%]: Loss = 626587.750\n",
      "Iteration  99700 [ 99%]: Loss = 620300.062\n",
      "Iteration  99800 [ 99%]: Loss = 630733.688\n",
      "Iteration  99900 [ 99%]: Loss = 617587.688\n",
      "Iteration 100000 [100%]: Loss = 605454.312\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=100000, n_print=100, n_samples=1,\n",
    "              logdir='data/run12',\n",
    "              optimizer=tf.train.AdamOptimizer(1e-3))\n",
    "#               scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyphPwhDBNr7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12945b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_A.mean().eval().ravel()); plt.hist(dA.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a27a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_B.mean().eval().ravel(), 200), plt.hist(dB.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b49efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lam_mu.eval(), 200), plt.hist(dlam, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.20435953,  3.21192765,  2.69087458, ...,  3.37591386,\n",
       "        3.34816861,  3.19215989], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_mu.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.20435953,  3.21192765,  2.69087458, ...,  3.37591386,\n",
       "        3.34816861,  3.19215989], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lam.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7152164], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sig.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
