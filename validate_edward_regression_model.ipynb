{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on fake data\n",
    "\n",
    "Here, we generate a synthetic data set for purposes of validating the model constructed in Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll want this function below\n",
    "def softplus(x):\n",
    "    return np.logaddexp(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(12225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the spike count $N_{us}$ observed when stimulus $s$ is presented to unit $u$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N &\\sim \\mathrm{Poisson}(e^\\lambda)  \\\\\n",
    "\\lambda_{us} &\\sim \\mathcal{N}(A_{u} + (B \\cdot X)_{us}, \\sigma^2) \\\\\n",
    "\\log \\sigma &\\sim \\mathcal{N}(-7, 1^2) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "With $X$ an $P \\times N_s$ matrix of known regressors, $Z$ a $K \\times N_s$ matrix of latent binary features\n",
    "governed by an Indian Buffet Process, $A$ and $N_u$ vector of baselines, and $(\\cdot)_+$ the softplus function: \n",
    "$(x)_+ = \\log(1 + e^x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic constants\n",
    "Nrep = 50  # number of observations per unit per stim\n",
    "NB = 10  # number of trials in minibatch\n",
    "NU = 50  # number of units\n",
    "NS = 50  # number of stims\n",
    "P = 3  # number of specified regressors\n",
    "K = 4  # number of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make neural response coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = np.log(softplus(25 + 5 * np.random.randn(NU)))  # baseline\n",
    "dB = np.log(np.array([0.75, 1.2, 1.5]) + 0.1 * np.random.randn(NU, P))  # regressor effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors and latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dX = 0.25 * np.random.randn(P, NS)\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU, dS = np.meshgrid(range(NU), range(NS))\n",
    "dU = dU.ravel()\n",
    "dS = dS.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlam_mean = np.tile(dA[dU] + np.sum(dB[dU] * dX[:, dS].T, axis=1), Nrep)\n",
    "\n",
    "dlam = stats.norm.rvs(loc=dlam_mean, scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcount = stats.poisson.rvs(np.exp(dlam))\n",
    "dcount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZhKQpAYzCUhSg5kEJKnB/j+0Lh8O8Hg/+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224ee0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.exp(dlam), bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJCkDjMEJKnD/j+b0CbBJ18+EwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224ee5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dcount, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = dcount.copy()\n",
    "Xdat = np.tile(dX[:, dS], (1, Nrep)).T\n",
    "unit = np.tile(dU, Nrep)\n",
    "stim = np.tile(dS, Nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some needed constants\n",
    "N = Xdat.shape[0]  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(Xdat.astype('float32'))\n",
    "U = tf.constant(unit)\n",
    "S = tf.constant(stim)\n",
    "counts = tf.constant(count)\n",
    "allinds = tf.constant(np.arange(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(125000), Dimension(3)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a node that produces `NB` indices from the range $[0, N - 1]$. These are the subset of data points we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inds = np.arange(N)\n",
    "batch_counts = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative (p) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pmodel\"):\n",
    "    A = ed.models.Normal(mu=tf.zeros(NU), sigma=5 * tf.ones(NU), name='A')\n",
    "    B = ed.models.Normal(mu=tf.zeros((NU, P)), sigma=3 * tf.ones((NU, P)), name='B')\n",
    "\n",
    "    sig = ed.models.Normal(mu=[-7.0], sigma=[1.], name='sig')\n",
    "\n",
    "    lam_vars = tf.gather(A, U) + tf.reduce_sum(tf.gather(B, U) * X, 1)\n",
    "    lam = ed.models.Normal(mu=tf.gather(lam_vars, batch_inds), \n",
    "                           sigma=tf.exp(sig), name='lam')\n",
    "\n",
    "    cnt = ed.models.Poisson(lam=tf.exp(lam), value=tf.ones(N), name='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition (q) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"qmodel\"):\n",
    "    q_A = ed.models.NormalWithSoftplusSigma(mu=np.log(25) + tf.Variable(tf.random_normal((NU,))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU,))),\n",
    "                                            name='A')\n",
    "    tf.scalar_summary('q_A', tf.reduce_mean(q_A.mean()))\n",
    "\n",
    "    q_B = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(tf.random_normal((NU, P))), \n",
    "                                            sigma=tf.Variable(tf.random_uniform((NU, P))),\n",
    "                                            name='B')\n",
    "    tf.scalar_summary('q_B', tf.reduce_mean(q_B.mean()))\n",
    "\n",
    "    lam_mu = tf.Variable(2 + tf.random_normal((N,)))\n",
    "    tf.scalar_summary('lam_mu_mean', tf.reduce_mean(tf.gather(lam_mu, batch_inds)))\n",
    "    lam_sig = tf.Variable(3 * tf.random_uniform((N,)) + 2)\n",
    "    q_lam = ed.models.NormalWithSoftplusSigma(mu=tf.gather(lam_mu, batch_inds),\n",
    "                                              sigma=tf.gather(lam_sig, batch_inds),\n",
    "                                              name='lam')\n",
    "\n",
    "    q_sig = ed.models.NormalWithSoftplusSigma(mu=tf.Variable(-7. * tf.random_uniform((1,))),\n",
    "                                              sigma=tf.Variable(tf.random_uniform((1,))),\n",
    "                                              name='sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {cnt: batch_counts}\n",
    "inference = ed.KLqp({A: q_A, B: q_B, sig: q_sig, lam: q_lam}, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes before inference:\n",
    "\n",
    "- The `logdir` keyword specifies the place to put the log file (assuming you've instrumented the code to save events, etc.). If a subdirectory is given, pointing Tensorboard at the parent directory allows you to compare across subdirectories (runs).\n",
    "    - I'm using the `jmp/instrumented` branch of the `jmxpearson/edward` fork\n",
    "- I had to lower the learning rate in Adam to avoid NaNs early on in learning. Gradient clipping might solve the same problem.\n",
    "- I'm currently using \"all\" the data, but this should probably be switched to minibatches.\n",
    "- I've used `n_samples` = 1, 5, 10, and 25, which all seem pretty similar after 10k iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1 [  0%]: Loss = 41458282496.000\n",
      "Iteration   100 [  0%]: Loss = 95568658432.000\n",
      "Iteration   200 [  0%]: Loss = 39306772480.000\n",
      "Iteration   300 [  0%]: Loss = 29107064832.000\n",
      "Iteration   400 [  0%]: Loss = 139300061184.000\n",
      "Iteration   500 [  1%]: Loss = 230833553408.000\n",
      "Iteration   600 [  1%]: Loss = 8891456512.000\n",
      "Iteration   700 [  1%]: Loss = 54228299776.000\n",
      "Iteration   800 [  1%]: Loss = 30593196032.000\n",
      "Iteration   900 [  1%]: Loss = 29723918336.000\n",
      "Iteration  1000 [  2%]: Loss = 4598548992.000\n",
      "Iteration  1100 [  2%]: Loss = 76396339200.000\n",
      "Iteration  1200 [  2%]: Loss = 2889106944.000\n",
      "Iteration  1300 [  2%]: Loss = 8476676096.000\n",
      "Iteration  1400 [  2%]: Loss = 2079582208.000\n",
      "Iteration  1500 [  3%]: Loss = 15410042880.000\n",
      "Iteration  1600 [  3%]: Loss = 958369408.000\n",
      "Iteration  1700 [  3%]: Loss = 222933360.000\n",
      "Iteration  1800 [  3%]: Loss = 5903687680.000\n",
      "Iteration  1900 [  3%]: Loss = 13617359872.000\n",
      "Iteration  2000 [  4%]: Loss = 3490253824.000\n",
      "Iteration  2100 [  4%]: Loss = 560661376.000\n",
      "Iteration  2200 [  4%]: Loss = 1402658304.000\n",
      "Iteration  2300 [  4%]: Loss = 714595456.000\n",
      "Iteration  2400 [  4%]: Loss = 2530318848.000\n",
      "Iteration  2500 [  5%]: Loss = 2071756288.000\n",
      "Iteration  2600 [  5%]: Loss = 1376093312.000\n",
      "Iteration  2700 [  5%]: Loss = 1079866880.000\n",
      "Iteration  2800 [  5%]: Loss = 1318990848.000\n",
      "Iteration  2900 [  5%]: Loss = 2800380160.000\n",
      "Iteration  3000 [  6%]: Loss = 501093600.000\n",
      "Iteration  3100 [  6%]: Loss = 1320756864.000\n",
      "Iteration  3200 [  6%]: Loss = 548264512.000\n",
      "Iteration  3300 [  6%]: Loss = 3512818944.000\n",
      "Iteration  3400 [  6%]: Loss = 1923883264.000\n",
      "Iteration  3500 [  7%]: Loss = 203711696.000\n",
      "Iteration  3600 [  7%]: Loss = 514895936.000\n",
      "Iteration  3700 [  7%]: Loss = 611619584.000\n",
      "Iteration  3800 [  7%]: Loss = 242741008.000\n",
      "Iteration  3900 [  7%]: Loss = 337280960.000\n",
      "Iteration  4000 [  8%]: Loss = 258588368.000\n",
      "Iteration  4100 [  8%]: Loss = 420924992.000\n",
      "Iteration  4200 [  8%]: Loss = 453587168.000\n",
      "Iteration  4300 [  8%]: Loss = 421208736.000\n",
      "Iteration  4400 [  8%]: Loss = 290346272.000\n",
      "Iteration  4500 [  9%]: Loss = 520944480.000\n",
      "Iteration  4600 [  9%]: Loss = 169322464.000\n",
      "Iteration  4700 [  9%]: Loss = 602617088.000\n",
      "Iteration  4800 [  9%]: Loss = 88623728.000\n",
      "Iteration  4900 [  9%]: Loss = 135139024.000\n",
      "Iteration  5000 [ 10%]: Loss = 927709824.000\n",
      "Iteration  5100 [ 10%]: Loss = 1199149312.000\n",
      "Iteration  5200 [ 10%]: Loss = 568467904.000\n",
      "Iteration  5300 [ 10%]: Loss = 126455864.000\n",
      "Iteration  5400 [ 10%]: Loss = 308447392.000\n",
      "Iteration  5500 [ 11%]: Loss = 209494560.000\n",
      "Iteration  5600 [ 11%]: Loss = 653900288.000\n",
      "Iteration  5700 [ 11%]: Loss = 143844368.000\n",
      "Iteration  5800 [ 11%]: Loss = 107241296.000\n",
      "Iteration  5900 [ 11%]: Loss = 70306160.000\n",
      "Iteration  6000 [ 12%]: Loss = 112735952.000\n",
      "Iteration  6100 [ 12%]: Loss = 140009264.000\n",
      "Iteration  6200 [ 12%]: Loss = 443077120.000\n",
      "Iteration  6300 [ 12%]: Loss = 145029600.000\n",
      "Iteration  6400 [ 12%]: Loss = 285096416.000\n",
      "Iteration  6500 [ 13%]: Loss = 41461228.000\n",
      "Iteration  6600 [ 13%]: Loss = 266838432.000\n",
      "Iteration  6700 [ 13%]: Loss = 111800872.000\n",
      "Iteration  6800 [ 13%]: Loss = 102992064.000\n",
      "Iteration  6900 [ 13%]: Loss = 1044123968.000\n",
      "Iteration  7000 [ 14%]: Loss = 126867576.000\n",
      "Iteration  7100 [ 14%]: Loss = 592479808.000\n",
      "Iteration  7200 [ 14%]: Loss = 99189752.000\n",
      "Iteration  7300 [ 14%]: Loss = 50045004.000\n",
      "Iteration  7400 [ 14%]: Loss = 28959662.000\n",
      "Iteration  7500 [ 15%]: Loss = 63991532.000\n",
      "Iteration  7600 [ 15%]: Loss = 57275844.000\n",
      "Iteration  7700 [ 15%]: Loss = 31059586.000\n",
      "Iteration  7800 [ 15%]: Loss = 120601344.000\n",
      "Iteration  7900 [ 15%]: Loss = 130896040.000\n",
      "Iteration  8000 [ 16%]: Loss = 26600770.000\n",
      "Iteration  8100 [ 16%]: Loss = 81895936.000\n",
      "Iteration  8200 [ 16%]: Loss = 104986312.000\n",
      "Iteration  8300 [ 16%]: Loss = 15243953.000\n",
      "Iteration  8400 [ 16%]: Loss = 146108656.000\n",
      "Iteration  8500 [ 17%]: Loss = 58369144.000\n",
      "Iteration  8600 [ 17%]: Loss = 21407320.000\n",
      "Iteration  8700 [ 17%]: Loss = 67492200.000\n",
      "Iteration  8800 [ 17%]: Loss = 97818320.000\n",
      "Iteration  8900 [ 17%]: Loss = 134390768.000\n",
      "Iteration  9000 [ 18%]: Loss = 67032968.000\n",
      "Iteration  9100 [ 18%]: Loss = 35888248.000\n",
      "Iteration  9200 [ 18%]: Loss = 45752720.000\n",
      "Iteration  9300 [ 18%]: Loss = 31903142.000\n",
      "Iteration  9400 [ 18%]: Loss = 36050452.000\n",
      "Iteration  9500 [ 19%]: Loss = 22067592.000\n",
      "Iteration  9600 [ 19%]: Loss = 14919772.000\n",
      "Iteration  9700 [ 19%]: Loss = 19472806.000\n",
      "Iteration  9800 [ 19%]: Loss = 47355016.000\n",
      "Iteration  9900 [ 19%]: Loss = 14545553.000\n",
      "Iteration 10000 [ 20%]: Loss = 131322216.000\n",
      "Iteration 10100 [ 20%]: Loss = 53959048.000\n",
      "Iteration 10200 [ 20%]: Loss = 34015516.000\n",
      "Iteration 10300 [ 20%]: Loss = 19296878.000\n",
      "Iteration 10400 [ 20%]: Loss = 33483780.000\n",
      "Iteration 10500 [ 21%]: Loss = 31121432.000\n",
      "Iteration 10600 [ 21%]: Loss = 115085400.000\n",
      "Iteration 10700 [ 21%]: Loss = 25923698.000\n",
      "Iteration 10800 [ 21%]: Loss = 11025091.000\n",
      "Iteration 10900 [ 21%]: Loss = 32221040.000\n",
      "Iteration 11000 [ 22%]: Loss = 32166122.000\n",
      "Iteration 11100 [ 22%]: Loss = 53994884.000\n",
      "Iteration 11200 [ 22%]: Loss = 15863097.000\n",
      "Iteration 11300 [ 22%]: Loss = 11312691.000\n",
      "Iteration 11400 [ 22%]: Loss = 13139871.000\n",
      "Iteration 11500 [ 23%]: Loss = 20197926.000\n",
      "Iteration 11600 [ 23%]: Loss = 6787010.500\n",
      "Iteration 11700 [ 23%]: Loss = 6305455.000\n",
      "Iteration 11800 [ 23%]: Loss = 10557581.000\n",
      "Iteration 11900 [ 23%]: Loss = 14834830.000\n",
      "Iteration 12000 [ 24%]: Loss = 10838203.000\n",
      "Iteration 12100 [ 24%]: Loss = 29596760.000\n",
      "Iteration 12200 [ 24%]: Loss = 13594697.000\n",
      "Iteration 12300 [ 24%]: Loss = 5642959.000\n",
      "Iteration 12400 [ 24%]: Loss = 21991462.000\n",
      "Iteration 12500 [ 25%]: Loss = 8122561.000\n",
      "Iteration 12600 [ 25%]: Loss = 14159559.000\n",
      "Iteration 12700 [ 25%]: Loss = 5655241.000\n",
      "Iteration 12800 [ 25%]: Loss = 8355930.000\n",
      "Iteration 12900 [ 25%]: Loss = 7992420.500\n",
      "Iteration 13000 [ 26%]: Loss = 7405963.000\n",
      "Iteration 13100 [ 26%]: Loss = 8926046.000\n",
      "Iteration 13200 [ 26%]: Loss = 6680944.000\n",
      "Iteration 13300 [ 26%]: Loss = 5416400.000\n",
      "Iteration 13400 [ 26%]: Loss = 5305485.500\n",
      "Iteration 13500 [ 27%]: Loss = 3103352.000\n",
      "Iteration 13600 [ 27%]: Loss = 13933952.000\n",
      "Iteration 13700 [ 27%]: Loss = 5718028.500\n",
      "Iteration 13800 [ 27%]: Loss = 4818524.500\n",
      "Iteration 13900 [ 27%]: Loss = 6607608.500\n",
      "Iteration 14000 [ 28%]: Loss = 5662871.000\n",
      "Iteration 14100 [ 28%]: Loss = 4541510.000\n",
      "Iteration 14200 [ 28%]: Loss = 2280351.500\n",
      "Iteration 14300 [ 28%]: Loss = 2109690.750\n",
      "Iteration 14400 [ 28%]: Loss = 4258865.000\n",
      "Iteration 14500 [ 28%]: Loss = 5946764.500\n",
      "Iteration 14600 [ 29%]: Loss = 3674560.000\n",
      "Iteration 14700 [ 29%]: Loss = 6652570.500\n",
      "Iteration 14800 [ 29%]: Loss = 4024384.750\n",
      "Iteration 14900 [ 29%]: Loss = 2414185.000\n",
      "Iteration 15000 [ 30%]: Loss = 2878081.000\n",
      "Iteration 15100 [ 30%]: Loss = 2933095.750\n",
      "Iteration 15200 [ 30%]: Loss = 4936345.500\n",
      "Iteration 15300 [ 30%]: Loss = 6835573.000\n",
      "Iteration 15400 [ 30%]: Loss = 1851913.500\n",
      "Iteration 15500 [ 31%]: Loss = 3479414.250\n",
      "Iteration 15600 [ 31%]: Loss = 1871990.750\n",
      "Iteration 15700 [ 31%]: Loss = 2984591.000\n",
      "Iteration 15800 [ 31%]: Loss = 4105871.750\n",
      "Iteration 15900 [ 31%]: Loss = 2620216.750\n",
      "Iteration 16000 [ 32%]: Loss = 2132409.750\n",
      "Iteration 16100 [ 32%]: Loss = 1541496.875\n",
      "Iteration 16200 [ 32%]: Loss = 1877935.000\n",
      "Iteration 16300 [ 32%]: Loss = 1970670.125\n",
      "Iteration 16400 [ 32%]: Loss = 2439798.250\n",
      "Iteration 16500 [ 33%]: Loss = 14250006.000\n",
      "Iteration 16600 [ 33%]: Loss = 1231111.875\n",
      "Iteration 16700 [ 33%]: Loss = 2417048.500\n",
      "Iteration 16800 [ 33%]: Loss = 1335692.500\n",
      "Iteration 16900 [ 33%]: Loss = 1535959.500\n",
      "Iteration 17000 [ 34%]: Loss = 1177201.375\n",
      "Iteration 17100 [ 34%]: Loss = 1018131.500\n",
      "Iteration 17200 [ 34%]: Loss = 1416013.125\n",
      "Iteration 17300 [ 34%]: Loss = 1642626.625\n",
      "Iteration 17400 [ 34%]: Loss = 3429715.250\n",
      "Iteration 17500 [ 35%]: Loss = 1022939.625\n",
      "Iteration 17600 [ 35%]: Loss = 1369427.750\n",
      "Iteration 17700 [ 35%]: Loss = 1507140.125\n",
      "Iteration 17800 [ 35%]: Loss = 1272554.750\n",
      "Iteration 17900 [ 35%]: Loss = 1244294.375\n",
      "Iteration 18000 [ 36%]: Loss = 1615842.875\n",
      "Iteration 18100 [ 36%]: Loss = 1153750.875\n",
      "Iteration 18200 [ 36%]: Loss = 1490175.250\n",
      "Iteration 18300 [ 36%]: Loss = 1164210.500\n",
      "Iteration 18400 [ 36%]: Loss = 1103385.250\n",
      "Iteration 18500 [ 37%]: Loss = 976989.438\n",
      "Iteration 18600 [ 37%]: Loss = 731669.688\n",
      "Iteration 18700 [ 37%]: Loss = 817790.312\n",
      "Iteration 18800 [ 37%]: Loss = 805200.812\n",
      "Iteration 18900 [ 37%]: Loss = 936931.000\n",
      "Iteration 19000 [ 38%]: Loss = 862987.188\n",
      "Iteration 19100 [ 38%]: Loss = 899538.750\n",
      "Iteration 19200 [ 38%]: Loss = 696213.375\n",
      "Iteration 19300 [ 38%]: Loss = 634536.500\n",
      "Iteration 19400 [ 38%]: Loss = 694776.188\n",
      "Iteration 19500 [ 39%]: Loss = 631823.500\n",
      "Iteration 19600 [ 39%]: Loss = 624360.125\n",
      "Iteration 19700 [ 39%]: Loss = 732515.938\n",
      "Iteration 19800 [ 39%]: Loss = 607139.188\n",
      "Iteration 19900 [ 39%]: Loss = 606124.062\n",
      "Iteration 20000 [ 40%]: Loss = 608677.062\n",
      "Iteration 20100 [ 40%]: Loss = 955647.500\n",
      "Iteration 20200 [ 40%]: Loss = 565811.062\n",
      "Iteration 20300 [ 40%]: Loss = 770660.250\n",
      "Iteration 20400 [ 40%]: Loss = 662218.562\n",
      "Iteration 20500 [ 41%]: Loss = 627762.375\n",
      "Iteration 20600 [ 41%]: Loss = 637972.000\n",
      "Iteration 20700 [ 41%]: Loss = 690156.562\n",
      "Iteration 20800 [ 41%]: Loss = 1041533.438\n",
      "Iteration 20900 [ 41%]: Loss = 567627.750\n",
      "Iteration 21000 [ 42%]: Loss = 657015.562\n",
      "Iteration 21100 [ 42%]: Loss = 572095.312\n",
      "Iteration 21200 [ 42%]: Loss = 622058.062\n",
      "Iteration 21300 [ 42%]: Loss = 585068.750\n",
      "Iteration 21400 [ 42%]: Loss = 569096.312\n",
      "Iteration 21500 [ 43%]: Loss = 538726.875\n",
      "Iteration 21600 [ 43%]: Loss = 575310.625\n",
      "Iteration 21700 [ 43%]: Loss = 647543.688\n",
      "Iteration 21800 [ 43%]: Loss = 547015.250\n",
      "Iteration 21900 [ 43%]: Loss = 640540.938\n",
      "Iteration 22000 [ 44%]: Loss = 529937.250\n",
      "Iteration 22100 [ 44%]: Loss = 759576.688\n",
      "Iteration 22200 [ 44%]: Loss = 510602.812\n",
      "Iteration 22300 [ 44%]: Loss = 502863.562\n",
      "Iteration 22400 [ 44%]: Loss = 514281.188\n",
      "Iteration 22500 [ 45%]: Loss = 503685.875\n",
      "Iteration 22600 [ 45%]: Loss = 505986.250\n",
      "Iteration 22700 [ 45%]: Loss = 506269.906\n",
      "Iteration 22800 [ 45%]: Loss = 499586.688\n",
      "Iteration 22900 [ 45%]: Loss = 514565.656\n",
      "Iteration 23000 [ 46%]: Loss = 497445.656\n",
      "Iteration 23100 [ 46%]: Loss = 504838.938\n",
      "Iteration 23200 [ 46%]: Loss = 492386.000\n",
      "Iteration 23300 [ 46%]: Loss = 511408.844\n",
      "Iteration 23400 [ 46%]: Loss = 493687.438\n",
      "Iteration 23500 [ 47%]: Loss = 481515.125\n",
      "Iteration 23600 [ 47%]: Loss = 479815.219\n",
      "Iteration 23700 [ 47%]: Loss = 519126.656\n",
      "Iteration 23800 [ 47%]: Loss = 488814.344\n",
      "Iteration 23900 [ 47%]: Loss = 496805.781\n",
      "Iteration 24000 [ 48%]: Loss = 472697.969\n",
      "Iteration 24100 [ 48%]: Loss = 499786.594\n",
      "Iteration 24200 [ 48%]: Loss = 467483.844\n",
      "Iteration 24300 [ 48%]: Loss = 478609.656\n",
      "Iteration 24400 [ 48%]: Loss = 512419.875\n",
      "Iteration 24500 [ 49%]: Loss = 564944.750\n",
      "Iteration 24600 [ 49%]: Loss = 481726.844\n",
      "Iteration 24700 [ 49%]: Loss = 503694.969\n",
      "Iteration 24800 [ 49%]: Loss = 503268.375\n",
      "Iteration 24900 [ 49%]: Loss = 463572.656\n",
      "Iteration 25000 [ 50%]: Loss = 463910.188\n",
      "Iteration 25100 [ 50%]: Loss = 456811.312\n",
      "Iteration 25200 [ 50%]: Loss = 518345.031\n",
      "Iteration 25300 [ 50%]: Loss = 468304.406\n",
      "Iteration 25400 [ 50%]: Loss = 458221.562\n",
      "Iteration 25500 [ 51%]: Loss = 502153.062\n",
      "Iteration 25600 [ 51%]: Loss = 453295.688\n",
      "Iteration 25700 [ 51%]: Loss = 448491.688\n",
      "Iteration 25800 [ 51%]: Loss = 559593.688\n",
      "Iteration 25900 [ 51%]: Loss = 447017.656\n",
      "Iteration 26000 [ 52%]: Loss = 450355.656\n",
      "Iteration 26100 [ 52%]: Loss = 460190.125\n",
      "Iteration 26200 [ 52%]: Loss = 446232.406\n",
      "Iteration 26300 [ 52%]: Loss = 449610.719\n",
      "Iteration 26400 [ 52%]: Loss = 440460.719\n",
      "Iteration 26500 [ 53%]: Loss = 440113.875\n",
      "Iteration 26600 [ 53%]: Loss = 433452.344\n",
      "Iteration 26700 [ 53%]: Loss = 433970.688\n",
      "Iteration 26800 [ 53%]: Loss = 438687.000\n",
      "Iteration 26900 [ 53%]: Loss = 443206.000\n",
      "Iteration 27000 [ 54%]: Loss = 439628.000\n",
      "Iteration 27100 [ 54%]: Loss = 435596.781\n",
      "Iteration 27200 [ 54%]: Loss = 432001.438\n",
      "Iteration 27300 [ 54%]: Loss = 481819.562\n",
      "Iteration 27400 [ 54%]: Loss = 439628.969\n",
      "Iteration 27500 [ 55%]: Loss = 432339.531\n",
      "Iteration 27600 [ 55%]: Loss = 424203.781\n",
      "Iteration 27700 [ 55%]: Loss = 441931.469\n",
      "Iteration 27800 [ 55%]: Loss = 430218.594\n",
      "Iteration 27900 [ 55%]: Loss = 422740.750\n",
      "Iteration 28000 [ 56%]: Loss = 422482.125\n",
      "Iteration 28100 [ 56%]: Loss = 428246.688\n",
      "Iteration 28200 [ 56%]: Loss = 426367.094\n",
      "Iteration 28300 [ 56%]: Loss = 414245.750\n",
      "Iteration 28400 [ 56%]: Loss = 420895.250\n",
      "Iteration 28500 [ 56%]: Loss = 426826.250\n",
      "Iteration 28600 [ 57%]: Loss = 409917.000\n",
      "Iteration 28700 [ 57%]: Loss = 440518.625\n",
      "Iteration 28800 [ 57%]: Loss = 418539.000\n",
      "Iteration 28900 [ 57%]: Loss = 410650.562\n",
      "Iteration 29000 [ 57%]: Loss = 412273.562\n",
      "Iteration 29100 [ 58%]: Loss = 423597.906\n",
      "Iteration 29200 [ 58%]: Loss = 406134.562\n",
      "Iteration 29300 [ 58%]: Loss = 410835.281\n",
      "Iteration 29400 [ 58%]: Loss = 408368.188\n",
      "Iteration 29500 [ 59%]: Loss = 406402.188\n",
      "Iteration 29600 [ 59%]: Loss = 411318.094\n",
      "Iteration 29700 [ 59%]: Loss = 413430.125\n",
      "Iteration 29800 [ 59%]: Loss = 411516.156\n",
      "Iteration 29900 [ 59%]: Loss = 405428.656\n",
      "Iteration 30000 [ 60%]: Loss = 415870.312\n",
      "Iteration 30100 [ 60%]: Loss = 408955.531\n",
      "Iteration 30200 [ 60%]: Loss = 402999.875\n",
      "Iteration 30300 [ 60%]: Loss = 400397.625\n",
      "Iteration 30400 [ 60%]: Loss = 401650.938\n",
      "Iteration 30500 [ 61%]: Loss = 399268.312\n",
      "Iteration 30600 [ 61%]: Loss = 398763.188\n",
      "Iteration 30700 [ 61%]: Loss = 402251.375\n",
      "Iteration 30800 [ 61%]: Loss = 402211.656\n",
      "Iteration 30900 [ 61%]: Loss = 397683.938\n",
      "Iteration 31000 [ 62%]: Loss = 408200.312\n",
      "Iteration 31100 [ 62%]: Loss = 398264.531\n",
      "Iteration 31200 [ 62%]: Loss = 396884.500\n",
      "Iteration 31300 [ 62%]: Loss = 398498.875\n",
      "Iteration 31400 [ 62%]: Loss = 392849.500\n",
      "Iteration 31500 [ 63%]: Loss = 396653.594\n",
      "Iteration 31600 [ 63%]: Loss = 393831.562\n",
      "Iteration 31700 [ 63%]: Loss = 396358.969\n",
      "Iteration 31800 [ 63%]: Loss = 401530.906\n",
      "Iteration 31900 [ 63%]: Loss = 396159.188\n",
      "Iteration 32000 [ 64%]: Loss = 395138.969\n",
      "Iteration 32100 [ 64%]: Loss = 390279.438\n",
      "Iteration 32200 [ 64%]: Loss = 396833.719\n",
      "Iteration 32300 [ 64%]: Loss = 392681.844\n",
      "Iteration 32400 [ 64%]: Loss = 392123.500\n",
      "Iteration 32500 [ 65%]: Loss = 395294.688\n",
      "Iteration 32600 [ 65%]: Loss = 390343.500\n",
      "Iteration 32700 [ 65%]: Loss = 393238.719\n",
      "Iteration 32800 [ 65%]: Loss = 397072.562\n",
      "Iteration 32900 [ 65%]: Loss = 392398.688\n",
      "Iteration 33000 [ 66%]: Loss = 391500.719\n",
      "Iteration 33100 [ 66%]: Loss = 390154.500\n",
      "Iteration 33200 [ 66%]: Loss = 387824.344\n",
      "Iteration 33300 [ 66%]: Loss = 388565.562\n",
      "Iteration 33400 [ 66%]: Loss = 388429.688\n",
      "Iteration 33500 [ 67%]: Loss = 391188.406\n",
      "Iteration 33600 [ 67%]: Loss = 390299.688\n",
      "Iteration 33700 [ 67%]: Loss = 388414.094\n",
      "Iteration 33800 [ 67%]: Loss = 389728.406\n",
      "Iteration 33900 [ 67%]: Loss = 388237.750\n",
      "Iteration 34000 [ 68%]: Loss = 386411.688\n",
      "Iteration 34100 [ 68%]: Loss = 389002.312\n",
      "Iteration 34200 [ 68%]: Loss = 386911.906\n",
      "Iteration 34300 [ 68%]: Loss = 386112.219\n",
      "Iteration 34400 [ 68%]: Loss = 388388.781\n",
      "Iteration 34500 [ 69%]: Loss = 389412.438\n",
      "Iteration 34600 [ 69%]: Loss = 388466.625\n",
      "Iteration 34700 [ 69%]: Loss = 386172.906\n",
      "Iteration 34800 [ 69%]: Loss = 388860.312\n",
      "Iteration 34900 [ 69%]: Loss = 385432.250\n",
      "Iteration 35000 [ 70%]: Loss = 389485.062\n",
      "Iteration 35100 [ 70%]: Loss = 389224.875\n",
      "Iteration 35200 [ 70%]: Loss = 388568.312\n",
      "Iteration 35300 [ 70%]: Loss = 385213.906\n",
      "Iteration 35400 [ 70%]: Loss = 385560.875\n",
      "Iteration 35500 [ 71%]: Loss = 392165.344\n",
      "Iteration 35600 [ 71%]: Loss = 385047.031\n",
      "Iteration 35700 [ 71%]: Loss = 384793.969\n",
      "Iteration 35800 [ 71%]: Loss = 386255.906\n",
      "Iteration 35900 [ 71%]: Loss = 387319.625\n",
      "Iteration 36000 [ 72%]: Loss = 384790.500\n",
      "Iteration 36100 [ 72%]: Loss = 386814.312\n",
      "Iteration 36200 [ 72%]: Loss = 385640.938\n",
      "Iteration 36300 [ 72%]: Loss = 384905.281\n",
      "Iteration 36400 [ 72%]: Loss = 385956.562\n",
      "Iteration 36500 [ 73%]: Loss = 387464.250\n",
      "Iteration 36600 [ 73%]: Loss = 384541.250\n",
      "Iteration 36700 [ 73%]: Loss = 384242.000\n",
      "Iteration 36800 [ 73%]: Loss = 385974.625\n",
      "Iteration 36900 [ 73%]: Loss = 385199.500\n",
      "Iteration 37000 [ 74%]: Loss = 385598.188\n",
      "Iteration 37100 [ 74%]: Loss = 388603.531\n",
      "Iteration 37200 [ 74%]: Loss = 387430.562\n",
      "Iteration 37300 [ 74%]: Loss = 383837.844\n",
      "Iteration 37400 [ 74%]: Loss = 386473.219\n",
      "Iteration 37500 [ 75%]: Loss = 384617.625\n",
      "Iteration 37600 [ 75%]: Loss = 384127.344\n",
      "Iteration 37700 [ 75%]: Loss = 384551.750\n",
      "Iteration 37800 [ 75%]: Loss = 384181.875\n",
      "Iteration 37900 [ 75%]: Loss = 386061.812\n",
      "Iteration 38000 [ 76%]: Loss = 384666.375\n",
      "Iteration 38100 [ 76%]: Loss = 388218.781\n",
      "Iteration 38200 [ 76%]: Loss = 385452.125\n",
      "Iteration 38300 [ 76%]: Loss = 384423.656\n",
      "Iteration 38400 [ 76%]: Loss = 384325.031\n",
      "Iteration 38500 [ 77%]: Loss = 384249.875\n",
      "Iteration 38600 [ 77%]: Loss = 385809.375\n",
      "Iteration 38700 [ 77%]: Loss = 384480.750\n",
      "Iteration 38800 [ 77%]: Loss = 385363.000\n",
      "Iteration 38900 [ 77%]: Loss = 384116.156\n",
      "Iteration 39000 [ 78%]: Loss = 385968.938\n",
      "Iteration 39100 [ 78%]: Loss = 384544.000\n",
      "Iteration 39200 [ 78%]: Loss = 384302.344\n",
      "Iteration 39300 [ 78%]: Loss = 384570.312\n",
      "Iteration 39400 [ 78%]: Loss = 384540.156\n",
      "Iteration 39500 [ 79%]: Loss = 384724.906\n",
      "Iteration 39600 [ 79%]: Loss = 384809.531\n",
      "Iteration 39700 [ 79%]: Loss = 384865.375\n",
      "Iteration 39800 [ 79%]: Loss = 385088.875\n",
      "Iteration 39900 [ 79%]: Loss = 384459.969\n",
      "Iteration 40000 [ 80%]: Loss = 384778.688\n",
      "Iteration 40100 [ 80%]: Loss = 383706.844\n",
      "Iteration 40200 [ 80%]: Loss = 384737.031\n",
      "Iteration 40300 [ 80%]: Loss = 383867.062\n",
      "Iteration 40400 [ 80%]: Loss = 383779.125\n",
      "Iteration 40500 [ 81%]: Loss = 383747.938\n",
      "Iteration 40600 [ 81%]: Loss = 384114.875\n",
      "Iteration 40700 [ 81%]: Loss = 384209.750\n",
      "Iteration 40800 [ 81%]: Loss = 384111.594\n",
      "Iteration 40900 [ 81%]: Loss = 385768.750\n",
      "Iteration 41000 [ 82%]: Loss = 383703.156\n",
      "Iteration 41100 [ 82%]: Loss = 386107.312\n",
      "Iteration 41200 [ 82%]: Loss = 383995.156\n",
      "Iteration 41300 [ 82%]: Loss = 383769.469\n",
      "Iteration 41400 [ 82%]: Loss = 384637.250\n",
      "Iteration 41500 [ 83%]: Loss = 383337.719\n",
      "Iteration 41600 [ 83%]: Loss = 384351.000\n",
      "Iteration 41700 [ 83%]: Loss = 383635.750\n",
      "Iteration 41800 [ 83%]: Loss = 388230.375\n",
      "Iteration 41900 [ 83%]: Loss = 383975.938\n",
      "Iteration 42000 [ 84%]: Loss = 383585.969\n",
      "Iteration 42100 [ 84%]: Loss = 386975.594\n",
      "Iteration 42200 [ 84%]: Loss = 383692.562\n",
      "Iteration 42300 [ 84%]: Loss = 383479.812\n",
      "Iteration 42400 [ 84%]: Loss = 383760.938\n",
      "Iteration 42500 [ 85%]: Loss = 383830.719\n",
      "Iteration 42600 [ 85%]: Loss = 383618.688\n",
      "Iteration 42700 [ 85%]: Loss = 402049.500\n",
      "Iteration 42800 [ 85%]: Loss = 384400.062\n",
      "Iteration 42900 [ 85%]: Loss = 383613.750\n",
      "Iteration 43000 [ 86%]: Loss = 384008.031\n",
      "Iteration 43100 [ 86%]: Loss = 383510.375\n",
      "Iteration 43200 [ 86%]: Loss = 383604.750\n",
      "Iteration 43300 [ 86%]: Loss = 383284.906\n",
      "Iteration 43400 [ 86%]: Loss = 383659.531\n",
      "Iteration 43500 [ 87%]: Loss = 384095.812\n",
      "Iteration 43600 [ 87%]: Loss = 383469.781\n",
      "Iteration 43700 [ 87%]: Loss = 383283.406\n",
      "Iteration 43800 [ 87%]: Loss = 384093.688\n",
      "Iteration 43900 [ 87%]: Loss = 383499.500\n",
      "Iteration 44000 [ 88%]: Loss = 383982.875\n",
      "Iteration 44100 [ 88%]: Loss = 383520.938\n",
      "Iteration 44200 [ 88%]: Loss = 383206.844\n",
      "Iteration 44300 [ 88%]: Loss = 383628.562\n",
      "Iteration 44400 [ 88%]: Loss = 383394.344\n",
      "Iteration 44500 [ 89%]: Loss = 383253.250\n",
      "Iteration 44600 [ 89%]: Loss = 383204.844\n",
      "Iteration 44700 [ 89%]: Loss = 383440.000\n",
      "Iteration 44800 [ 89%]: Loss = 383253.219\n",
      "Iteration 44900 [ 89%]: Loss = 383342.469\n",
      "Iteration 45000 [ 90%]: Loss = 383348.531\n",
      "Iteration 45100 [ 90%]: Loss = 383624.875\n",
      "Iteration 45200 [ 90%]: Loss = 383334.781\n",
      "Iteration 45300 [ 90%]: Loss = 383281.906\n",
      "Iteration 45400 [ 90%]: Loss = 383375.750\n",
      "Iteration 45500 [ 91%]: Loss = 383141.938\n",
      "Iteration 45600 [ 91%]: Loss = 383320.812\n",
      "Iteration 45700 [ 91%]: Loss = 383523.438\n",
      "Iteration 45800 [ 91%]: Loss = 383270.781\n",
      "Iteration 45900 [ 91%]: Loss = 383458.438\n",
      "Iteration 46000 [ 92%]: Loss = 383496.719\n",
      "Iteration 46100 [ 92%]: Loss = 383279.094\n",
      "Iteration 46200 [ 92%]: Loss = 383201.406\n",
      "Iteration 46300 [ 92%]: Loss = 383326.156\n",
      "Iteration 46400 [ 92%]: Loss = 383120.188\n",
      "Iteration 46500 [ 93%]: Loss = 383328.656\n",
      "Iteration 46600 [ 93%]: Loss = 383328.875\n",
      "Iteration 46700 [ 93%]: Loss = 383346.969\n",
      "Iteration 46800 [ 93%]: Loss = 383267.312\n",
      "Iteration 46900 [ 93%]: Loss = 383308.406\n",
      "Iteration 47000 [ 94%]: Loss = 383161.156\n",
      "Iteration 47100 [ 94%]: Loss = 383335.500\n",
      "Iteration 47200 [ 94%]: Loss = 383279.375\n",
      "Iteration 47300 [ 94%]: Loss = 383235.188\n",
      "Iteration 47400 [ 94%]: Loss = 383217.688\n",
      "Iteration 47500 [ 95%]: Loss = 383152.656\n",
      "Iteration 47600 [ 95%]: Loss = 383046.656\n",
      "Iteration 47700 [ 95%]: Loss = 383282.625\n",
      "Iteration 47800 [ 95%]: Loss = 383160.906\n",
      "Iteration 47900 [ 95%]: Loss = 383226.000\n",
      "Iteration 48000 [ 96%]: Loss = 383172.938\n",
      "Iteration 48100 [ 96%]: Loss = 383236.125\n",
      "Iteration 48200 [ 96%]: Loss = 383132.281\n",
      "Iteration 48300 [ 96%]: Loss = 383237.094\n",
      "Iteration 48400 [ 96%]: Loss = 383250.875\n",
      "Iteration 48500 [ 97%]: Loss = 383364.375\n",
      "Iteration 48600 [ 97%]: Loss = 383206.000\n",
      "Iteration 48700 [ 97%]: Loss = 383298.500\n",
      "Iteration 48800 [ 97%]: Loss = 383051.625\n",
      "Iteration 48900 [ 97%]: Loss = 383198.000\n",
      "Iteration 49000 [ 98%]: Loss = 383309.625\n",
      "Iteration 49100 [ 98%]: Loss = 383297.438\n",
      "Iteration 49200 [ 98%]: Loss = 383282.125\n",
      "Iteration 49300 [ 98%]: Loss = 383175.750\n",
      "Iteration 49400 [ 98%]: Loss = 383153.750\n",
      "Iteration 49500 [ 99%]: Loss = 383265.969\n",
      "Iteration 49600 [ 99%]: Loss = 383318.969\n",
      "Iteration 49700 [ 99%]: Loss = 383072.250\n",
      "Iteration 49800 [ 99%]: Loss = 383003.969\n",
      "Iteration 49900 [ 99%]: Loss = 383199.688\n",
      "Iteration 50000 [100%]: Loss = 383237.812\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=50000, n_print=100, n_samples=1,\n",
    "              logdir='data/run14',\n",
    "              optimizer=tf.train.AdamOptimizer(1e-2))\n",
    "#               scale={lam: N/NB, cnt: N/NB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a45d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_A.mean().eval().ravel()); plt.hist(dA.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b30ee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_B.mean().eval().ravel(), 200), plt.hist(dB.ravel(), 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129c0c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lam_mu.eval(), 200), plt.hist(dlam, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.43366909,  3.37957621,  3.18125939, ...,  3.38823128,\n",
       "        3.43631458,  3.11984277], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_mu.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.43366909,  3.37957621,  3.18125939, ...,  3.38823128,\n",
       "        3.43631458,  3.11984277], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lam.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.26502538], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sig.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
